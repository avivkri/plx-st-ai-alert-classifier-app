1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_critical - Latency to the upstream service management/aris-kube-prometheus-stack-prometheus has been critically high (more than 30 seconds) for the last 3 minutes. Service is unresponsive and should be restarted.
3,[pr-cp-reg-10043 - aris-kube-prometheus-stack] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10039 - aris-spot] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10002 - aris-sealed-secrets] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-nginx-ingress/pr-cp-reg-10022 cluster is .
3,[pr-cp-reg-10024 - aris-keda] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10003 - aris-keda] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-keda/keda-operator-metrics-apiserver-7c746565cb-t6krb"
3,[pr-cp-reg-10025 - aris-cluster-autoscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10011 - aris-cluster-autoscaler] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10024 - pr-customer-env-spark] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-147-93.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10014 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container log-backup in pod collaboration-0.
3,[pr-cp-reg-10025 - basic-application-bootstrapping] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-cluster-autoscaler/exported_job failed to complete.
3,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-prometheus-stack/pr-cp-reg-10012 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10030 - aris-keda] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,"[pr-cp-reg-10006 - aris-sealed-secrets] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-sealed-secrets/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
1,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-prometheus-stack is down.
3,[pr-cp-reg-10023 - aris-kube-prometheus-stack] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_high - Latency to the upstream service aris-nginx-ingress/aris-kube-prometheus-stack-prometheus has been high (more than 10 seconds) for the last 3 minutes. Service might become unresponsive shortly.
3,[pr-cp-reg-10023 - pr-customer-env] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-cluster-autoscaler/pr-cp-reg-10043 cluster is .
3,[pr-cp-reg-10039 - kube-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10047 - kube-public] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10048 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10006 - pr-customer-env] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10039 - aris-keda] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10013 - aris-cluster-autoscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10023 - aris-kube-downscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10022 - pr-customer-env] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface eni0039e8a4ad3 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10029 - aris-keda] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10037 - kube-system] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cdf in pod cloudsearch.
3,[pr-cp-reg-10021 - kube-system] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on nvme2 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10024 - basic-application-bootstrapping] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10031 - aris-keda] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10010 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10022 - pr-customer-env] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_node_memory_pressure - ip-10-0-143-220.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10045 - kube-node-lease] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10021 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10016 - kube-node-lease] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container portalserver in pod backup-tenants-28123035-nswx7.
1,[pr-cp-reg-10004 - aris-keda] - ARIS_k8s_pod_restarted - The pod aris-keda/keda-operator has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10011 - kube-system] - KubeNodeReadinessFlapping - The readiness status of node ip-10-0-139-113.eu-central-1.compute.internal has changed  times in the last 15 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod postgres-0.
1,[pr-cp-reg-10011 - pr-customer-env] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10015 - kube-node-lease] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 failed to send  of notifications to wechat.
1,[pr-cp-reg-10050 - falcon-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10043 - kube-node-lease] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10004 - kube-system] - KubeNodeUnreachable - ip-10-0-138-163.eu-central-1.compute.internal is unreachable and some workloads may be rescheduled.
1,[pr-cp-reg-10017 - kube-public] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""0"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-1"
1,[pr-cp-reg-10037 - aris-kube-prometheus-stack] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10001 - pr-customer-env-spark] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,"[pr-cp-reg-10009 - aris-sealed-secrets] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller config-reloader sidecar attempts to sync config in aris-sealed-secrets namespace. As a result, configuration for service running in sealed-secrets-controller may be stale and cannot be updated anymore."
1,[pr-cp-reg-10036 - aris-kube-downscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10030 - kube-public] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-public namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10040 - kube-public] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10004 - aris-sealed-secrets] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-sealed-secrets/sealed-secrets-controller"
3,[pr-cp-reg-10030 - pr-customer-env-spark] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10017 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10046 - kube-node-lease] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10010 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme1n1"" changing its up status often on node-exporter kasten-io/logging-svc-58b457d7d8-pfqmx"
1,[pr-cp-reg-10023 - aris-cluster-autoscaler] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_k8s_node_out_of_disk - ip-10-0-138-163.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10019 - aris-cluster-autoscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10027 - aris-kube-prometheus-stack] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10012 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 8h to complete.
3,[pr-cp-reg-10030 - falcon-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10044 - aris-keda] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
0,"[pr-cp-reg-10043 - aris-kube-prometheus-stack] - Watchdog - This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the ""DeadMansSnitch"" integration in PagerDuty."
1,[pr-cp-reg-10020 - aris-kube-downscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-downscaler is down.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container create-es-index-template in pod cdf-0.
3,[pr-cp-reg-10010 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
1,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10024 - aris-keda] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10005 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10008 - aris-kube-downscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-downscaler has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10026 - aris-kube-downscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10030 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme7n1"" changing its up status often on node-exporter management/argocd-repo-server-6676cd4f9c-l95m8"
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_argocd_app_unknown - The ArgoCD app 04abfe3da01db1f02bdd752a9e3436b7cf688976b23b6f6c51773d813348eea1 is in unknown state for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10016 - default] - CPUThrottlingHigh -  throttling of CPU in namespace default for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_k8s_node_pid_pressure - ip-10-0-143-220.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10001 - basic-application-bootstrapping] - CPUThrottlingHigh -  throttling of CPU in namespace basic-application-bootstrapping for container basic-application-bootstrapping in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10046 - aris-cluster-autoscaler] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10008 - aris-cluster-autoscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10043 - kube-system] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to slack sent from any instance in the kubelet cluster is .
3,[pr-cp-reg-10027 - pr-customer-env] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
0,"[pr-cp-reg-10016 - default] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app aris-kube-prometheus-stack-grafana is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container delay in pod efs-csi-controller.
3,[pr-cp-reg-10045 - kube-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10050 - aris-kube-prometheus-stack] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-prometheus-stack/pr-cp-reg-10011 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10025 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10007 - aris-kube-downscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10037 - aris-sealed-secrets] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10017 - aris-spot] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10010 - aris-nginx-ingress] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme7n1"" changing its up status often on node-exporter management/argocd-redis-ha-haproxy"
3,[pr-cp-reg-10004 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API cache/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to victorops sent from any instance in the kube-state-metrics cluster is .
3,[pr-cp-reg-10005 - management] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme4n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10041 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10031 - pr-customer-env-spark] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10042 - aris-sealed-secrets] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10041 - pr-customer-env] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/octopus-0 (processengine) is restarting  times / 5 minutes.
1,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_k8s_node_pid_pressure - ip-10-0-143-220.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10015 - aris-sealed-secrets] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,"[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana will run full within 24 hours."
3,[pr-cp-reg-10031 - aris-keda] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10045 - aris-kube-downscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10005 - kasten-io] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10047 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10031 - basic-application-bootstrapping] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the kubelet/aris-kube-prometheus-stack-prometheus-node-exporter targets in aris-kube-prometheus-stack namespace are down.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter management/cert-manager-795fd7b44f-f5hrn"
1,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10028 cluster.
3,[pr-cp-reg-10022 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controller in pod dashboardbff-svc.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod simulation-0.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod backup-tenants-28125915-9ps25.
3,[pr-cp-reg-10039 - aris-nginx-ingress] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10014 - aris-keda] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10038 - aris-kube-downscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-downscaler/exported_job failed to complete.
3,"[pr-cp-reg-10035 - pr-customer-env-spark] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container collaboration in pod tm-0.
3,"[pr-cp-reg-10044 - aris-spot] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf-0 (admintools) is restarting  times / 5 minutes.
3,[pr-cp-reg-10024 - basic-application-bootstrapping] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10016 - kasten-io] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10006 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10010 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/octopus (get-zone) is restarting  times / 5 minutes.
1,[pr-cp-reg-10029 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10008 - aris-sealed-secrets] - CPUThrottlingHigh -  throttling of CPU in namespace aris-sealed-secrets for container controller in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10028 - pr-customer-env] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
4,[pr-cp-reg-10011 - aris-keda] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-keda/exported_job failed to complete.
0,"[pr-cp-reg-10032 - management] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10035 - aris-nginx-ingress] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10003 - aris-kube-downscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10031 - falcon-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10014 - kube-system] - KubeQuotaAlmostFull - Namespace kube-system is using  of its memory quota.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to wechat sent from any instance in the aris-kube-prometheus-stack-operator cluster is .
1,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env-spark/pr-cp-reg-10006 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app data-volume-zookeeper-0 is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10005 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme6n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10041 - aris-keda] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10008 - pr-customer-env] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10008 cluster.
0,"[pr-cp-reg-10006 - aris-sealed-secrets] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10017 - aris-keda] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10005 - pr-customer-env] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10005 cluster.
3,"[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ConfigReloaderSidecarErrors - Errors encountered while the prometheus-aris-kube-prometheus-stack-prometheus config-reloader sidecar attempts to sync config in aris-kube-prometheus-stack namespace. As a result, configuration for service running in prometheus-aris-kube-prometheus-stack-prometheus may be stale and cannot be updated anymore."
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/serviceenabling"
1,[pr-cp-reg-10009 - pr-customer-env] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10001 - default] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tm in pod simulation.
3,[pr-cp-reg-10019 - pr-customer-env] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on nvme1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface eth2 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10028 - aris-keda] - ARIS_postgresql_down - The postgres pod of namespace aris-keda is down.
1,[pr-cp-reg-10039 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding-0 (cloudsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10035 - pr-customer-env] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-sealed-secrets/pr-cp-reg-10001 cluster is .
1,[pr-cp-reg-10006 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10028 - aris-kube-prometheus-stack] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-prometheus-stack is down.
3,"[pr-cp-reg-10008 - aris-nginx-ingress] - ConfigReloaderSidecarErrors - Errors encountered while the aris-nginx-ingress-ingress-nginx-controller config-reloader sidecar attempts to sync config in aris-nginx-ingress namespace. As a result, configuration for service running in aris-nginx-ingress-ingress-nginx-controller may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana-sc-datasources in pod prometheus-aris-kube-prometheus-stack-prometheus.
3,[pr-cp-reg-10042 - aris-cluster-autoscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-cluster-autoscaler has too many active connections. More than 90% of available connections are used.
4,[pr-cp-reg-10039 - aris-spot] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot has faced > 5 deadlocks in last minute.
4,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-nginx-ingress has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10016 - kasten-io] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10005 - aris-keda] - ConfigReloaderSidecarErrors - Errors encountered while the keda-operator-5867cd94d9-vhxdf config-reloader sidecar attempts to sync config in aris-keda namespace. As a result, configuration for service running in keda-operator-5867cd94d9-vhxdf may be stale and cannot be updated anymore."
3,[pr-cp-reg-10024 - aris-nginx-ingress] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10007 - aris-kube-downscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-downscaler for container controller in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,"[pr-cp-reg-10002 - aris-keda] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-keda/keda-operator-metrics-apiserver-7c746565cb-t6krb"
3,[pr-cp-reg-10019 - falcon-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
2,[pr-cp-reg-10031 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10031 cluster may be low for segment merges.
3,[pr-cp-reg-10010 - aris-nginx-ingress] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10014 - pr-customer-env-spark] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10003 - pr-customer-env-spark] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env-spark for container kube-state-metrics in pod pr-customer-env-spark-spark-operat-wh-init-hwzrg.
4,[pr-cp-reg-10022 - aris-keda] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-keda has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10038 - aris-keda] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-keda has too many active connections. More than 90% of available connections are used.
4,[pr-cp-reg-10023 - kube-system] - KubeQuotaFullyUsed - Namespace kube-system is using  of its cpu quota.
3,[pr-cp-reg-10050 - aris-kube-downscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-prometheus-stack has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10036 - falcon-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container state-svc in pod executor-svc-5fbdbbc689-7nnch.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10007 - kube-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10046 - kube-public] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10033 - aris-nginx-ingress] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod management-external-dns-65bfcbd6d7-zlfjj.
3,"[pr-cp-reg-10045 - aris-kube-downscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app mining-src-data is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface eth1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10047 - aris-kube-downscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10024 - aris-spot-metrics] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10009 - aris-keda] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10018 - management] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10011 - aris-sealed-secrets] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10026 - kube-system] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10035 - aris-nginx-ingress] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10030 - aris-kube-downscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-downscaler is down.
3,[pr-cp-reg-10012 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme6n1"" changing its up status often on node-exporter management/argocd-redis-6645d4fb89-kpv95"
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod adsadmin-0.
0,"[pr-cp-reg-10044 - aris-kube-prometheus-stack] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-sealed-secrets has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10047 - aris-keda] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app Amazon Linux is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-147-93.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10007 - aris-spot] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod processengine-0.
1,[pr-cp-reg-10001 - kube-public] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container log-backup in pod loadbalancer-0.
1,[pr-cp-reg-10032 - aris-cluster-autoscaler] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to pagerduty sent from any instance in the aris-kube-prometheus-stack-grafana cluster is .
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10034 - aris-keda] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10003 - kube-system] - KubeNodeReadinessFlapping - The readiness status of node ip-10-0-136-224.eu-central-1.compute.internal has changed  times in the last 15 minutes.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface tmpfs has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10045 - aris-kube-downscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10031 - pr-customer-env] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10030 - aris-keda] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-keda/pr-cp-reg-10030 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10002 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container garbagecollector-svc in pod aggregatedapis-svc-5c7ccfdd78-slnk8.
3,[pr-cp-reg-10005 - aris-keda] - ARIS_argocd_app_out_of_sync - The ArgoCD app 79cd6a440adbed21dbd5f26526d771b9c008e326b136813e4a930cd85d28d9bf is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10035 - aris-cluster-autoscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10011 - pr-customer-env] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10028 - aris-sealed-secrets] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10007 - aris-cluster-autoscaler] - ARIS_argocd_app_progressing - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10021 - pr-customer-env] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10016 - aris-nginx-ingress] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod ebs-csi-controller-86cb997fdc-bs5bs.
3,[pr-cp-reg-10043 - aris-kube-downscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10031 - aris-nginx-ingress] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10049 - kube-node-lease] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cdf in pod cdf.
3,[pr-cp-reg-10039 - aris-spot] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10029 - pr-customer-env-spark] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-registry-secret is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on nvme1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10046 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10004 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10007 - kasten-io] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10042 - aris-kube-downscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10027 - aris-spot-metrics] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10034 - aris-nginx-ingress] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-nginx-ingress/exported_job failed to complete.
1,[pr-cp-reg-10021 - kube-system] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10031 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10045 - aris-kube-downscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10010 - pr-customer-env-spark] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-147-93.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on eni006a31b57f1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod argocd-redis-ha-server-2.
3,[pr-cp-reg-10011 - aris-kube-downscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container kube-state-metrics in pod efs-csi-node-4bvqz.
3,[pr-cp-reg-10016 - aris-keda] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics config-reloader sidecar attempts to sync config in aris-kube-prometheus-stack namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics may be stale and cannot be updated anymore."
3,[pr-cp-reg-10036 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10049 - aris-kube-prometheus-stack] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10047 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
2,[pr-cp-reg-10022 - aris-keda] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10022 cluster.
1,[pr-cp-reg-10008 - pr-customer-env] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container auth-svc in pod frontend-svc-7f5bd48b8-99m2g.
0,"[pr-cp-reg-10026 - kasten-io] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10023 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_argocd_app_progressing - The ArgoCD app aris-solutionsgallery-secret is progressing for longer than 15 minutes and requires manual intervention.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""nvme0"" changing its up status often on node-exporter aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus"
3,[pr-cp-reg-10006 - management] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10011 - falcon-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10008 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env/exported_job failed to complete.
3,[pr-cp-reg-10036 - aris-spot] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10018 - aris-spot-metrics] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot-metrics has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10038 - aris-keda] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-keda/pr-cp-reg-10038 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10050 - pr-customer-env-spark] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10033 - aris-kube-downscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10019 - aris-keda] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
0,"[pr-cp-reg-10048 - aris-kube-downscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env-spark/pr-customer-env-spark-spark-operator-74d54645cb-5gpbl (kube-state-metrics) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_restarted - The pod aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10035 - aris-nginx-ingress] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10037 - aris-keda] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-keda has faced > 5 deadlocks in last minute.
2,[pr-cp-reg-10024 - aris-spot] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10024 cluster.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10027 - pr-customer-env-spark] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10035 - aris-keda] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10011 - pr-customer-env] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10012 - aris-sealed-secrets] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10046 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter management/argocd-server-74fbd754cb-n2sgr"
3,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10010 - pr-customer-env-spark] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10029 - aris-nginx-ingress] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_suspended - CronJob pr-customer-env/backup-tenants is suspended.
3,[pr-cp-reg-10024 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 1h to complete.
3,[pr-cp-reg-10016 - kube-node-lease] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10030 - default] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in default namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10043 - aris-kube-downscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,"[pr-cp-reg-10004 - aris-nginx-ingress] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4"
3,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
0,"[pr-cp-reg-10028 - aris-spot] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10033 - aris-nginx-ingress] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10004 - aris-nginx-ingress] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/octopus (tenant-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10028 - kube-public] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-sealed-secrets/exported_job failed to complete.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod ces.
1,[pr-cp-reg-10027 - aris-keda] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10035 - kube-public] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10026 - pr-customer-env-spark] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-147-93.eu-central-1.compute.internal in pr-customer-env-spark/pr-cp-reg-10026 cluster is .
1,[pr-cp-reg-10021 - aris-nginx-ingress] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10019 - aris-spot] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter management/argocd-server"
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kanister-sidecar in pod adsadmin.
1,[pr-cp-reg-10002 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - ARIS_argocd_app_progressing - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10002 - aris-spot] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10026 - aris-keda] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-138-163.eu-central-1.compute.internal in aris-kube-prometheus-stack/pr-cp-reg-10001 cluster is .
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper (abs) is restarting  times / 5 minutes.
3,[pr-cp-reg-10036 - management] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10031 - aris-cluster-autoscaler] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics has only found  members of the aris-kube-prometheus-stack-operator cluster.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter kasten-io/catalog-svc-bf949fc65-9cblt"
3,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to victorops sent from any instance in the node-exporter cluster is .
1,[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_k8s_pod_restarted - The pod pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/elasticsearch-default (cloudsearch) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod ebs-csi-controller-86cb997fdc-bs5bs.
3,[pr-cp-reg-10002 - falcon-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
2,[pr-cp-reg-10016 - aris-kube-prometheus-stack] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10016 cluster.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-registry-secret is out of sync for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - aris-keda] - CPUThrottlingHigh -  throttling of CPU in namespace aris-keda for container keda-operator in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10009 - aris-cluster-autoscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf config-reloader sidecar attempts to sync config in aris-cluster-autoscaler namespace. As a result, configuration for service running in aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf may be stale and cannot be updated anymore."
3,[pr-cp-reg-10026 - aris-cluster-autoscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10003 - pr-customer-env-spark] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env-spark for container main in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10023 - pr-customer-env-spark] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env-spark has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10003 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10044 - aris-kube-downscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-143-220.eu-central-1.compute.internal in aris-nginx-ingress/pr-cp-reg-10014 cluster is .
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer (abs) is restarting  times / 5 minutes.
3,[pr-cp-reg-10014 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10002 - kube-system] - KubeNodeReadinessFlapping - The readiness status of node ip-10-0-136-224.eu-central-1.compute.internal has changed  times in the last 15 minutes.
1,[pr-cp-reg-10002 - kube-system] - KubeletServerCertificateExpiration - Server certificate for Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal expires in .
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine-0 (collaboration) is restarting  times / 5 minutes.
1,[pr-cp-reg-10050 - aris-kube-downscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10004 - pr-customer-env-spark] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - management] - NodeRAIDDegraded - RAID array '/dev/nvme3n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_argocd_app_unknown - The ArgoCD app aris-secret-s3 is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10031 - pr-customer-env] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface nvme3 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on eth1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod collaboration-0.
1,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_k8s_pod_restarted - The pod aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10019 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container logging-svc in pod frontend-svc-7f5bd48b8-99m2g.
3,[pr-cp-reg-10041 - aris-cluster-autoscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10036 - aris-kube-downscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - AlertmanagerFailedReload - Configuration has failed to load for aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana.
4,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-cluster-autoscaler/exported_job failed to complete.
1,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10014 - pr-customer-env] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10038 - aris-kube-prometheus-stack] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
2,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10012 cluster.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in pr-customer-env/pr-cp-reg-10004 cluster is .
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_pod_crash_looping - Pod aris-nginx-ingress/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10001 - aris-keda] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-keda/pr-cp-reg-10001 cluster is .
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app annotationComments is out of sync for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""nvme3"" changing its up status often on node-exporter aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env-spark/pr-cp-reg-10043 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10047 - aris-sealed-secrets] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container dashboarding in pod abs-0.
3,[pr-cp-reg-10006 - kube-system] - KubeletClientCertificateRenewalErrors - Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal has failed to renew its client certificate ( errors in the last 5 minutes).
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container catalog-svc in pod metering-svc.
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10042 - kube-public] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
1,[pr-cp-reg-10025 - aris-nginx-ingress] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10008 - kube-system] - KubeNodeUnreachable - ip-10-0-136-224.eu-central-1.compute.internal is unreachable and some workloads may be rescheduled.
0,"[pr-cp-reg-10016 - management] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10032 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10017 - aris-nginx-ingress] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10036 - aris-nginx-ingress] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10019 - aris-cluster-autoscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10014 - aris-spot] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10015 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_takes_too_long - CronJob pr-customer-env/backup-logs is taking more than 1h to complete.
3,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10011 - kube-system] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10003 - aris-cluster-autoscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-cluster-autoscaler for container controller in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10003 - aris-kube-downscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
0,"[pr-cp-reg-10049 - kube-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10018 - falcon-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ConfigReloaderSidecarErrors - Errors encountered while the alertmanager-aris-kube-prometheus-stack-alertmanager-0 config-reloader sidecar attempts to sync config in aris-kube-prometheus-stack namespace. As a result, configuration for service running in alertmanager-aris-kube-prometheus-stack-alertmanager-0 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10017 - aris-cluster-autoscaler] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on eni0039e8a4ad3 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10039 - aris-keda] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10006 - aris-sealed-secrets] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-sealed-secrets/pr-cp-reg-10006 cluster is .
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container upgrade-init in pod crypto-svc.
3,[pr-cp-reg-10001 - kube-system] - KubeNodeReadinessFlapping - The readiness status of node ip-10-0-138-163.eu-central-1.compute.internal has changed  times in the last 15 minutes.
4,[pr-cp-reg-10001 - aris-kube-downscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-downscaler for container controller in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10046 - management] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
2,[pr-cp-reg-10016 - pr-customer-env] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10016 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/elasticsearch-default-0 (kanister-sidecar) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container create-es-index-template in pod zookeeper-0.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container jobs-svc in pod metering-svc.
3,[pr-cp-reg-10029 - aris-kube-downscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10049 - aris-sealed-secrets] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-solutionsgallery-secret is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_pod_crash_looping - Pod aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-hwdv6 (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10041 - pr-customer-env] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10031 - pr-customer-env] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10007 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - aris-spot] - ARIS_postgresql_down - The postgres pod of namespace aris-spot is down.
3,[pr-cp-reg-10041 - kube-node-lease] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10020 - pr-customer-env] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
0,"[pr-cp-reg-10007 - aris-kube-prometheus-stack] - Watchdog - This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the ""DeadMansSnitch"" integration in PagerDuty."
3,[pr-cp-reg-10027 - kube-node-lease] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10032 - aris-keda] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10001 - aris-spot] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10020 - pr-customer-env-spark] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10011 - pr-customer-env] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10009 - aris-spot-metrics] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod ces-0.
3,[pr-cp-reg-10014 - kube-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on eth1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10015 - aris-nginx-ingress] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10018 - aris-keda] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-keda has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10010 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10007 cluster may be low for segment merges.
2,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/enhancements is full (< 5% free space left) and should be increased.
2,[pr-cp-reg-10034 - aris-cluster-autoscaler] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10034 cluster.
1,[pr-cp-reg-10009 - pr-customer-env] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app alertingBigTransactions is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10020 - default] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
2,[pr-cp-reg-10005 - pr-customer-env] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10005 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left.
2,[pr-cp-reg-10024 - aris-spot] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10024 cluster.
3,[pr-cp-reg-10049 - aris-cluster-autoscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
4,[pr-cp-reg-10002 - pr-customer-env-spark] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env-spark for container spark-operator in pod pr-customer-env-spark-spark-operator-74d54645cb-5gpbl.
1,[pr-cp-reg-10015 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_k8s_node_memory_pressure - ip-10-0-147-93.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10023 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod efs-csi-controller.
1,[pr-cp-reg-10016 - kube-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10017 - aris-kube-downscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10006 - aris-spot] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-spot/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10011 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
1,[pr-cp-reg-10046 - aris-keda] - ARIS_postgresql_down - The postgres pod of namespace aris-keda is down.
3,[pr-cp-reg-10017 - falcon-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10042 - aris-nginx-ingress] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10026 - aris-sealed-secrets] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-sealed-secrets/exported_job failed to complete.
0,"[pr-cp-reg-10050 - kasten-io] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/register-smtp-server-6d78c-stqpc (processengine) is restarting  times / 5 minutes.
3,[pr-cp-reg-10042 - pr-customer-env] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10003 - kube-system] - KubeletPlegDurationHigh - The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of  seconds on node ip-10-0-136-224.eu-central-1.compute.internal.
1,[pr-cp-reg-10026 - aris-sealed-secrets] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10026 cluster.
3,[pr-cp-reg-10039 - pr-customer-env-spark] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env-spark has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10023 - aris-keda] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10007 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10005 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to wechat sent from any instance in the aris-kube-prometheus-stack-grafana cluster is .
3,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10040 - aris-sealed-secrets] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_k8s_pod_restarted - The pod aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10010 - kube-public] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10015 - pr-customer-env-spark] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg"
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-svc in pod jobs-svc-685557c545-4mnxp.
3,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-cluster-autoscaler/pr-cp-reg-10006 cluster is .
3,[pr-cp-reg-10032 - aris-kube-downscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-operator-78b758c475-qs69v failed to send  of notifications to opsgenie.
3,[pr-cp-reg-10050 - pr-customer-env-spark] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface nvme1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10046 - aris-sealed-secrets] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10014 - aris-cluster-autoscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10041 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot-metrics/pr-cp-reg-10041 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10004 - aris-kube-downscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10002 - aris-keda] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10045 - aris-cluster-autoscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10018 - aris-keda] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10039 - aris-sealed-secrets] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10032 - pr-customer-env-spark] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env-spark has too many active connections. More than 90% of available connections are used.
2,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10044 cluster may be low for segment merges.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/octopus (loadbalancer) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod kanister-job-7lbrs.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on eni0039e8a4ad3 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_k8s_node_pid_pressure - ip-10-0-147-93.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10021 - aris-nginx-ingress] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10007 - aris-keda] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10002 - aris-cluster-autoscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-cluster-autoscaler for container aws-cluster-autoscaler in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
1,[pr-cp-reg-10036 - aris-sealed-secrets] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10046 - aris-kube-prometheus-stack] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-prometheus-stack has too many active connections. More than 90% of available connections are used.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controller in pod kanister-svc.
0,"[pr-cp-reg-10035 - aris-kube-downscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
4,[pr-cp-reg-10002 - falcon-system] - CPUThrottlingHigh -  throttling of CPU in namespace falcon-system for container init-falconstore in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10015 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
1,[pr-cp-reg-10019 - pr-customer-env-spark] - ARIS_k8s_node_memory_pressure - ip-10-0-147-93.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on eni006a31b57f1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eni0039e8a4ad3"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-822lp"
3,[pr-cp-reg-10009 - aris-keda] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - AlertmanagerConfigInconsistent - Alertmanager instances within the aris-kube-prometheus-stack-prometheus cluster have different configurations.
0,"[pr-cp-reg-10022 - aris-sealed-secrets] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod argocd-redis-ha-server.
3,[pr-cp-reg-10045 - aris-spot] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot/pr-cp-reg-10045 health status has been YELLOW for at least 20 minutes.
3,"[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10031 - aris-nginx-ingress] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10032 - pr-customer-env] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_k8s_pod_crash_looping - Pod aris-kube-downscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (kube-state-metrics) is restarting  times / 5 minutes.
1,[pr-cp-reg-10008 - pr-customer-env] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_postgresql_down - The postgres pod of namespace aris-sealed-secrets is down.
3,[pr-cp-reg-10010 - kasten-io] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10036 - kasten-io] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10045 - pr-customer-env-spark] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10009 - aris-nginx-ingress] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_argocd_app_out_of_sync - The ArgoCD app 96ff84e25e745ef28b97b02b2ccf87ca2cb688b9ac0a8c41bb217d4cb18ccddb is out of sync for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tm in pod ces.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container dashboardbff-svc in pod dashboardbff-svc-97dc4585b-n64dn.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver (zookeeper) is restarting  times / 5 minutes.
2,[pr-cp-reg-10012 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10012 cluster.
3,[pr-cp-reg-10040 - aris-kube-downscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env-spark has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_k8s_pod_crash_looping - Pod aris-nginx-ingress/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (kube-state-metrics) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/ces-0"
1,[pr-cp-reg-10033 - aris-spot-metrics] - ARIS_postgresql_down - The postgres pod of namespace aris-spot-metrics is down.
1,[pr-cp-reg-10042 - aris-spot] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot/pr-cp-reg-10042 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10039 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10005 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API loadbalance/kube-system has been only % available over the last 10m.
3,[pr-cp-reg-10044 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10013 - falcon-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10015 - pr-customer-env] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10027 - management] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10031 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env/exported_job failed to complete.
1,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container ces in pod umcadmin.
1,[pr-cp-reg-10035 - aris-keda] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10024 - aris-kube-downscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10044 - pr-customer-env] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container auth-svc in pod controllermanager-svc.
3,[pr-cp-reg-10037 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-sealed-secrets/pr-cp-reg-10008 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10042 - aris-keda] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
3,"[pr-cp-reg-10004 - kube-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kube-system/aws-node-9qqfk"
3,[pr-cp-reg-10028 - aris-kube-prometheus-stack] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28124475-fjqvp (prometheus-exporter) is restarting  times / 5 minutes.
3,[pr-cp-reg-10035 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10039 - default] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_postgresql_down - The postgres pod of namespace aris-nginx-ingress is down.
3,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
2,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10004 cluster.
3,[pr-cp-reg-10001 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme5n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10005 - kube-system] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
1,[pr-cp-reg-10006 - aris-keda] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10030 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k failed to send  of notifications to pushover.
3,[pr-cp-reg-10028 - aris-keda] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container get-zone in pod tm.
3,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,"[pr-cp-reg-10026 - aris-cluster-autoscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10016 - aris-cluster-autoscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10011 - aris-spot] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot/pr-cp-reg-10011 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10007 - pr-customer-env] - NodeRAIDDegraded - RAID array '/dev/nvme4n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10024 - aris-keda] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container log-backup in pod ces.
1,[pr-cp-reg-10032 - aris-kube-prometheus-stack] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10021 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-svc in pod controllermanager-svc.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-1 has only found  members of the aris-kube-prometheus-stack-grafana cluster.
3,"[pr-cp-reg-10040 - default] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in default namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,"[pr-cp-reg-10007 - aris-keda] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10033 - aris-cluster-autoscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10032 - pr-customer-env] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-downscaler/exported_job failed to complete.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10039 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver-0 (cdf) is restarting  times / 5 minutes.
3,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-nginx-ingress/pr-cp-reg-10048 health status has been YELLOW for at least 20 minutes.
3,"[pr-cp-reg-10008 - aris-keda] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-keda/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env/backup-tenants-28124475 failed to complete.
3,"[pr-cp-reg-10012 - basic-application-bootstrapping] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in basic-application-bootstrapping namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_k8s_node_memory_pressure - ip-10-0-147-93.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10024 - aris-keda] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod processboard.
1,[pr-cp-reg-10021 - aris-keda] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-keda/pr-cp-reg-10021 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container aggregatedapis-svc in pod jobs-svc.
1,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10033 cluster.
3,[pr-cp-reg-10028 - pr-customer-env-spark] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10021 - pr-customer-env-spark] - ARIS_argocd_app_out_of_sync - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10009 - aris-keda] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_k8s_node_out_of_disk - ip-10-0-143-220.eu-central-1.compute.internal has OutOfDisk condition.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter management/ebs-csi-node"
3,[pr-cp-reg-10050 - aris-sealed-secrets] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-sealed-secrets has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10027 - aris-sealed-secrets] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-sealed-secrets/pr-cp-reg-10027 cluster is .
3,[pr-cp-reg-10031 - aris-kube-prometheus-stack] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
2,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10005 cluster may be low for segment merges.
3,[pr-cp-reg-10013 - kube-system] - KubeNodeReadinessFlapping - The readiness status of node ip-10-0-138-163.eu-central-1.compute.internal has changed  times in the last 15 minutes.
3,[pr-cp-reg-10017 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processboard-0 (abs) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10018 - kube-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10046 - aris-cluster-autoscaler] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
2,[pr-cp-reg-10007 - aris-spot] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10007 cluster.
3,[pr-cp-reg-10043 - kube-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10016 - aris-kube-prometheus-stack] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10013 - aris-sealed-secrets] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10034 - aris-keda] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/serviceenabling-0 (serviceenabling) is restarting  times / 5 minutes.
3,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_cluster_autoscaler_unschedulable_pods - The cluster autoscaler is unable to scale up and there are unschedulable pods because of this condition.
3,[pr-cp-reg-10044 - pr-customer-env] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10008 - aris-cluster-autoscaler] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10007 - aris-spot-metrics] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot-metrics has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10017 - aris-nginx-ingress] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10012 - management] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10034 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10008 - aris-spot] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10047 - aris-keda] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10048 - aris-keda] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
4,[pr-cp-reg-10015 - kube-system] - KubeQuotaAlmostFull - Namespace kube-system is using  of its memory quota.
3,[pr-cp-reg-10030 - kube-public] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10011 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
4,[pr-cp-reg-10042 - pr-customer-env-spark] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env-spark has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10005 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10021 - aris-spot-metrics] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""nvme3"" changing its up status often on node-exporter aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus"
3,[pr-cp-reg-10020 - aris-keda] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
2,[pr-cp-reg-10002 - aris-keda] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10002 cluster.
1,[pr-cp-reg-10009 - aris-cluster-autoscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10007 - kube-system] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
4,[pr-cp-reg-10015 - pr-customer-env] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10046 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10048 - pr-customer-env-spark] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod ebs-csi-controller.
2,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10045 cluster may be low for segment merges.
3,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
2,[pr-cp-reg-10032 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10032 cluster may be low for segment merges.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod argocd-redis-ha-server-2.
3,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
0,"[pr-cp-reg-10012 - aris-sealed-secrets] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on nvme2 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10002 - aris-kube-downscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container portalserver in pod kanister-job-7lbrs.
1,[pr-cp-reg-10044 - aris-keda] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10038 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-nginx-ingress/pr-cp-reg-10038 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10040 - kube-node-lease] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_k8s_pod_restarted - The pod aris-kube-downscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
0,"[pr-cp-reg-10012 - aris-keda] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10002 - management] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10007 - kube-system] - KubeletPlegDurationHigh - The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of  seconds on node ip-10-0-139-113.eu-central-1.compute.internal.
3,[pr-cp-reg-10045 - aris-keda] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container aws-node in pod coredns-cbbbbb9cb-djqr5.
1,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-cluster-autoscaler/pr-cp-reg-10044 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10026 - kube-node-lease] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10009 - pr-customer-env] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the aris-kube-prometheus-stack-prometheus/sealed-secrets-controller targets in aris-kube-prometheus-stack namespace are down.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter kasten-io/auth-svc-97bd685df-phk9x"
3,[pr-cp-reg-10040 - aris-nginx-ingress] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10013 - aris-keda] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod efs-csi-node-2r2zp.
3,[pr-cp-reg-10015 - kasten-io] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10040 - aris-kube-downscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10031 - pr-customer-env] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-repo-server in pod argocd-dex-server-6b74fb9695-kbk62.
1,[pr-cp-reg-10025 - pr-customer-env-spark] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10025 cluster.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container dex in pod efs-csi-node-2r2zp.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on tmpfs at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10046 - aris-sealed-secrets] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10049 - aris-kube-downscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10032 - aris-kube-downscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10009 cluster.
3,"[pr-cp-reg-10001 - aris-nginx-ingress] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-hwdv6"
1,[pr-cp-reg-10001 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme5n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10044 - pr-customer-env-spark] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10023 - falcon-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_argocd_app_unknown - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10035 - aris-keda] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10035 cluster.
1,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-prometheus-stack is down.
1,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10023 - aris-keda] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-keda/exported_job failed to complete.
3,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10002 - kube-system] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10013 - default] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container adsadmin in pod tm.
3,[pr-cp-reg-10025 - pr-customer-env] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_host_high_cpu_load - CPU load is > 90%.
2,[pr-cp-reg-10025 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10025 cluster may be low for segment merges.
3,[pr-cp-reg-10049 - aris-kube-downscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10035 - pr-customer-env-spark] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10005 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10026 - aris-kube-prometheus-stack] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tm in pod loadbalancer.
3,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-cluster-autoscaler/pr-cp-reg-10001 cluster is .
3,[pr-cp-reg-10050 - aris-sealed-secrets] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-sealed-secrets/pr-cp-reg-10050 cluster is .
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-cluster-autoscaler/pr-cp-reg-10038 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10006 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10041 - aris-spot] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
2,[pr-cp-reg-10018 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10018 cluster.
3,[pr-cp-reg-10026 - aris-nginx-ingress] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container state-svc in pod metering-svc-685b59dfb-pcknm.
3,[pr-cp-reg-10048 - aris-sealed-secrets] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10044 - kube-public] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10011 - kube-system] - KubeletTooManyPods - Kubelet 'ip-10-0-138-163.eu-central-1.compute.internal' is running at  of its Pod capacity.
1,[pr-cp-reg-10047 - kube-public] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10018 - kube-node-lease] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
1,[pr-cp-reg-10050 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10025 - aris-keda] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,"[pr-cp-reg-10012 - pr-customer-env-spark] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg"
1,[pr-cp-reg-10025 - basic-application-bootstrapping] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10008 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs (dashboarding) is restarting  times / 5 minutes.
4,[pr-cp-reg-10007 - kube-node-lease] - CPUThrottlingHigh -  throttling of CPU in namespace kube-node-lease for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10049 - aris-sealed-secrets] - ARIS_sealedsecret_not_unsealed - There are secrets that could not be unsealed in last 30 minutes. Check the sealing key.
3,[pr-cp-reg-10012 - aris-sealed-secrets] - ARIS_argocd_app_unknown - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is in unknown state for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kasten-io/frontend-svc"
3,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10016 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10006 - kube-system] - KubeletClientCertificateExpiration - Client certificate for Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal expires in .
1,[pr-cp-reg-10011 - aris-keda] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10014 - kube-node-lease] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
3,[pr-cp-reg-10024 - aris-kube-prometheus-stack] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10046 - pr-customer-env-spark] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10013 - falcon-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration-0 (loadbalancer) is restarting  times / 5 minutes.
3,[pr-cp-reg-10045 - aris-cluster-autoscaler] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10011 - aris-kube-downscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-kube-downscaler namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
2,[pr-cp-reg-10012 - aris-cluster-autoscaler] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10012 cluster.
3,[pr-cp-reg-10002 - aris-keda] - ARIS_argocd_app_unknown - The ArgoCD app bf4949571e1d348960cfaea8d505c51028858bed802f8e8fe13fc4ea5ab020a1 is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10033 - aris-cluster-autoscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf (tenant-backup) is restarting  times / 5 minutes.
3,[pr-cp-reg-10049 - pr-customer-env] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10038 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10008 - aris-keda] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'eni006a31b57f1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10023 - aris-nginx-ingress] - ARIS_postgresql_down - The postgres pod of namespace aris-nginx-ingress is down.
1,[pr-cp-reg-10045 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
0,"[pr-cp-reg-10003 - kube-node-lease] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10008 - aris-kube-downscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10011 - pr-customer-env] - ARIS_k8s_node_pid_pressure - ip-10-0-138-163.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container config-init in pod ae-aw-euc1-15995-aws-load-balancer-controller-7895559794-c8nd5.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/cert-manager-795fd7b44f-f5hrn"
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/backup-logs-28124160-s7ps9"
3,"[pr-cp-reg-10012 - basic-application-bootstrapping] - ConfigReloaderSidecarErrors - Errors encountered while the basic-application-bootstrapping-2wp59 config-reloader sidecar attempts to sync config in basic-application-bootstrapping namespace. As a result, configuration for service running in basic-application-bootstrapping-2wp59 may be stale and cannot be updated anymore."
1,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter kasten-io/crypto-svc"
3,[pr-cp-reg-10048 - kube-node-lease] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
1,[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_k8s_pod_crash_looping - Pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10008 - kube-node-lease] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-0 (kube-prometheus-stack) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (processengine) is restarting  times / 5 minutes.
2,[pr-cp-reg-10023 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10023 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-6kclt (adsadmin) is restarting  times / 5 minutes.
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10002 cluster.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10009 - aris-keda] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-keda/pr-cp-reg-10009 cluster is .
1,[pr-cp-reg-10006 - aris-sealed-secrets] - ARIS_k8s_pod_restarted - The pod aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10018 - kube-public] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,"[pr-cp-reg-10004 - aris-cluster-autoscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-cluster-autoscaler-aws-cluster-autoscaler config-reloader sidecar attempts to sync config in aris-cluster-autoscaler namespace. As a result, configuration for service running in aris-cluster-autoscaler-aws-cluster-autoscaler may be stale and cannot be updated anymore."
2,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10010 cluster.
3,[pr-cp-reg-10038 - aris-kube-downscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-downscaler has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_k8s_pod_crash_looping - Pod aris-cluster-autoscaler/aris-cluster-autoscaler-aws-cluster-autoscaler (aws-cluster-autoscaler) is restarting  times / 5 minutes.
1,[pr-cp-reg-10010 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10035 - aris-nginx-ingress] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10029 - aris-cluster-autoscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-cluster-autoscaler/exported_job failed to complete.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container setmaxmapcount in pod abs.
2,[pr-cp-reg-10006 - aris-keda] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10006 cluster may be low for segment merges.
3,[pr-cp-reg-10029 - aris-keda] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
0,"[pr-cp-reg-10033 - aris-kube-prometheus-stack] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elastic-metrics-sidecar in pod processboard-0.
1,[pr-cp-reg-10003 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10038 - aris-kube-prometheus-stack] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
4,[pr-cp-reg-10038 - aris-nginx-ingress] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-nginx-ingress/exported_job failed to complete.
3,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10013 - aris-keda] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10017 - pr-customer-env-spark] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10016 - kube-node-lease] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10035 - aris-kube-downscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod processboard-0.
3,[pr-cp-reg-10013 - kube-public] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10002 - kube-public] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_argocd_app_unknown - The ArgoCD app 705ccec2911bb44176bb5a664bb99d8cb0e9ff09e31f6355ef29837c92560a00 is in unknown state for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10002 - falcon-system] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in falcon-system namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10018 - aris-spot] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin-0 (setmaxmapcount) is restarting  times / 5 minutes.
3,[pr-cp-reg-10007 - aris-cluster-autoscaler] - ARIS_argocd_app_unknown - The ArgoCD app a6d67856bcc285f0f6ca4349c619dff9fba796cd94aeb13735a2f19b88abfe1e is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10047 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-sealed-secrets/pr-cp-reg-10047 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine (settcpkeepalivetime) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod adsadmin.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container admintools in pod simulation.
1,[pr-cp-reg-10044 - kube-node-lease] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10036 - aris-keda] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10036 cluster.
3,[pr-cp-reg-10026 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10008 - kube-system] - KubeletTooManyPods - Kubelet 'ip-10-0-136-224.eu-central-1.compute.internal' is running at  of its Pod capacity.
3,[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124160-s7ps9 (postgres) is restarting  times / 5 minutes.
3,[pr-cp-reg-10009 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available inodes left.
2,[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/logs is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10001 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the logging-svc config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in logging-svc may be stale and cannot be updated anymore."
3,[pr-cp-reg-10004 - kube-system] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,"[pr-cp-reg-10001 - kube-system] - ConfigReloaderSidecarErrors - Errors encountered while the coredns-cbbbbb9cb-djqr5 config-reloader sidecar attempts to sync config in kube-system namespace. As a result, configuration for service running in coredns-cbbbbb9cb-djqr5 may be stale and cannot be updated anymore."
1,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10010 cluster.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface lo has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10016 - kube-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10033 - aris-kube-downscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10024 - aris-kube-prometheus-stack] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10025 - aris-keda] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,"[pr-cp-reg-10006 - aris-nginx-ingress] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-nginx-ingress/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10032 - aris-keda] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10040 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10007 - aris-sealed-secrets] - CPUThrottlingHigh -  throttling of CPU in namespace aris-sealed-secrets for container kube-state-metrics in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10044 - aris-keda] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10046 - aris-kube-prometheus-stack] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10040 - aris-nginx-ingress] - ARIS_postgresql_down - The postgres pod of namespace aris-nginx-ingress is down.
3,"[pr-cp-reg-10001 - kube-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kube-system/kube-proxy-5lqbj"
3,[pr-cp-reg-10016 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10012 - kube-system] - KubeletClientCertificateRenewalErrors - Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal has failed to renew its client certificate ( errors in the last 5 minutes).
3,[pr-cp-reg-10008 - kube-node-lease] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
1,[pr-cp-reg-10010 - aris-keda] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app 1 is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod argocd-server-74fbd754cb-cjfrb.
1,[pr-cp-reg-10034 - aris-spot] - ARIS_postgresql_down - The postgres pod of namespace aris-spot is down.
3,[pr-cp-reg-10011 - aris-cluster-autoscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-downscaler/pr-cp-reg-10046 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10002 - aris-keda] - ARIS_k8s_pod_crash_looping - Pod aris-keda/keda-operator-metrics-apiserver-7c746565cb-t6krb (kube-state-metrics) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10006 - aris-kube-downscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-kube-downscaler namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10024 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10006 - falcon-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin-0 (adsadmin) is restarting  times / 5 minutes.
3,[pr-cp-reg-10014 - aris-sealed-secrets] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10041 - basic-application-bootstrapping] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10004 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10049 - kube-public] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10021 - aris-keda] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10023 - pr-customer-env-spark] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
0,"[pr-cp-reg-10026 - default] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod argocd-redis-ha-server.
1,[pr-cp-reg-10029 - kasten-io] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10008 - aris-kube-downscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10025 - aris-keda] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-keda/pr-cp-reg-10025 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5 (config-reloader) is restarting  times / 5 minutes.
4,[pr-cp-reg-10043 - aris-kube-prometheus-stack] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-prometheus-stack/exported_job failed to complete.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on eth1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10005 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
0,"[pr-cp-reg-10042 - management] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10008 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10013 - kasten-io] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10045 - pr-customer-env-spark] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - AlertmanagerConfigInconsistent - Alertmanager instances within the kube-state-metrics cluster have different configurations.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme7n1"" changing its up status often on node-exporter management/argocd-dex-server-6b74fb9695-kbk62"
3,[pr-cp-reg-10007 - aris-kube-downscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10024 - aris-keda] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10005 - pr-customer-env] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10045 - aris-kube-prometheus-stack] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-repo-server in pod efs-csi-node-4cgpg.
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-cluster-autoscaler/pr-cp-reg-10002 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10029 - pr-customer-env] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10036 - kube-node-lease] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
0,"[pr-cp-reg-10044 - kube-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres (serviceenabling) is restarting  times / 5 minutes.
3,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_argocd_app_progressing - The ArgoCD app 46d6ed101040cf8df2a51291aad961a76c082eba555ed2401548a98d1bb4d54d is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10040 - aris-keda] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10047 - pr-customer-env] - ARIS_host_high_cpu_load - CPU load is > 90%.
2,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/solutiondata is full (< 5% free space left) and should be increased.
4,[pr-cp-reg-10036 - aris-spot-metrics] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot-metrics for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10007 - aris-cluster-autoscaler] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10004 - aris-spot] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10004 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine-0 (kanister-sidecar) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_k8s_pod_crash_looping - Pod aris-cluster-autoscaler/sealed-secrets-controller-cbbc8bd6b-qq22k (controller) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container dashboarding in pod backup-tenants-28124475-fjqvp.
4,[pr-cp-reg-10009 - pr-customer-env-spark] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env-spark/exported_job failed to complete.
3,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app queryOverLive is out of sync for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10031 - kube-system] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10022 - aris-sealed-secrets] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod processboard.
3,[pr-cp-reg-10016 - aris-kube-downscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-downscaler has too many active connections. More than 90% of available connections are used.
0,"[pr-cp-reg-10008 - management] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10007 - kasten-io] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer-0 (processengine) is restarting  times / 5 minutes.
3,[pr-cp-reg-10035 - kube-node-lease] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer (log-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/simulation (tenant-backup) is restarting  times / 5 minutes.
3,[pr-cp-reg-10029 - aris-kube-prometheus-stack] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-prometheus-stack has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10012 - pr-customer-env-spark] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the sealed-secrets-controller/sealed-secrets-controller targets in aris-kube-prometheus-stack namespace are down.
1,[pr-cp-reg-10035 - kube-public] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10029 - pr-customer-env-spark] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10017 - aris-nginx-ingress] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod argocd-server.
4,[pr-cp-reg-10032 - pr-customer-env] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_k8s_node_out_of_disk - ip-10-0-147-93.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10010 - aris-keda] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10031 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs (tenant-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on tmpfs at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-nginx-ingress has faced > 5 deadlocks in last minute.
2,[pr-cp-reg-10019 - aris-spot] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10019 cluster may be low for segment merges.
3,"[pr-cp-reg-10001 - management] - ConfigReloaderSidecarErrors - Errors encountered while the efs-csi-node-2r2zp config-reloader sidecar attempts to sync config in management namespace. As a result, configuration for service running in efs-csi-node-2r2zp may be stale and cannot be updated anymore."
3,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-cluster-autoscaler/pr-cp-reg-10050 cluster is .
1,[pr-cp-reg-10004 - pr-customer-env-spark] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env-spark/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (spark-operator) is restarting  times / 5 minutes.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-prometheus-stack/pr-cp-reg-10011 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10022 - basic-application-bootstrapping] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10040 - aris-kube-downscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10015 - aris-kube-downscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10016 - kube-system] - KubeletClientCertificateExpiration - Client certificate for Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal expires in .
3,[pr-cp-reg-10009 - pr-customer-env] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10026 - aris-cluster-autoscaler] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10026 - aris-kube-downscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-downscaler has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10016 - aris-spot] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10027 - pr-customer-env] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10004 - kube-public] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
0,"[pr-cp-reg-10027 - aris-keda] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10012 - aris-cluster-autoscaler] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10016 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10041 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10031 - aris-nginx-ingress] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedReload - Configuration has failed to load for aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager.
1,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_cluster_autoscaler_unschedulable_pods - The cluster autoscaler is unable to scale up and there are unschedulable pods because of this condition.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-application-controller in pod ebs-csi-node-bspjp.
3,[pr-cp-reg-10050 - pr-customer-env] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k failed to send  of notifications to victorops.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-provisioner in pod argocd-redis-6645d4fb89-kpv95.
1,[pr-cp-reg-10003 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot-metrics/pr-cp-reg-10003 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod umcadmin.
1,[pr-cp-reg-10011 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10037 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on shm at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-repo-server in pod argocd-application-controller-0.
3,[pr-cp-reg-10017 - pr-customer-env] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in pr-customer-env/pr-cp-reg-10017 cluster is .
3,[pr-cp-reg-10022 - kube-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
2,[pr-cp-reg-10050 - aris-keda] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10050 cluster.
1,[pr-cp-reg-10014 - kube-node-lease] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter management/cert-manager-webhook"
0,"[pr-cp-reg-10002 - aris-spot] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
0,"[pr-cp-reg-10006 - aris-cluster-autoscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10009 - aris-kube-downscaler] - ARIS_k8s_pod_crash_looping - Pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10004 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API DynamicCABundle-client-ca-bundle/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
3,[pr-cp-reg-10014 - aris-spot-metrics] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
2,[pr-cp-reg-10015 - pr-customer-env] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10015 cluster.
3,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10017 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_suspended - CronJob pr-customer-env/backup-tenants is suspended.
4,[pr-cp-reg-10024 - aris-keda] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-keda/exported_job failed to complete.
3,[pr-cp-reg-10016 - pr-customer-env] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,"[pr-cp-reg-10001 - pr-customer-env] - ConfigReloaderSidecarErrors - Errors encountered while the portalserver-0 config-reloader sidecar attempts to sync config in pr-customer-env namespace. As a result, configuration for service running in portalserver-0 may be stale and cannot be updated anymore."
2,[pr-cp-reg-10044 - aris-kube-prometheus-stack] - ARIS_k8s_volume_out_of_disk_space - Volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
2,[pr-cp-reg-10012 - aris-spot] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10012 cluster.
3,[pr-cp-reg-10001 - aris-kube-downscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10024 - aris-kube-downscaler] - ARIS_k8s_pod_restarted - The pod aris-kube-downscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
4,[pr-cp-reg-10007 - default] - CPUThrottlingHigh -  throttling of CPU in namespace default for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10004 - kube-system] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod simulation-0.
1,[pr-cp-reg-10014 - pr-customer-env] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
2,[pr-cp-reg-10024 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10024 cluster may be low for segment merges.
1,[pr-cp-reg-10003 - aris-keda] - ARIS_k8s_pod_restarted - The pod aris-keda/keda-operator-metrics-apiserver has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10023 - pr-customer-env-spark] - ARIS_argocd_app_unknown - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is in unknown state for longer than 10 minutes and requires manual intervention.
2,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10014 cluster.
1,[pr-cp-reg-10048 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10011 - aris-kube-downscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10016 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
2,[pr-cp-reg-10019 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10019 cluster may be low for segment merges.
3,[pr-cp-reg-10037 - pr-customer-env] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-6kclt (elasticsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10045 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10031 - aris-spot] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10034 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10037 - falcon-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10038 - pr-customer-env-spark] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,"[pr-cp-reg-10001 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the executor-svc-5fbdbbc689-7nnch config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in executor-svc-5fbdbbc689-7nnch may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod processboard.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10041 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10008 - kube-system] - KubeletClientCertificateExpiration - Client certificate for Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal expires in .
3,[pr-cp-reg-10038 - pr-customer-env-spark] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10006 - falcon-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10014 - kasten-io] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10024 - aris-cluster-autoscaler] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces-0 (processengine) is restarting  times / 5 minutes.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-secret-elasticsearch is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
0,"[pr-cp-reg-10010 - pr-customer-env-spark] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_k8s_node_out_of_disk - ip-10-0-138-163.eu-central-1.compute.internal has OutOfDisk condition.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container cert-manager in pod efs-csi-node-4bvqz.
1,[pr-cp-reg-10032 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
0,"[pr-cp-reg-10046 - kube-node-lease] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod create-es-index-template-61d85-nz2dz.
3,[pr-cp-reg-10014 - aris-sealed-secrets] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - aris-keda] - ARIS_k8s_pod_crash_looping - Pod aris-keda/sealed-secrets-controller-cbbc8bd6b-qq22k (keda-operator) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics (kube-prometheus-stack) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-0 has only found  members of the node-exporter cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container ces in pod tm.
3,[pr-cp-reg-10042 - aris-spot-metrics] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot-metrics has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10016 - kube-system] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kube-state-metrics in pod aggregatedapis-svc.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-sidecar in pod metering-svc.
3,[pr-cp-reg-10020 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124160-s7ps9 (tenant-backup) is restarting  times / 5 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app lokiMonacoEditor is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-provisioner in pod argocd-redis-ha-haproxy-84b857bc4b-qd5km.
3,[pr-cp-reg-10039 - aris-sealed-secrets] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10003 - kube-system] - KubeletPodStartUpLatencyHigh - Kubelet Pod startup 99th percentile latency is  seconds on node ip-10-0-139-113.eu-central-1.compute.internal.
3,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10012 - aris-cluster-autoscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
2,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_k8s_volume_out_of_disk_space - Volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10041 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10038 - aris-spot] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10028 - aris-spot] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot/pr-cp-reg-10028 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10013 - aris-keda] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics (prometheus) is restarting  times / 5 minutes.
3,[pr-cp-reg-10044 - aris-kube-prometheus-stack] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10004 - aris-keda] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-keda/exported_job failed to complete.
3,[pr-cp-reg-10016 - aris-cluster-autoscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
0,"[pr-cp-reg-10032 - kube-public] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
0,"[pr-cp-reg-10007 - aris-cluster-autoscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10039 - aris-sealed-secrets] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_argocd_app_out_of_sync - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10008 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
2,[pr-cp-reg-10042 - aris-keda] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10042 cluster may be low for segment merges.
3,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10006 - aris-sealed-secrets] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10035 - aris-kube-downscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10028 - pr-customer-env-spark] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10018 - falcon-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme6n1"" changing its up status often on node-exporter management/ebs-csi-node-bspjp"
1,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10042 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerClusterDown -  of Alertmanager instances within the aris-kube-prometheus-stack-alertmanager cluster have been up for less than half of the last 5m.
3,[pr-cp-reg-10004 - aris-keda] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10010 - pr-customer-env] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-138-163.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10027 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10012 - kube-system] - KubeletPodStartUpLatencyHigh - Kubelet Pod startup 99th percentile latency is  seconds on node ip-10-0-138-163.eu-central-1.compute.internal.
1,[pr-cp-reg-10004 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter management/efs-csi-node"
3,[pr-cp-reg-10008 - kasten-io] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10023 - aris-nginx-ingress] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
0,"[pr-cp-reg-10039 - aris-keda] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10031 - default] - CPUThrottlingHigh -  throttling of CPU in namespace default for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10017 - aris-cluster-autoscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10021 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10025 - aris-kube-downscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/tm-0 (admintools) is restarting  times / 5 minutes.
1,[pr-cp-reg-10022 - aris-kube-downscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10018 - aris-kube-downscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-downscaler/exported_job failed to complete.
3,"[pr-cp-reg-10017 - aris-spot-metrics] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10050 - kube-public] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10048 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10048 - basic-application-bootstrapping] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter kasten-io/dashboardbff-svc"
0,"[pr-cp-reg-10024 - basic-application-bootstrapping] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (elastic-metrics-sidecar) is restarting  times / 5 minutes.
3,[pr-cp-reg-10045 - pr-customer-env-spark] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10043 - kube-node-lease] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10017 cluster.
1,[pr-cp-reg-10012 - kube-node-lease] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10025 - aris-cluster-autoscaler] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10047 - aris-keda] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10006 - kasten-io] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10017 - pr-customer-env-spark] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kanister-sidecar in pod cloudsearch.
1,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10019 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10015 - kube-system] - KubeletPodStartUpLatencyHigh - Kubelet Pod startup 99th percentile latency is  seconds on node ip-10-0-138-163.eu-central-1.compute.internal.
3,[pr-cp-reg-10003 - aris-keda] - ARIS_argocd_app_out_of_sync - The ArgoCD app clustertriggerauthentication is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/adsadmin (kanister-sidecar) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 (grafana-sc-dashboard) is restarting  times / 5 minutes.
3,[pr-cp-reg-10022 - aris-kube-downscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10004 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme6n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10026 - aris-cluster-autoscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10002 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124160-s7ps9 (tm) is restarting  times / 5 minutes.
3,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_argocd_app_progressing - The ArgoCD app tls-secret is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_node_pid_pressure - ip-10-0-143-220.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10022 - kube-public] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container tools in pod catalog-svc-bf949fc65-9cblt.
1,[pr-cp-reg-10042 - aris-kube-downscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10048 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme6n1"" changing its up status often on node-exporter management/argocd-server-74fbd754cb-cjfrb"
3,[pr-cp-reg-10031 - aris-nginx-ingress] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10040 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the kube-state-metrics/aris-kube-prometheus-stack-prometheus targets in aris-kube-prometheus-stack namespace are down.
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10029 - pr-customer-env] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env/pr-cp-reg-10029 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_cluster_autoscaler_unschedulable_pods - The cluster autoscaler is unable to scale up and there are unschedulable pods because of this condition.
2,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10014 cluster.
4,[pr-cp-reg-10008 - kube-public] - CPUThrottlingHigh -  throttling of CPU in namespace kube-public for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env-spark/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (main) is restarting  times / 5 minutes.
3,[pr-cp-reg-10018 - falcon-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10035 - pr-customer-env] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10017 - kube-node-lease] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10026 - aris-keda] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10020 - kube-public] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10018 - kube-public] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10020 - basic-application-bootstrapping] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10012 - kube-node-lease] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10043 - aris-keda] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10012 - pr-customer-env] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10011 - aris-cluster-autoscaler] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10036 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-sealed-secrets/pr-cp-reg-10036 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10011 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10009 - pr-customer-env] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container aws-vpc-cni-init in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10006 - aris-sealed-secrets] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - pr-customer-env-spark] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env-spark for container kube-state-metrics in pod pr-customer-env-spark-spark-operator-74d54645cb-5gpbl.
1,[pr-cp-reg-10029 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10032 - aris-sealed-secrets] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
4,[pr-cp-reg-10003 - aris-kube-downscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-downscaler for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10014 - pr-customer-env] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10049 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-7lbrs (get-zone) is restarting  times / 5 minutes.
3,[pr-cp-reg-10035 - aris-spot-metrics] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10018 - pr-customer-env-spark] - ARIS_k8s_node_memory_pressure - ip-10-0-147-93.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_critical - Latency to the upstream service aris-kube-prometheus-stack/argocd-server has been critically high (more than 30 seconds) for the last 3 minutes. Service is unresponsive and should be restarted.
1,[pr-cp-reg-10022 - pr-customer-env] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container setmaxmapcount in pod postgres.
3,"[pr-cp-reg-10006 - aris-spot] - ConfigReloaderSidecarErrors - Errors encountered while the spotinst-kubernetes-cluster-controller-cd9fc697f-hx99n config-reloader sidecar attempts to sync config in aris-spot namespace. As a result, configuration for service running in spotinst-kubernetes-cluster-controller-cd9fc697f-hx99n may be stale and cannot be updated anymore."
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on nvme3 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,"[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10021 - aris-nginx-ingress] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-nginx-ingress/exported_job failed to complete.
3,[pr-cp-reg-10022 - pr-customer-env] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10010 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10033 - aris-spot] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot/pr-cp-reg-10033 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10043 - aris-sealed-secrets] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10026 - kube-node-lease] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
1,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_k8s_node_disk_pressure - ip-10-0-138-163.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces-0 (settcpkeepalivetime) is restarting  times / 5 minutes.
1,[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_node_out_of_disk - ip-10-0-138-163.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10020 - aris-kube-prometheus-stack] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10006 - kube-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,"[pr-cp-reg-10045 - aris-spot] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10049 - kube-system] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod cloudsearch-0.
3,[pr-cp-reg-10012 - aris-sealed-secrets] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k (grafana-sc-datasources) is restarting  times / 5 minutes.
3,[pr-cp-reg-10033 - aris-nginx-ingress] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10011 - basic-application-bootstrapping] - CPUThrottlingHigh -  throttling of CPU in namespace basic-application-bootstrapping for container kube-state-metrics in pod basic-application-bootstrapping-2wp59.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_cluster_autoscaler_unschedulable_pods - The cluster autoscaler is unable to scale up and there are unschedulable pods because of this condition.
4,[pr-cp-reg-10027 - aris-kube-prometheus-stack] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-prometheus-stack/exported_job failed to complete.
3,[pr-cp-reg-10032 - aris-sealed-secrets] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10038 - aris-keda] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-keda/exported_job failed to complete.
3,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_argocd_app_progressing - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10037 - aris-sealed-secrets] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-sealed-secrets has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10012 - pr-customer-env] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10042 - aris-nginx-ingress] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_k8s_pod_restarted - The pod aris-kube-downscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10007 - aris-cluster-autoscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,"[pr-cp-reg-10034 - pr-customer-env] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10029 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10034 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-nginx-ingress/pr-cp-reg-10034 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10021 - pr-customer-env-spark] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10027 - aris-spot] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod zookeeper.
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-cluster-autoscaler has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10044 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod argocd-redis-ha-server-0.
3,[pr-cp-reg-10037 - falcon-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10036 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10036 - aris-nginx-ingress] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
2,[pr-cp-reg-10018 - aris-spot] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10018 cluster.
1,[pr-cp-reg-10021 - aris-nginx-ingress] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10031 - pr-customer-env-spark] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on nvme1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10030 - aris-sealed-secrets] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-application-controller in pod argocd-server-74fbd754cb-cjfrb.
1,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_k8s_pod_restarted - The pod aris-sealed-secrets/sealed-secrets-controller has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10014 - kube-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,"[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana will run full within 24 hours."
1,[pr-cp-reg-10016 - kube-node-lease] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10026 - aris-kube-downscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10036 - kube-public] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10043 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10034 - kube-public] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10008 - aris-spot] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10035 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_argocd_app_progressing - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10028 - aris-nginx-ingress] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10035 - kube-system] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10042 - aris-nginx-ingress] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app prometheusBufferedClient is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10050 - aris-kube-downscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10031 - aris-cluster-autoscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10006 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10003 - aris-keda] - ARIS_argocd_app_progressing - The ArgoCD app clustertriggerauthentication is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10016 - kube-public] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10025 - aris-spot] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10005 - management] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10034 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10015 - aris-keda] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_argocd_app_progressing - The ArgoCD app a6d67856bcc285f0f6ca4349c619dff9fba796cd94aeb13735a2f19b88abfe1e is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod management-external-dns-65bfcbd6d7-zlfjj.
1,[pr-cp-reg-10025 - kube-public] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10001 - aris-kube-downscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on nvme1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10047 - falcon-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod efs-csi-node-4cgpg.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics failed to send  of notifications to pagerduty.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (tenant-backup) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod efs-csi-controller.
4,[pr-cp-reg-10021 - aris-sealed-secrets] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-sealed-secrets/exported_job failed to complete.
1,[pr-cp-reg-10016 - pr-customer-env-spark] - ARIS_k8s_node_memory_pressure - ip-10-0-147-93.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10043 - aris-keda] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-keda/pr-cp-reg-10043 cluster is .
1,[pr-cp-reg-10002 - pr-customer-env] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10011 - aris-keda] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-keda/pr-cp-reg-10011 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container log-backup in pod adsadmin.
2,[pr-cp-reg-10043 - aris-keda] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10043 cluster.
3,[pr-cp-reg-10038 - management] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10003 - kube-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10043 - aris-nginx-ingress] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10017 - kasten-io] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10031 - aris-sealed-secrets] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10050 - aris-sealed-secrets] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processboard-0 (processengine) is restarting  times / 5 minutes.
1,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - ARIS_k8s_node_memory_pressure - ip-10-0-138-163.eu-central-1.compute.internal has MemoryPressure condition.
0,"[pr-cp-reg-10019 - management] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10039 - aris-nginx-ingress] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration-0 (elastic-metrics-sidecar) is restarting  times / 5 minutes.
3,[pr-cp-reg-10020 - pr-customer-env-spark] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10002 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10042 - kube-system] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10004 - pr-customer-env] - NodeRAIDDegraded - RAID array '/dev/nvme2n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-138-163.eu-central-1.compute.internal in aris-kube-prometheus-stack/pr-cp-reg-10008 cluster is .
0,"[pr-cp-reg-10024 - aris-kube-prometheus-stack] - Watchdog - This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the ""DeadMansSnitch"" integration in PagerDuty."
3,"[pr-cp-reg-10050 - pr-customer-env-spark] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10041 - aris-sealed-secrets] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10007 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10009 - kube-public] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10049 - pr-customer-env] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10012 - kube-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10010 - aris-kube-downscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container frontend-svc in pod catalog-svc-bf949fc65-9cblt.
3,[pr-cp-reg-10005 - management] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10014 - pr-customer-env] - ARIS_k8s_node_pid_pressure - ip-10-0-138-163.eu-central-1.compute.internal has PIDPressure condition.
2,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10022 cluster.
3,[pr-cp-reg-10047 - aris-sealed-secrets] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10034 - management] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10043 - aris-kube-prometheus-stack] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10004 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10014 - pr-customer-env-spark] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-147-93.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10049 - aris-kube-downscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10025 - kube-public] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
1,[pr-cp-reg-10049 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container copyutil in pod ebs-csi-node-2fbnc.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processboard-0 (tm) is restarting  times / 5 minutes.
3,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10023 - pr-customer-env-spark] - ARIS_argocd_app_out_of_sync - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10005 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
4,[pr-cp-reg-10001 - aris-nginx-ingress] - CPUThrottlingHigh -  throttling of CPU in namespace aris-nginx-ingress for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10048 - aris-kube-prometheus-stack] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10005 - management] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10005 - pr-customer-env-spark] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10012 - pr-customer-env-spark] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10011 - kasten-io] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
2,[pr-cp-reg-10036 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10036 cluster may be low for segment merges.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_node_out_of_disk - ip-10-0-138-163.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10016 - aris-kube-prometheus-stack] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container octopus in pod collaboration-0.
4,[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-downscaler has faced > 5 deadlocks in last minute.
2,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10008 cluster may be low for segment merges.
3,[pr-cp-reg-10002 - aris-keda] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10028 - kube-node-lease] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
1,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env-spark is down.
3,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface lo has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10048 - aris-sealed-secrets] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10024 - aris-kube-prometheus-stack] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-prometheus-stack is down.
1,[pr-cp-reg-10028 - aris-nginx-ingress] - ARIS_postgresql_down - The postgres pod of namespace aris-nginx-ingress is down.
3,[pr-cp-reg-10013 - aris-nginx-ingress] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10046 - aris-cluster-autoscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod tm-0.
3,[pr-cp-reg-10013 - kube-system] - KubeletServerCertificateRenewalErrors - Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal has failed to renew its server certificate ( errors in the last 5 minutes).
1,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration (elasticsearch) is restarting  times / 5 minutes.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - AlertmanagerClusterCrashlooping -  of Alertmanager instances within the aris-kube-prometheus-stack-prometheus cluster have restarted at least 5 times in the last 10m.
3,[pr-cp-reg-10004 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10026 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-prometheus-stack/pr-cp-reg-10026 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10029 - aris-kube-downscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container controller in pod umcadmin.
3,[pr-cp-reg-10050 - aris-kube-downscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10001 - kasten-io] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface nvme0 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_k8s_node_disk_pressure - ip-10-0-147-93.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10038 - aris-keda] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10011 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-cluster-autoscaler is down.
3,[pr-cp-reg-10003 - kube-system] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/simulation-0 (postgres) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod simulation.
3,[pr-cp-reg-10027 - aris-kube-downscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10004 - aris-spot] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-spot/spotinst-kubernetes-cluster-controller-cd9fc697f-hx99n"
1,[pr-cp-reg-10003 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
4,[pr-cp-reg-10007 - kube-system] - KubeQuotaFullyUsed - Namespace kube-system is using  of its cpu quota.
3,[pr-cp-reg-10012 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,"[pr-cp-reg-10049 - aris-keda] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_k8s_pod_restarted - The pod aris-cluster-autoscaler/aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10029 - aris-cluster-autoscaler] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10036 - aris-keda] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10043 - aris-nginx-ingress] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme4n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
4,[pr-cp-reg-10025 - kube-system] - KubeQuotaFullyUsed - Namespace kube-system is using  of its memory quota.
3,[pr-cp-reg-10028 - aris-kube-prometheus-stack] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-prometheus-stack has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10002 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10007 - management] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10021 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-prometheus-stack/pr-cp-reg-10007 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-cluster-autoscaler has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10003 - pr-customer-env-spark] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10040 - aris-nginx-ingress] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10041 - aris-spot] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10012 - kube-system] - KubeletServerCertificateExpiration - Server certificate for Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal expires in .
3,[pr-cp-reg-10020 - kube-system] - KubeQuotaExceeded - Namespace kube-system is using  of its cpu quota.
3,[pr-cp-reg-10018 - management] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10003 - kube-node-lease] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10025 - kube-public] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10023 - aris-cluster-autoscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod efs-csi-controller-94d8968c6-9gpjc.
3,[pr-cp-reg-10044 - aris-keda] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-keda has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_k8s_node_pid_pressure - ip-10-0-138-163.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10016 - pr-customer-env] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10041 - aris-cluster-autoscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10031 - aris-nginx-ingress] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10010 - kube-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app disableSecretsCompatibility is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana-sc-dashboard in pod aris-kube-prometheus-stack-prometheus-node-exporter-822lp.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod efs-csi-node.
3,[pr-cp-reg-10006 - kube-system] - KubeNodeUnreachable - ip-10-0-138-163.eu-central-1.compute.internal is unreachable and some workloads may be rescheduled.
3,[pr-cp-reg-10026 - aris-spot-metrics] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot-metrics has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10016 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10023 - falcon-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,"[pr-cp-reg-10001 - pr-customer-env-spark] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg"
3,[pr-cp-reg-10023 - pr-customer-env] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
2,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10005 cluster.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10009 - aris-cluster-autoscaler] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10009 - pr-customer-env] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10038 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env-spark/pr-cp-reg-10038 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10048 - aris-cluster-autoscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10032 - aris-keda] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10018 - aris-cluster-autoscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface shm has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10028 - aris-kube-downscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10018 - aris-kube-downscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10014 - aris-keda] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10019 - aris-nginx-ingress] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10019 - aris-sealed-secrets] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_argocd_app_unknown - The ArgoCD app aris-secret-smtp is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10021 - kube-public] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10030 - aris-kube-downscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10045 - aris-spot] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10006 - management] - NodeRAIDDegraded - RAID array '/dev/nvme6n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10022 - kube-node-lease] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10008 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-cluster-autoscaler/pr-cp-reg-10024 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10050 - aris-kube-prometheus-stack] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
4,[pr-cp-reg-10008 - aris-kube-downscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-downscaler for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10032 - aris-kube-prometheus-stack] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-svc in pod controllermanager-svc-788dc96c8d-rg47t.
2,[pr-cp-reg-10014 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10014 cluster.
4,[pr-cp-reg-10046 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env/exported_job failed to complete.
3,[pr-cp-reg-10019 - kasten-io] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10016 - aris-cluster-autoscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-cluster-autoscaler has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10017 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10008 - kube-system] - KubeNodeNotReady - ip-10-0-138-163.eu-central-1.compute.internal has been unready for more than 15 minutes.
3,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10007 - kube-node-lease] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
1,[pr-cp-reg-10003 - kube-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod efs-csi-controller-94d8968c6-6ts4k.
3,[pr-cp-reg-10023 - aris-sealed-secrets] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10027 - aris-cluster-autoscaler] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10004 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_restarted - The pod pr-customer-env/kanister-job-6gvp5 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10018 - aris-sealed-secrets] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10039 - aris-sealed-secrets] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-sealed-secrets has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10046 - kube-node-lease] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper (processboard) is restarting  times / 5 minutes.
1,[pr-cp-reg-10019 - aris-nginx-ingress] - ARIS_k8s_node_disk_pressure - ip-10-0-143-220.eu-central-1.compute.internal has DiskPressure condition.
4,[pr-cp-reg-10027 - aris-sealed-secrets] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-sealed-secrets has faced > 5 deadlocks in last minute.
2,[pr-cp-reg-10003 - pr-customer-env] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10003 cluster may be low for segment merges.
1,[pr-cp-reg-10044 - aris-sealed-secrets] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10031 - kube-public] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod processboard-0.
1,[pr-cp-reg-10031 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod ces-0.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container delay in pod ebs-csi-controller-86cb997fdc-bs5bs.
1,[pr-cp-reg-10020 - aris-spot] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
2,[pr-cp-reg-10030 - aris-keda] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10030 cluster.
3,[pr-cp-reg-10005 - kube-system] - KubeletPodStartUpLatencyHigh - Kubelet Pod startup 99th percentile latency is  seconds on node ip-10-0-138-163.eu-central-1.compute.internal.
4,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env-spark has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10032 - kube-node-lease] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
0,"[pr-cp-reg-10036 - aris-sealed-secrets] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10049 - kube-node-lease] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10002 - aris-sealed-secrets] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10037 - aris-sealed-secrets] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10033 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10013 - pr-customer-env] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10025 - pr-customer-env] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/ae-aw-euc1-15995-aws-load-balancer-controller-7895559794-c8nd5"
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container tools in pod metering-svc.
3,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
0,"[pr-cp-reg-10024 - aris-nginx-ingress] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10035 - kube-public] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,"[pr-cp-reg-10001 - aris-nginx-ingress] - ConfigReloaderSidecarErrors - Errors encountered while the aris-nginx-ingress-ingress-nginx-controller config-reloader sidecar attempts to sync config in aris-nginx-ingress namespace. As a result, configuration for service running in aris-nginx-ingress-ingress-nginx-controller may be stale and cannot be updated anymore."
1,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10045 - aris-keda] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10018 - aris-sealed-secrets] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10003 - kasten-io] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10022 - aris-sealed-secrets] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10049 - pr-customer-env-spark] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10046 - kasten-io] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10013 - aris-keda] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10044 cluster.
1,[pr-cp-reg-10005 - management] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,"[pr-cp-reg-10005 - aris-spot] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-spot/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10032 - aris-spot-metrics] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10021 - pr-customer-env-spark] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10034 - aris-keda] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod elasticsearch-default.
3,"[pr-cp-reg-10043 - kube-public] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-public namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10027 - aris-sealed-secrets] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10007 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_argocd_app_progressing - The ArgoCD app 04abfe3da01db1f02bdd752a9e3436b7cf688976b23b6f6c51773d813348eea1 is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10047 - kube-node-lease] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_argocd_app_out_of_sync - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is out of sync for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10002 - aris-spot] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot for container controller in pod spot-ocean-metric-exporter-56dfbdbbcb-82vqg.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface nvme0 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10005 - aris-cluster-autoscaler] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface nvme1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10020 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""lo"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter"
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'lo' needs attention and possibly a disk swap.
1,[pr-cp-reg-10032 - aris-cluster-autoscaler] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10018 - aris-keda] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10018 - kube-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-prometheus-stack/pr-cp-reg-10030 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container catalog-svc in pod controllermanager-svc-788dc96c8d-rg47t.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-sidecar in pod aggregatedapis-svc-5c7ccfdd78-slnk8.
3,[pr-cp-reg-10016 - kube-node-lease] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_k8s_pod_restarted - The pod aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container portalserver in pod abs-0.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10010 - aris-spot] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot has too many active connections. More than 90% of available connections are used.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/backup-logs-28124400-ktxxf"
3,[pr-cp-reg-10014 - kasten-io] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme2n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10035 - kube-node-lease] - CPUThrottlingHigh -  throttling of CPU in namespace kube-node-lease for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10014 - basic-application-bootstrapping] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10006 - pr-customer-env] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10006 cluster.
3,[pr-cp-reg-10029 - aris-sealed-secrets] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10003 - aris-sealed-secrets] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10026 - aris-kube-prometheus-stack] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterDown -  of Alertmanager instances within the node-exporter cluster have been up for less than half of the last 5m.
2,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10006 cluster may be low for segment merges.
4,[pr-cp-reg-10005 - aris-nginx-ingress] - CPUThrottlingHigh -  throttling of CPU in namespace aris-nginx-ingress for container controller in pod aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4.
3,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10035 - aris-spot-metrics] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10008 - aris-keda] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10003 - aris-keda] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-keda/pr-cp-reg-10003 cluster is .
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerFailedReload - Configuration has failed to load for aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container kube-prometheus-stack in pod aris-kube-prometheus-stack-prometheus-node-exporter.
0,"[pr-cp-reg-10001 - pr-customer-env-spark] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10004 - aris-keda] - ConfigReloaderSidecarErrors - Errors encountered while the keda-operator config-reloader sidecar attempts to sync config in aris-keda namespace. As a result, configuration for service running in keda-operator may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-sealed-secrets/exported_job failed to complete.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (dashboarding) is restarting  times / 5 minutes.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10042 - aris-spot-metrics] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10041 - aris-kube-prometheus-stack] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10005 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_postgresql_down - The postgres pod of namespace aris-nginx-ingress is down.
1,[pr-cp-reg-10018 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10027 - pr-customer-env] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env is down.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10011 - aris-kube-downscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
2,[pr-cp-reg-10015 - aris-spot] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10015 cluster may be low for segment merges.
1,[pr-cp-reg-10045 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
2,[pr-cp-reg-10008 - aris-keda] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10008 cluster.
1,[pr-cp-reg-10023 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10038 - kube-system] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-solutionsgallery-secret is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces-0 (setmaxmapcount) is restarting  times / 5 minutes.
3,[pr-cp-reg-10022 - kube-system] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10034 - aris-keda] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_k8s_node_memory_pressure - ip-10-0-138-163.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10020 - kube-node-lease] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10009 - aris-sealed-secrets] - ARIS_host_oom_kill_detected - OOM kill detected.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/efs-csi-node-2r2zp"
1,[pr-cp-reg-10012 - pr-customer-env] - NodeRAIDDegraded - RAID array '/dev/nvme4n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10028 - aris-cluster-autoscaler] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10002 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10031 - basic-application-bootstrapping] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10012 - aris-spot-metrics] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot-metrics has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10008 - pr-customer-env] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,"[pr-cp-reg-10014 - pr-customer-env-spark] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in pr-customer-env-spark namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - ARIS_k8s_node_disk_pressure - ip-10-0-138-163.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10045 - aris-cluster-autoscaler] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'eni006a31b57f1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_k8s_pod_crash_looping - Pod aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (aws-cluster-autoscaler) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod backup-tenants-28123035-nswx7.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28124475-fjqvp (setmaxmapcount) is restarting  times / 5 minutes.
3,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-7lbrs (tenant-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10039 - aris-keda] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10047 - pr-customer-env] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env is down.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elastic-metrics-sidecar in pod kanister-job-7lbrs.
1,[pr-cp-reg-10001 - kube-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (cloudsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10049 - default] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10034 - aris-nginx-ingress] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10001 cluster.
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10049 - kube-system] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10038 - aris-kube-prometheus-stack] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array 'eth2' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,"[pr-cp-reg-10004 - aris-keda] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-keda/keda-operator"
3,[pr-cp-reg-10050 - aris-sealed-secrets] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer (cloudsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10034 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10023 - aris-kube-prometheus-stack] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,"[pr-cp-reg-10009 - aris-cluster-autoscaler] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-cluster-autoscaler/aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf"
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerClusterCrashlooping -  of Alertmanager instances within the aris-kube-prometheus-stack-operator cluster have restarted at least 5 times in the last 10m.
3,"[pr-cp-reg-10001 - default] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in default namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container dashboarding in pod backup-logs-28124280-qwm2q.
3,[pr-cp-reg-10005 - default] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10048 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container delay in pod argocd-server.
3,[pr-cp-reg-10009 - pr-customer-env] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10007 - pr-customer-env] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10026 - aris-keda] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10004 - kube-system] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_high - Latency to the upstream service aris-nginx-ingress/argocd-server has been high (more than 10 seconds) for the last 3 minutes. Service might become unresponsive shortly.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container container in pod abs-0.
3,[pr-cp-reg-10020 - aris-kube-downscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10012 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_takes_too_long - CronJob pr-customer-env/backup-logs is taking more than 8h to complete.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10004 - kube-node-lease] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10046 - aris-nginx-ingress] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,"[pr-cp-reg-10009 - aris-sealed-secrets] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-sealed-secrets/sealed-secrets-controller"
3,[pr-cp-reg-10011 - aris-sealed-secrets] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10003 - pr-customer-env] - NodeRAIDDegraded - RAID array '/dev/nvme4n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on eni0039e8a4ad3 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10042 - pr-customer-env-spark] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10010 - aris-spot] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10010 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10018 - pr-customer-env] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controller in pod controllermanager-svc.
3,[pr-cp-reg-10031 - kube-node-lease] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
1,[pr-cp-reg-10018 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env-spark/pr-cp-reg-10018 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10020 - aris-kube-downscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10047 - aris-kube-prometheus-stack] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28125915-9ps25 (portalserver) is restarting  times / 5 minutes.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array 'eth0' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface eth0 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10015 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10009 - aris-sealed-secrets] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container kube-state-metrics in pod argocd-redis-6645d4fb89-kpv95.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env-spark has too many active connections. More than 90% of available connections are used.
0,"[pr-cp-reg-10008 - default] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10013 - kube-system] - KubeletPlegDurationHigh - The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of  seconds on node ip-10-0-139-113.eu-central-1.compute.internal.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter pr-customer-env/abs-0"
4,[pr-cp-reg-10002 - aris-keda] - CPUThrottlingHigh -  throttling of CPU in namespace aris-keda for container keda-operator-metrics-apiserver in pod keda-operator-metrics-apiserver-7c746565cb-t6krb.
3,[pr-cp-reg-10026 - pr-customer-env-spark] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10005 - kube-system] - KubeletServerCertificateRenewalErrors - Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal has failed to renew its server certificate ( errors in the last 5 minutes).
1,[pr-cp-reg-10001 - aris-keda] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env/backup-tenants-28125915-9ps25"
1,[pr-cp-reg-10008 - aris-keda] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,"[pr-cp-reg-10002 - kube-system] - ConfigReloaderSidecarErrors - Errors encountered while the kube-proxy-46qsh config-reloader sidecar attempts to sync config in kube-system namespace. As a result, configuration for service running in kube-proxy-46qsh may be stale and cannot be updated anymore."
3,[pr-cp-reg-10040 - pr-customer-env-spark] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10040 - aris-sealed-secrets] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10024 - kube-node-lease] - CPUThrottlingHigh -  throttling of CPU in namespace kube-node-lease for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10045 - aris-spot-metrics] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10038 - aris-sealed-secrets] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container collaboration in pod elasticsearch-default.
1,[pr-cp-reg-10015 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (tm) is restarting  times / 5 minutes.
3,[pr-cp-reg-10049 - pr-customer-env] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10040 - kube-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10004 - management] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme5n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10003 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
1,[pr-cp-reg-10010 - aris-spot] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10010 cluster.
1,[pr-cp-reg-10050 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer (simulation) is restarting  times / 5 minutes.
3,[pr-cp-reg-10031 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container liveness-probe in pod argocd-dex-server-6b74fb9695-kbk62.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod processengine-0.
3,[pr-cp-reg-10020 - aris-kube-downscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10014 - aris-sealed-secrets] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10049 - basic-application-bootstrapping] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10032 - aris-spot] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10022 - aris-keda] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10032 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10018 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration-0 (dashboarding) is restarting  times / 5 minutes.
3,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-nginx-ingress/pr-cp-reg-10014 cluster is .
4,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-nginx-ingress/exported_job failed to complete.
3,[pr-cp-reg-10003 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10035 - falcon-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10006 - aris-sealed-secrets] - ARIS_k8s_pod_crash_looping - Pod aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10015 - kube-public] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10003 - pr-customer-env-spark] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10001 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10024 - kube-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10049 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
4,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-sealed-secrets has faced > 5 deadlocks in last minute.
0,"[pr-cp-reg-10010 - kasten-io] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10018 - falcon-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter falcon-system/falcon-falcon-sensor-2bjwf"
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/adsadmin (get-zone) is restarting  times / 5 minutes.
1,[pr-cp-reg-10017 - aris-cluster-autoscaler] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10042 - falcon-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10022 - aris-kube-prometheus-stack] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-prometheus-stack has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10038 - pr-customer-env] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10041 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10007 - aris-nginx-ingress] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
0,"[pr-cp-reg-10010 - aris-spot-metrics] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10030 - kube-public] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10028 - aris-spot] - ARIS_postgresql_down - The postgres pod of namespace aris-spot is down.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container kube-state-metrics in pod argocd-redis-ha-server-2.
1,[pr-cp-reg-10008 - kube-system] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedReload - Configuration has failed to load for aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-822lp.
3,[pr-cp-reg-10007 - aris-spot] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-spot/pr-cp-reg-10007 cluster is .
3,[pr-cp-reg-10039 - aris-kube-downscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10047 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10001 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
0,"[pr-cp-reg-10042 - aris-spot] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10040 - aris-keda] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10038 - kube-system] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10022 - aris-sealed-secrets] - ARIS_argocd_app_progressing - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10031 - aris-keda] - ARIS_host_oom_kill_detected - OOM kill detected.
0,"[pr-cp-reg-10047 - aris-keda] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cloudsearch (processboard) is restarting  times / 5 minutes.
1,[pr-cp-reg-10043 - aris-kube-prometheus-stack] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container kube-state-metrics in pod kube-proxy-46qsh.
2,[pr-cp-reg-10005 - aris-sealed-secrets] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10005 cluster.
1,[pr-cp-reg-10010 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 config-reloader sidecar attempts to sync config in aris-kube-prometheus-stack namespace. As a result, configuration for service running in aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10006 - kube-node-lease] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
1,[pr-cp-reg-10036 - aris-keda] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to slack sent from any instance in the aris-kube-prometheus-stack-grafana cluster is .
2,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10014 cluster.
3,[pr-cp-reg-10014 - kube-system] - KubeNodeNotReady - ip-10-0-138-163.eu-central-1.compute.internal has been unready for more than 15 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 failed to send  of notifications to victorops.
3,[pr-cp-reg-10001 - aris-keda] - ARIS_argocd_app_progressing - The ArgoCD app bf4949571e1d348960cfaea8d505c51028858bed802f8e8fe13fc4ea5ab020a1 is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10026 - kube-system] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10040 - aris-nginx-ingress] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/create-es-index-template-61d85-nz2dz (create-es-index-template) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10050 - aris-spot-metrics] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10014 - pr-customer-env-spark] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10019 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env-spark/pr-cp-reg-10019 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10013 - kube-system] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
2,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10007 cluster may be low for segment merges.
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10034 - aris-kube-downscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_k8s_node_disk_pressure - ip-10-0-147-93.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10028 - pr-customer-env-spark] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10022 - aris-keda] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10035 - pr-customer-env] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10043 - pr-customer-env] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10016 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10021 - aris-keda] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10015 - aris-kube-downscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme1n1"" changing its up status often on node-exporter kasten-io/kanister-svc-9559f74-79zgn"
3,[pr-cp-reg-10009 - kube-system] - KubeletPodStartUpLatencyHigh - Kubelet Pod startup 99th percentile latency is  seconds on node ip-10-0-139-113.eu-central-1.compute.internal.
3,[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10047 - aris-sealed-secrets] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface nvme3 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-server in pod ebs-csi-controller-86cb997fdc-bs5bs.
1,[pr-cp-reg-10047 - aris-keda] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-keda/pr-cp-reg-10047 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10007 - kube-node-lease] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10022 - aris-kube-prometheus-stack] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-prometheus-stack is down.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod backup-logs-28124400-ktxxf.
1,[pr-cp-reg-10031 - aris-kube-downscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10033 - aris-spot] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10041 - default] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-147-93.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10005 - aris-keda] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
2,[pr-cp-reg-10048 - aris-keda] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10048 cluster may be low for segment merges.
1,[pr-cp-reg-10041 - aris-nginx-ingress] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10027 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10044 - aris-cluster-autoscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10007 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10032 - pr-customer-env] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-cluster-autoscaler is down.
3,[pr-cp-reg-10027 - kube-public] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
1,[pr-cp-reg-10006 - aris-cluster-autoscaler] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eth0"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-0"
1,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10007 - aris-cluster-autoscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme6n1"" changing its up status often on node-exporter management/aris-management-ingress-delay-ntdn6"
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-nginx-ingress/pr-cp-reg-10002 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10042 - aris-kube-prometheus-stack] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-prometheus-stack has too many active connections. More than 90% of available connections are used.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container efs-plugin in pod cert-manager-cainjector-7d8985bbc8-x7www.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controllermanager-svc in pod aggregatedapis-svc.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_k8s_node_disk_pressure - ip-10-0-138-163.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10042 - aris-cluster-autoscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10036 - aris-keda] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kasten-io/auth-svc-97bd685df-phk9x"
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_high - Latency to the upstream service aris-kube-prometheus-stack/loadbalancer has been high (more than 10 seconds) for the last 3 minutes. Service might become unresponsive shortly.
3,[pr-cp-reg-10044 - aris-kube-prometheus-stack] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/adsadmin-0"
1,[pr-cp-reg-10027 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10008 - aris-sealed-secrets] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
2,[pr-cp-reg-10010 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10010 cluster.
1,[pr-cp-reg-10001 - pr-customer-env-spark] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10011 - pr-customer-env] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10037 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10016 - aris-nginx-ingress] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10026 - aris-nginx-ingress] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
0,"[pr-cp-reg-10016 - pr-customer-env] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container copyutil in pod ebs-csi-controller.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container node-exporter in pod aris-kube-prometheus-stack-grafana.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container jobs-svc in pod crypto-svc.
1,[pr-cp-reg-10007 - pr-customer-env] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10009 - pr-customer-env] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10039 - kube-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-nginx-ingress-ingress-nginx-leader is out of sync for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10043 - aris-kube-downscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10019 - pr-customer-env] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10022 - aris-kube-downscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10021 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10010 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod ces-0.
3,[pr-cp-reg-10010 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot-metrics/pr-cp-reg-10010 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cdf in pod ces.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod kanister-job-6kclt.
1,[pr-cp-reg-10015 - pr-customer-env] - ARIS_k8s_node_out_of_disk - ip-10-0-138-163.eu-central-1.compute.internal has OutOfDisk condition.
3,"[pr-cp-reg-10011 - aris-cluster-autoscaler] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-cluster-autoscaler/aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf"
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eth2"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics"
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container efs-plugin in pod management-external-dns.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod loadbalancer-0.
3,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-nginx-ingress-ingress-nginx-leader is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10024 - kube-node-lease] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10027 - management] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10035 - aris-keda] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10016 - aris-spot-metrics] - ARIS_postgresql_down - The postgres pod of namespace aris-spot-metrics is down.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container frontend-svc in pod catalog-svc.
1,[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_k8s_node_disk_pressure - ip-10-0-147-93.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
2,[pr-cp-reg-10023 - aris-spot] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10023 cluster may be low for segment merges.
0,"[pr-cp-reg-10042 - basic-application-bootstrapping] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-provisioner in pod ebs-csi-controller-86cb997fdc-t2z6p.
3,[pr-cp-reg-10012 - kube-public] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-nginx-ingress has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10031 - aris-spot] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10002 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10040 - aris-kube-downscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10041 - aris-nginx-ingress] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10026 - aris-cluster-autoscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10033 - aris-cluster-autoscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10044 - pr-customer-env-spark] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_k8s_pod_crash_looping - Pod aris-cluster-autoscaler/aris-cluster-autoscaler-aws-cluster-autoscaler (controller) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10001 - management] - ConfigReloaderSidecarErrors - Errors encountered while the efs-csi-controller-94d8968c6-6ts4k config-reloader sidecar attempts to sync config in management namespace. As a result, configuration for service running in efs-csi-controller-94d8968c6-6ts4k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10022 - pr-customer-env] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
4,[pr-cp-reg-10016 - aris-nginx-ingress] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-nginx-ingress has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env-spark is down.
1,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
2,[pr-cp-reg-10026 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10026 cluster may be low for segment merges.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-sidecar in pod controllermanager-svc.
1,[pr-cp-reg-10018 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10017 - pr-customer-env] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on eth1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10007 - kube-system] - KubeClientErrors - Kubernetes API server client 'kubelet/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
1,[pr-cp-reg-10008 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on nvme1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env-spark/pr-cp-reg-10040 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10033 - kube-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container create-es-index-template in pod backup-logs-28124400-ktxxf.
3,[pr-cp-reg-10050 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10006 - management] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10050 - kube-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10036 - aris-kube-prometheus-stack] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10029 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10002 - management] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10001 - aris-spot-metrics] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10045 - pr-customer-env] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10024 - aris-kube-downscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10050 - pr-customer-env-spark] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container sentinel in pod cert-manager-cainjector-7d8985bbc8-x7www.
3,[pr-cp-reg-10034 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10018 - aris-keda] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10046 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10030 - pr-customer-env-spark] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10025 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10028 - falcon-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10001 - management] - ConfigReloaderSidecarErrors - Errors encountered while the ae-aw-euc1-15995-aws-load-balancer-controller-7895559794-c8nd5 config-reloader sidecar attempts to sync config in management namespace. As a result, configuration for service running in ae-aw-euc1-15995-aws-load-balancer-controller-7895559794-c8nd5 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cloudsearch-0 (cdf) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kube-state-metrics in pod backup-logs-28124280-qwm2q.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container dashboardbff-svc in pod kanister-svc.
3,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10005 - aris-kube-downscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,"[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/mining-storage will run full within 24 hours."
1,[pr-cp-reg-10003 - kasten-io] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'eth2' needs attention and possibly a disk swap.
1,[pr-cp-reg-10004 - kasten-io] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10025 - aris-keda] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10050 - kube-system] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container auth-svc in pod auth-svc.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cdf in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
1,[pr-cp-reg-10045 - aris-spot] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container umcadmin in pod cloudsearch-0.
3,[pr-cp-reg-10026 - aris-spot] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot/pr-cp-reg-10026 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10023 - aris-keda] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10006 - management] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_argocd_app_unknown - The ArgoCD app 705ccec2911bb44176bb5a664bb99d8cb0e9ff09e31f6355ef29837c92560a00 is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10021 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 8h to complete.
3,[pr-cp-reg-10008 - aris-keda] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10037 - aris-keda] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-keda/pr-cp-reg-10037 cluster is .
1,[pr-cp-reg-10048 - pr-customer-env] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env is down.
1,[pr-cp-reg-10006 - kube-public] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
1,[pr-cp-reg-10034 - aris-keda] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10033 - management] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10019 - aris-keda] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10001 - aris-keda] - ARIS_k8s_pod_crash_looping - Pod aris-keda/sealed-secrets-controller-cbbc8bd6b-qq22k (controller) is restarting  times / 5 minutes.
1,[pr-cp-reg-10050 - aris-kube-prometheus-stack] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10004 - kube-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-downscaler/pr-cp-reg-10046 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10017 - aris-spot] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10031 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,[pr-cp-reg-10006 - aris-kube-downscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10035 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10047 - aris-nginx-ingress] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container delay in pod argocd-repo-server.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container delay in pod ae-aw-euc1-15995-aws-load-balancer-controller.
2,[pr-cp-reg-10045 - aris-keda] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10045 cluster.
3,[pr-cp-reg-10018 - aris-kube-downscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container controller in pod portalserver.
3,[pr-cp-reg-10034 - aris-kube-prometheus-stack] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_argocd_app_unknown - The ArgoCD app aris-registry-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10041 - pr-customer-env] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10043 - kasten-io] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10015 - aris-sealed-secrets] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10045 - aris-keda] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10046 - aris-keda] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10001 - aris-kube-downscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10010 - kube-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10045 - aris-cluster-autoscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-cluster-autoscaler has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10002 - aris-spot] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10008 - pr-customer-env] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10009 - aris-cluster-autoscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10016 - aris-sealed-secrets] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10023 - aris-kube-prometheus-stack] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10014 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10043 - kube-system] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10026 - aris-sealed-secrets] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10038 - basic-application-bootstrapping] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10002 - aris-keda] - ARIS_k8s_pod_crash_looping - Pod aris-keda/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (keda-operator-metrics-apiserver) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper-0 (loadbalancer) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container schema-upgrade-check in pod dashboardbff-svc.
3,[pr-cp-reg-10027 - aris-kube-prometheus-stack] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10010 - pr-customer-env-spark] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env-spark has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10011 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10009 - pr-customer-env] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on nvme2 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10005 - management] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
0,"[pr-cp-reg-10029 - kube-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10019 - aris-keda] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10003 - falcon-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus failed to send  of notifications to wechat.
4,[pr-cp-reg-10010 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_suspended - CronJob pr-customer-env/backup-tenants is suspended.
3,"[pr-cp-reg-10001 - management] - ConfigReloaderSidecarErrors - Errors encountered while the argocd-repo-server-6676cd4f9c-l95m8 config-reloader sidecar attempts to sync config in management namespace. As a result, configuration for service running in argocd-repo-server-6676cd4f9c-l95m8 may be stale and cannot be updated anymore."
1,[pr-cp-reg-10037 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10030 - pr-customer-env-spark] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-147-93.eu-central-1.compute.internal in pr-customer-env-spark/pr-cp-reg-10030 cluster is .
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterDown -  of Alertmanager instances within the kubelet cluster have been up for less than half of the last 5m.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10012 - kube-system] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10010 - aris-cluster-autoscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10032 - aris-nginx-ingress] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-nginx-ingress has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin-0 (processboard) is restarting  times / 5 minutes.
3,[pr-cp-reg-10040 - aris-kube-downscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/cloudsearch-0"
1,[pr-cp-reg-10005 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10019 - aris-spot] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot/pr-cp-reg-10019 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10017 - kube-system] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container liveness-probe in pod argocd-redis-6645d4fb89-kpv95.
4,[pr-cp-reg-10014 - kube-system] - KubeletTooManyPods - Kubelet 'ip-10-0-139-113.eu-central-1.compute.internal' is running at  of its Pod capacity.
3,[pr-cp-reg-10046 - aris-keda] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10033 - aris-kube-downscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10040 - pr-customer-env] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10015 - kube-system] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,"[pr-cp-reg-10009 - basic-application-bootstrapping] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in basic-application-bootstrapping namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10038 - aris-sealed-secrets] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10027 - aris-kube-prometheus-stack] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app cloudWatchCrossAccountQuerying is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_critical - Latency to the upstream service aris-nginx-ingress/loadbalancer has been critically high (more than 30 seconds) for the last 3 minutes. Service is unresponsive and should be restarted.
3,[pr-cp-reg-10038 - pr-customer-env] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-cluster-autoscaler/pr-cp-reg-10044 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10042 - aris-keda] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10038 - aris-keda] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
2,[pr-cp-reg-10024 - aris-sealed-secrets] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10024 cluster.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app topnav is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10046 - aris-sealed-secrets] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10024 - pr-customer-env-spark] - ARIS_k8s_node_memory_pressure - ip-10-0-147-93.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10012 - kube-system] - KubeletClientCertificateExpiration - Client certificate for Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal expires in .
1,[pr-cp-reg-10038 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10018 - aris-sealed-secrets] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10018 cluster.
3,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10026 - default] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10003 - kube-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10013 - pr-customer-env] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in pr-customer-env/pr-cp-reg-10013 cluster is .
1,[pr-cp-reg-10002 - pr-customer-env] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10027 - aris-kube-downscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_k8s_node_disk_pressure - ip-10-0-147-93.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10026 - kasten-io] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10049 - aris-cluster-autoscaler] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10031 - aris-sealed-secrets] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10001 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the state-svc-745f7ffd6d-jfrkr config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in state-svc-745f7ffd6d-jfrkr may be stale and cannot be updated anymore."
3,[pr-cp-reg-10006 - kasten-io] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
1,[pr-cp-reg-10032 - aris-sealed-secrets] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10005 - kube-system] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container events-svc in pod metering-svc.
1,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-cluster-autoscaler is down.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter pr-customer-env/portalserver-0"
1,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10004 - pr-customer-env] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_progressing - The ArgoCD app bf4949571e1d348960cfaea8d505c51028858bed802f8e8fe13fc4ea5ab020a1 is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10007 - kube-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-138-163.eu-central-1.compute.internal in pr-customer-env/pr-cp-reg-10004 cluster is .
1,[pr-cp-reg-10047 - aris-spot] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container liveness-probe in pod argocd-server.
3,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10014 - aris-nginx-ingress] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10046 - default] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10012 - pr-customer-env] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10024 - aris-spot] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10016 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10036 - basic-application-bootstrapping] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10027 - aris-nginx-ingress] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10022 - pr-customer-env-spark] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10010 - kube-system] - KubeletTooManyPods - Kubelet 'ip-10-0-139-113.eu-central-1.compute.internal' is running at  of its Pod capacity.
3,"[pr-cp-reg-10003 - aris-keda] - ConfigReloaderSidecarErrors - Errors encountered while the keda-operator-metrics-apiserver-7c746565cb-t6krb config-reloader sidecar attempts to sync config in aris-keda namespace. As a result, configuration for service running in keda-operator-metrics-apiserver-7c746565cb-t6krb may be stale and cannot be updated anymore."
4,[pr-cp-reg-10031 - aris-cluster-autoscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-cluster-autoscaler/exported_job failed to complete.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/tm (octopus) is restarting  times / 5 minutes.
1,[pr-cp-reg-10003 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container adsadmin in pod tm-0.
3,[pr-cp-reg-10022 - pr-customer-env] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod efs-csi-node-2r2zp.
3,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10031 - aris-kube-downscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10028 - aris-cluster-autoscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10036 - pr-customer-env-spark] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10026 - aris-keda] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10030 - aris-keda] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10032 - kube-node-lease] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10027 - kube-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10013 - kube-public] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10030 - aris-kube-downscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10005 - management] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10020 - aris-nginx-ingress] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10010 - default] - CPUThrottlingHigh -  throttling of CPU in namespace default for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10031 - pr-customer-env] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10024 - aris-spot] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container aws-vpc-cni-init in pod kube-proxy-4znh2.
3,[pr-cp-reg-10015 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10020 - aris-kube-prometheus-stack] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
2,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10050 cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod collaboration.
3,[pr-cp-reg-10050 - pr-customer-env-spark] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container schema-upgrade-check in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10049 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,"[pr-cp-reg-10041 - aris-kube-downscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10020 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
2,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10008 cluster may be low for segment merges.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10006 - management] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme5n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10001 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-downscaler/pr-cp-reg-10001 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10019 - aris-cluster-autoscaler] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-cluster-autoscaler is down.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-svc in pod gateway.
3,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
4,[pr-cp-reg-10002 - aris-keda] - CPUThrottlingHigh -  throttling of CPU in namespace aris-keda for container kube-state-metrics in pod keda-operator-5867cd94d9-vhxdf.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerConfigInconsistent - Alertmanager instances within the kube-state-metrics cluster have different configurations.
1,[pr-cp-reg-10009 - aris-keda] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,"[pr-cp-reg-10018 - pr-customer-env] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10002 - aris-keda] - ARIS_k8s_pod_restarted - The pod aris-keda/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface eth2 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10032 - aris-keda] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10012 - aris-spot] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controller in pod executor-svc.
3,[pr-cp-reg-10001 - aris-keda] - ARIS_argocd_app_out_of_sync - The ArgoCD app 23122faab4957433d2ae942409d7bd757433bb06c6ef58aabc0599bb6382b619 is out of sync for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-cluster-autoscaler has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10036 - aris-nginx-ingress] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10040 - aris-nginx-ingress] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10014 - aris-keda] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,"[pr-cp-reg-10014 - aris-sealed-secrets] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10014 - aris-cluster-autoscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10033 - aris-kube-prometheus-stack] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf-0 (settcpkeepalivetime) is restarting  times / 5 minutes.
1,[pr-cp-reg-10006 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env/backup-tenants-28124475 failed to complete.
1,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_k8s_pod_restarted - The pod pr-customer-env-spark/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10002 - kasten-io] - NodeRAIDDegraded - RAID array '/dev/nvme2n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10050 - pr-customer-env] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter management/management-external-dns"
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kasten-io/executor-svc"
3,[pr-cp-reg-10014 - pr-customer-env-spark] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10009 - kube-system] - KubeQuotaAlmostFull - Namespace kube-system is using  of its cpu quota.
1,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter management/ae-aw-euc1-15995-external-dns-56db7877c9-9mtn7"
3,[pr-cp-reg-10021 - pr-customer-env] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
0,"[pr-cp-reg-10011 - aris-kube-prometheus-stack] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10027 - aris-keda] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_argocd_app_out_of_sync - The ArgoCD app 0137b8d8487077523c9bc7efe6b90af6636a5722148492370b387532bae836dd is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10007 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10005 - kube-system] - KubeQuotaExceeded - Namespace kube-system is using  of its memory quota.
3,[pr-cp-reg-10029 - aris-cluster-autoscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10026 - aris-keda] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on eth2 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10005 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API kubernetes/kube-system has been only % available over the last 10m.
3,[pr-cp-reg-10007 - aris-keda] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container external-dns in pod efs-csi-controller-94d8968c6-6ts4k.
1,[pr-cp-reg-10027 - aris-keda] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10027 cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container dashboarding in pod serviceenabling-0.
3,[pr-cp-reg-10016 - aris-nginx-ingress] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10041 - pr-customer-env] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env/pr-cp-reg-10041 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10007 - kube-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10023 - aris-sealed-secrets] - ARIS_argocd_app_out_of_sync - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10037 - aris-kube-downscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10001 - aris-keda] - ARIS_k8s_pod_crash_looping - Pod aris-keda/keda-operator-metrics-apiserver (kube-state-metrics) is restarting  times / 5 minutes.
4,[pr-cp-reg-10019 - aris-nginx-ingress] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-nginx-ingress has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10050 - aris-nginx-ingress] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_argocd_app_progressing - The ArgoCD app aris-solutionsgallery-secret is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10004 - aris-kube-downscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10025 - pr-customer-env-spark] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env-spark has too many active connections. More than 90% of available connections are used.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kube-state-metrics in pod gateway.
3,[pr-cp-reg-10011 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10033 - kube-node-lease] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
1,[pr-cp-reg-10050 - aris-sealed-secrets] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10036 - management] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10006 - aris-sealed-secrets] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-sealed-secrets has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10020 - aris-keda] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10046 - kube-node-lease] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
4,[pr-cp-reg-10001 - pr-customer-env-spark] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env-spark has faced > 5 deadlocks in last minute.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container aws-vpc-cni-init in pod coredns-cbbbbb9cb-djqr5.
1,[pr-cp-reg-10037 - pr-customer-env] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10021 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10016 - kube-node-lease] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface eth1 has encountered  transmit errors in the last two minutes.
0,"[pr-cp-reg-10037 - aris-kube-downscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container external-dns in pod argocd-server.
1,[pr-cp-reg-10010 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container ambassador in pod gateway.
4,[pr-cp-reg-10001 - aris-spot] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot for container kube-state-metrics in pod spotinst-kubernetes-cluster-controller-cd9fc697f-hx99n.
3,[pr-cp-reg-10004 - management] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  receive errors in the last two minutes.
2,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10003 cluster.
3,[pr-cp-reg-10034 - kube-public] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10039 - aris-keda] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-svc in pod gateway-cc7bc5b8d-jqbkl.
1,[pr-cp-reg-10008 - kasten-io] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10046 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10015 - pr-customer-env] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10015 cluster.
3,[pr-cp-reg-10037 - basic-application-bootstrapping] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10003 - aris-sealed-secrets] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10044 - aris-nginx-ingress] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10001 - pr-customer-env] - ConfigReloaderSidecarErrors - Errors encountered while the kanister-job-6gvp5 config-reloader sidecar attempts to sync config in pr-customer-env namespace. As a result, configuration for service running in kanister-job-6gvp5 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10037 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10036 - pr-customer-env] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10034 - pr-customer-env-spark] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10049 - aris-sealed-secrets] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10028 - default] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10035 - kube-node-lease] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,"[pr-cp-reg-10020 - falcon-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter falcon-system/falcon-falcon-sensor-2bjwf"
1,[pr-cp-reg-10002 - aris-keda] - ARIS_k8s_pod_crash_looping - Pod aris-keda/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (keda-operator) is restarting  times / 5 minutes.
3,[pr-cp-reg-10002 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme5n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod ebs-csi-controller-86cb997fdc-t2z6p.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env/pr-cp-reg-10005 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10008 - aris-kube-downscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10006 - aris-spot] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10006 cluster.
1,[pr-cp-reg-10011 - kube-system] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10040 - aris-cluster-autoscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10021 - pr-customer-env-spark] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10021 cluster.
0,"[pr-cp-reg-10024 - aris-cluster-autoscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10050 - aris-kube-downscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,"[pr-cp-reg-10021 - aris-kube-downscaler] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller-cbbc8bd6b-qq22k config-reloader sidecar attempts to sync config in aris-kube-downscaler namespace. As a result, configuration for service running in sealed-secrets-controller-cbbc8bd6b-qq22k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding (cloudsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10003 - kube-node-lease] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10047 - pr-customer-env-spark] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,"[pr-cp-reg-10015 - falcon-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter falcon-system/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container events-svc in pod logging-svc-58b457d7d8-pfqmx.
3,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-sealed-secrets/pr-cp-reg-10034 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-nginx-ingress/pr-cp-reg-10003 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10037 - aris-kube-prometheus-stack] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod serviceenabling.
1,[pr-cp-reg-10012 - pr-customer-env] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env/pr-cp-reg-10012 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration-0 (processboard) is restarting  times / 5 minutes.
1,[pr-cp-reg-10032 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot-metrics/pr-cp-reg-10032 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124280-qwm2q (adsadmin) is restarting  times / 5 minutes.
3,[pr-cp-reg-10042 - aris-kube-downscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-downscaler has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10035 - aris-spot] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10029 - aris-sealed-secrets] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus-0 (init-config-reloader) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod argocd-redis-ha-haproxy.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env-spark has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_argocd_app_progressing - The ArgoCD app aris-secret-smtp is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10012 - aris-nginx-ingress] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
2,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_k8s_volume_out_of_disk_space - Volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana is full (< 5% free space left) and should be increased.
3,"[pr-cp-reg-10019 - aris-spot-metrics] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot-metrics namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container cert-manager in pod argocd-server.
3,[pr-cp-reg-10047 - aris-nginx-ingress] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10013 - aris-keda] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-keda/pr-cp-reg-10013 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (abs) is restarting  times / 5 minutes.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface nvme2 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper-0 (ces) is restarting  times / 5 minutes.
3,[pr-cp-reg-10021 - aris-kube-prometheus-stack] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fznn7 (node-exporter) is restarting  times / 5 minutes.
1,[pr-cp-reg-10048 - pr-customer-env] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env/pr-cp-reg-10048 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10016 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10036 - aris-sealed-secrets] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-sealed-secrets has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10042 - pr-customer-env] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-nginx-ingress/exported_job failed to complete.
3,[pr-cp-reg-10016 - kube-system] - KubeNodeReadinessFlapping - The readiness status of node ip-10-0-138-163.eu-central-1.compute.internal has changed  times in the last 15 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processboard (cdf) is restarting  times / 5 minutes.
1,[pr-cp-reg-10029 - kube-system] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10049 - kube-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10022 - pr-customer-env-spark] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10037 - pr-customer-env-spark] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
0,"[pr-cp-reg-10046 - pr-customer-env-spark] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10023 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container collaboration in pod kanister-job-7lbrs.
1,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - AlertmanagerClusterCrashlooping -  of Alertmanager instances within the kubelet cluster have restarted at least 5 times in the last 10m.
1,[pr-cp-reg-10023 - kube-public] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10023 - aris-cluster-autoscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10004 - aris-cluster-autoscaler] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10030 - aris-spot] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10022 - aris-spot-metrics] - ARIS_postgresql_down - The postgres pod of namespace aris-spot-metrics is down.
1,[pr-cp-reg-10035 - pr-customer-env-spark] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10035 cluster.
3,[pr-cp-reg-10037 - pr-customer-env-spark] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-147-93.eu-central-1.compute.internal in pr-customer-env-spark/pr-cp-reg-10037 cluster is .
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana (prometheus) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod ae-aw-euc1-15995-aws-load-balancer-controller.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod dashboarding.
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10029 - kube-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10006 - kube-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10027 - aris-kube-prometheus-stack] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on eni0039e8a4ad3 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus-0 has only found  members of the aris-kube-prometheus-stack-grafana cluster.
1,[pr-cp-reg-10031 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - aris-keda] - ARIS_k8s_pod_crash_looping - Pod aris-keda/keda-operator-metrics-apiserver (controller) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - aris-spot] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot for container spotinst-kubernetes-cluster-controller in pod spot-ocean-metric-exporter-56dfbdbbcb-82vqg.
0,"[pr-cp-reg-10034 - aris-spot-metrics] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10027 - aris-sealed-secrets] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10001 - aris-keda] - ConfigReloaderSidecarErrors - Errors encountered while the keda-operator-metrics-apiserver config-reloader sidecar attempts to sync config in aris-keda namespace. As a result, configuration for service running in keda-operator-metrics-apiserver may be stale and cannot be updated anymore."
3,[pr-cp-reg-10014 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10034 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container octopus in pod octopus.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28125915-9ps25 (dashboarding) is restarting  times / 5 minutes.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on lo at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10011 - kasten-io] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10007 - aris-kube-downscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10029 - aris-cluster-autoscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10017 - pr-customer-env] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10050 - aris-sealed-secrets] - ARIS_postgresql_down - The postgres pod of namespace aris-sealed-secrets is down.
3,[pr-cp-reg-10035 - aris-cluster-autoscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10023 - aris-nginx-ingress] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10016 - aris-sealed-secrets] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-sealed-secrets has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10047 - aris-kube-prometheus-stack] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter kasten-io/crypto-svc-67b9b7c65d-gg6z9"
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10006 - management] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme6n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container abs in pod kanister-job-6gvp5.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter pr-customer-env/backup-logs-28124160-s7ps9"
2,[pr-cp-reg-10043 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10043 cluster may be low for segment merges.
3,[pr-cp-reg-10049 - aris-sealed-secrets] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10011 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod argocd-application-controller.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface shm has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_k8s_node_pid_pressure - ip-10-0-147-93.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kube-state-metrics in pod backup-tenants-28124475-fjqvp.
1,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10042 - default] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10035 - aris-keda] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
0,"[pr-cp-reg-10021 - pr-customer-env] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,[pr-cp-reg-10023 - aris-keda] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10031 - pr-customer-env] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container efs-plugin in pod argocd-redis-ha-server-0.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the aris-kube-prometheus-stack-operator/aris-kube-prometheus-stack-operator targets in aris-kube-prometheus-stack namespace are down.
0,"[pr-cp-reg-10049 - pr-customer-env-spark] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10004 - pr-customer-env-spark] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in pr-customer-env-spark namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app tmdata is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10038 - aris-spot] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container logging-svc in pod metering-svc-685b59dfb-pcknm.
3,[pr-cp-reg-10006 - kube-public] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
4,[pr-cp-reg-10001 - aris-spot] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot for container controller in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
1,[pr-cp-reg-10045 - aris-spot] - ARIS_postgresql_down - The postgres pod of namespace aris-spot is down.
1,[pr-cp-reg-10001 - pr-customer-env-spark] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env-spark is down.
0,"[pr-cp-reg-10048 - aris-keda] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10041 - aris-kube-downscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-downscaler/exported_job failed to complete.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/cert-manager-cainjector-7d8985bbc8-x7www"
1,[pr-cp-reg-10015 - aris-sealed-secrets] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10019 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10040 - aris-keda] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10036 - aris-kube-downscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10020 - aris-kube-prometheus-stack] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10004 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API loop/kube-system has been only % available over the last 10m.
3,[pr-cp-reg-10029 - pr-customer-env] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10028 - aris-kube-prometheus-stack] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10036 - aris-kube-prometheus-stack] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10007 - aris-spot] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
2,[pr-cp-reg-10014 - pr-customer-env] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10014 cluster may be low for segment merges.
3,"[pr-cp-reg-10001 - pr-customer-env-spark] - ConfigReloaderSidecarErrors - Errors encountered while the pr-customer-env-spark-spark-operat-wh-init-hwzrg config-reloader sidecar attempts to sync config in pr-customer-env-spark namespace. As a result, configuration for service running in pr-customer-env-spark-spark-operat-wh-init-hwzrg may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-driver-registrar in pod argocd-redis-ha-server.
3,[pr-cp-reg-10003 - kube-system] - KubeletServerCertificateRenewalErrors - Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal has failed to renew its server certificate ( errors in the last 5 minutes).
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28123035-nswx7 (kanister-sidecar) is restarting  times / 5 minutes.
3,[pr-cp-reg-10039 - aris-kube-downscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10035 - aris-spot] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot/pr-cp-reg-10035 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10026 - aris-cluster-autoscaler] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array 'tmpfs' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10019 - aris-keda] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10042 - aris-sealed-secrets] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-sealed-secrets/pr-cp-reg-10042 cluster is .
3,"[pr-cp-reg-10025 - default] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in default namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10030 - pr-customer-env-spark] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10016 - pr-customer-env] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container container in pod cdf-0.
3,[pr-cp-reg-10039 - falcon-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app storage is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10012 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_k8s_pod_restarted - The pod aris-cluster-autoscaler/aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10035 - aris-nginx-ingress] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10035 - kube-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10002 - kube-public] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10002 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10030 - aris-sealed-secrets] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_k8s_node_memory_pressure - ip-10-0-138-163.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10037 - aris-kube-prometheus-stack] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10004 - kasten-io] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10021 - kube-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to opsgenie sent from any instance in the aris-kube-prometheus-stack-alertmanager cluster is .
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter failed to send  of notifications to pagerduty.
3,[pr-cp-reg-10011 - pr-customer-env] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme2n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10025 - pr-customer-env-spark] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container alertmanager in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,"[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/data-volume-elasticsearch-default-0 will run full within 24 hours."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/octopus-0 (dashboarding) is restarting  times / 5 minutes.
3,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces-0 (kanister-sidecar) is restarting  times / 5 minutes.
3,[pr-cp-reg-10044 - aris-spot] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot/pr-cp-reg-10044 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10016 - kasten-io] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10049 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10038 - aris-keda] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10001 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API kubernetes/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
1,[pr-cp-reg-10011 - aris-kube-downscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,"[pr-cp-reg-10008 - aris-spot] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
0,"[pr-cp-reg-10005 - aris-kube-prometheus-stack] - Watchdog - This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the ""DeadMansSnitch"" integration in PagerDuty."
3,[pr-cp-reg-10044 - aris-sealed-secrets] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container ces in pod backup-tenants-28123035-nswx7.
3,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10013 - aris-sealed-secrets] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres (create-es-index-template) is restarting  times / 5 minutes.
1,[pr-cp-reg-10004 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10005 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
2,[pr-cp-reg-10048 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10048 cluster.
2,[pr-cp-reg-10026 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10026 cluster.
1,[pr-cp-reg-10007 - kube-system] - KubeletClientCertificateExpiration - Client certificate for Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal expires in .
3,[pr-cp-reg-10028 - aris-nginx-ingress] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10012 - kube-system] - KubeletServerCertificateRenewalErrors - Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal has failed to renew its server certificate ( errors in the last 5 minutes).
3,[pr-cp-reg-10009 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana in pod aris-kube-prometheus-stack-operator-78b758c475-qs69v.
3,[pr-cp-reg-10016 - pr-customer-env-spark] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10015 - pr-customer-env] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""shm"" changing its up status often on node-exporter aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus"
3,[pr-cp-reg-10003 - aris-keda] - ARIS_argocd_app_progressing - The ArgoCD app scaledjob is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-cluster-autoscaler/pr-cp-reg-10022 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124400-ktxxf (adsadmin) is restarting  times / 5 minutes.
3,[pr-cp-reg-10029 - pr-customer-env-spark] - ARIS_host_oom_kill_detected - OOM kill detected.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod argocd-server-74fbd754cb-n2sgr.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'nvme3' needs attention and possibly a disk swap.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the sealed-secrets-controller/aris-kube-prometheus-stack-grafana targets in aris-kube-prometheus-stack namespace are down.
3,[pr-cp-reg-10047 - pr-customer-env-spark] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod collaboration.
3,[pr-cp-reg-10033 - kube-public] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10006 - pr-customer-env] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme3n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10029 - aris-nginx-ingress] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app useLegacyHeatmapPanel is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10037 - aris-nginx-ingress] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_argocd_app_unknown - The ArgoCD app aris-secret-s3 is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10047 - falcon-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10032 - aris-kube-prometheus-stack] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10017 - aris-nginx-ingress] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10031 - pr-customer-env-spark] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env-spark has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10009 - pr-customer-env] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10028 - kube-system] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10019 - aris-sealed-secrets] - ARIS_argocd_app_progressing - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10045 - aris-nginx-ingress] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-nginx-ingress has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10011 - kube-system] - KubeletClientCertificateExpiration - Client certificate for Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal expires in .
3,[pr-cp-reg-10001 - aris-keda] - ARIS_argocd_app_unknown - The ArgoCD app scaledobject is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28124475-fjqvp (controller) is restarting  times / 5 minutes.
1,[pr-cp-reg-10016 - kube-system] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod collaboration-0.
3,[pr-cp-reg-10008 - default] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10039 - aris-keda] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10013 - aris-sealed-secrets] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container setmaxmapcount in pod dashboarding-0.
2,[pr-cp-reg-10011 - aris-spot] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10011 cluster.
4,[pr-cp-reg-10010 - pr-customer-env] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10040 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-nginx-ingress/pr-cp-reg-10040 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface 0 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10010 - aris-keda] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10010 cluster.
3,[pr-cp-reg-10028 - aris-keda] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10041 - aris-sealed-secrets] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10049 - pr-customer-env] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10012 - aris-sealed-secrets] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10033 - aris-kube-prometheus-stack] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10012 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10019 - aris-kube-prometheus-stack] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10025 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,"[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/tmdata will run full within 24 hours."
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana-sc-dashboard in pod alertmanager-aris-kube-prometheus-stack-alertmanager.
1,[pr-cp-reg-10024 - kasten-io] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10029 - aris-sealed-secrets] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10011 - pr-customer-env] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10035 - aris-kube-prometheus-stack] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10029 - aris-nginx-ingress] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10018 - pr-customer-env-spark] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10025 - pr-customer-env] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10009 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10017 - basic-application-bootstrapping] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10044 - kube-public] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,"[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ConfigReloaderSidecarErrors - Errors encountered while the prometheus-aris-kube-prometheus-stack-prometheus config-reloader sidecar attempts to sync config in aris-kube-prometheus-stack namespace. As a result, configuration for service running in prometheus-aris-kube-prometheus-stack-prometheus may be stale and cannot be updated anymore."
1,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10021 - default] - CPUThrottlingHigh -  throttling of CPU in namespace default for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,"[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-prometheus-node-exporter config-reloader sidecar attempts to sync config in aris-kube-prometheus-stack namespace. As a result, configuration for service running in aris-kube-prometheus-stack-prometheus-node-exporter may be stale and cannot be updated anymore."
3,[pr-cp-reg-10041 - aris-kube-downscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10007 - aris-cluster-autoscaler] - ARIS_k8s_pod_restarted - The pod aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10048 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on tmpfs at <IP_ADDRESS>:<PORT> has only % available inodes left.
2,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10040 cluster.
3,"[pr-cp-reg-10006 - aris-nginx-ingress] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-nginx-ingress/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface eni006a31b57f1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (container) is restarting  times / 5 minutes.
1,[pr-cp-reg-10010 - pr-customer-env] - ARIS_k8s_node_out_of_disk - ip-10-0-138-163.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,"[pr-cp-reg-10010 - falcon-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter falcon-system/falcon-falcon-sensor-2bjwf"
1,[pr-cp-reg-10006 - aris-keda] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-kube-prometheus-stack/pr-cp-reg-10005 cluster is .
3,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,"[pr-cp-reg-10005 - aris-nginx-ingress] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-nginx-ingress/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
1,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_k8s_pod_restarted - The pod aris-cluster-autoscaler/aris-cluster-autoscaler-aws-cluster-autoscaler has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
2,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10045 cluster.
4,[pr-cp-reg-10008 - aris-sealed-secrets] - CPUThrottlingHigh -  throttling of CPU in namespace aris-sealed-secrets for container controller in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10008 - pr-customer-env] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter management/argocd-application-controller"
3,[pr-cp-reg-10008 - kube-node-lease] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app logging-pv-claim is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10026 - aris-sealed-secrets] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10002 - aris-sealed-secrets] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10007 - aris-cluster-autoscaler] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10031 - aris-cluster-autoscaler] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container catalog-svc in pod logging-svc-58b457d7d8-pfqmx.
1,[pr-cp-reg-10004 - aris-keda] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10002 - management] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10006 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres-0 (prometheus-exporter) is restarting  times / 5 minutes.
3,[pr-cp-reg-10004 - kube-node-lease] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
1,[pr-cp-reg-10007 - pr-customer-env] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10024 - aris-kube-prometheus-stack] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10015 - aris-spot] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-6gvp5 (portalserver) is restarting  times / 5 minutes.
3,[pr-cp-reg-10025 - aris-cluster-autoscaler] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10013 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10043 - aris-spot] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding (create-es-index-template) is restarting  times / 5 minutes.
3,[pr-cp-reg-10013 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10038 - aris-spot-metrics] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10021 - aris-nginx-ingress] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10001 - kube-node-lease] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10012 - kube-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10039 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env-spark/pr-cp-reg-10039 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10021 - pr-customer-env] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env/pr-cp-reg-10021 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10047 - kube-system] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10028 - aris-kube-downscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10036 - aris-nginx-ingress] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10046 - aris-keda] - ARIS_host_oom_kill_detected - OOM kill detected.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/octopus"
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_unknown - The ArgoCD app scaledobject is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
0,"[pr-cp-reg-10003 - pr-customer-env] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10031 - pr-customer-env-spark] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
2,[pr-cp-reg-10012 - aris-spot] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10012 cluster may be low for segment merges.
3,[pr-cp-reg-10050 - kube-node-lease] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres-0 (ces) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container collaboration in pod create-es-index-template-61d85-nz2dz.
1,[pr-cp-reg-10024 - pr-customer-env-spark] - ARIS_k8s_node_pid_pressure - ip-10-0-147-93.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10040 - aris-spot] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
2,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10006 cluster.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fznn7 failed to send  of notifications to email.
1,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,"[pr-cp-reg-10004 - aris-kube-downscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container octopus in pod backup-logs-28124160-s7ps9.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_restarted - The pod pr-customer-env/abs-0 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
4,[pr-cp-reg-10004 - aris-kube-downscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-downscaler for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
0,"[pr-cp-reg-10014 - aris-spot] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10002 - management] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app jobs-pv-claim is progressing for longer than 15 minutes and requires manual intervention.
3,"[pr-cp-reg-10005 - aris-cluster-autoscaler] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-cluster-autoscaler/sealed-secrets-controller-cbbc8bd6b-qq22k"
1,[pr-cp-reg-10010 - pr-customer-env] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10017 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10007 - pr-customer-env-spark] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10008 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-downscaler/pr-cp-reg-10008 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10001 - aris-keda] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-keda/keda-operator-5867cd94d9-vhxdf"
2,[pr-cp-reg-10020 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10020 cluster.
3,[pr-cp-reg-10049 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10027 - pr-customer-env] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,"[pr-cp-reg-10045 - aris-cluster-autoscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,"[pr-cp-reg-10004 - kube-public] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-public namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10033 - aris-nginx-ingress] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
0,"[pr-cp-reg-10042 - aris-nginx-ingress] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10010 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10030 - pr-customer-env-spark] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10008 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-downscaler/pr-cp-reg-10008 health status has been RED for at least 2 minutes.
0,"[pr-cp-reg-10040 - pr-customer-env] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerClusterDown -  of Alertmanager instances within the aris-kube-prometheus-stack-grafana cluster have been up for less than half of the last 5m.
3,[pr-cp-reg-10014 - aris-kube-downscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_k8s_node_disk_pressure - ip-10-0-143-220.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - AlertmanagerClusterDown -  of Alertmanager instances within the kubelet cluster have been up for less than half of the last 5m.
3,[pr-cp-reg-10045 - pr-customer-env] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10024 - aris-sealed-secrets] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
2,[pr-cp-reg-10013 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10013 cluster may be low for segment merges.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_argocd_app_out_of_sync - The ArgoCD app 04a76c021edb62fce481f9769a5a57b9da39b97aa3b7ea282e36427592da5183 is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10036 - aris-keda] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10006 - aris-nginx-ingress] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10007 - aris-keda] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10008 - aris-kube-downscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10021 - aris-kube-downscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10004 - aris-sealed-secrets] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
2,[pr-cp-reg-10014 - aris-spot] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10014 cluster may be low for segment merges.
1,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
0,"[pr-cp-reg-10032 - pr-customer-env] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10050 - kube-node-lease] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
2,[pr-cp-reg-10045 - pr-customer-env-spark] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10045 cluster.
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10012 - pr-customer-env] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10023 - aris-cluster-autoscaler] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine (container) is restarting  times / 5 minutes.
3,[pr-cp-reg-10022 - aris-kube-downscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10025 - aris-spot-metrics] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10003 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10044 - aris-keda] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
0,"[pr-cp-reg-10031 - aris-spot] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10007 - aris-sealed-secrets] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller config-reloader sidecar attempts to sync config in aris-sealed-secrets namespace. As a result, configuration for service running in sealed-secrets-controller may be stale and cannot be updated anymore."
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_sealedsecret_not_unsealed - There are secrets that could not be unsealed in last 30 minutes. Check the sealing key.
3,[pr-cp-reg-10032 - kasten-io] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10029 - aris-nginx-ingress] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10035 - aris-keda] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10009 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_sealedsecret_not_unsealed - There are secrets that could not be unsealed in last 30 minutes. Check the sealing key.
3,[pr-cp-reg-10016 - kube-system] - KubeletServerCertificateRenewalErrors - Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal has failed to renew its server certificate ( errors in the last 5 minutes).
3,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod octopus-0.
3,[pr-cp-reg-10019 - pr-customer-env] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kube-state-metrics in pod adsadmin.
1,[pr-cp-reg-10013 - aris-cluster-autoscaler] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10011 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container aggregatedapis-svc in pod state-svc-745f7ffd6d-jfrkr.
1,[pr-cp-reg-10017 - aris-sealed-secrets] - ARIS_postgresql_down - The postgres pod of namespace aris-sealed-secrets is down.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container abs in pod portalserver.
1,[pr-cp-reg-10013 - aris-sealed-secrets] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver-0 (kanister-sidecar) is restarting  times / 5 minutes.
1,[pr-cp-reg-10005 - management] - NodeRAIDDegraded - RAID array '/dev/nvme5n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array 'shm' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10036 - pr-customer-env-spark] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-147-93.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10030 - aris-nginx-ingress] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10013 - kube-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper-0 (elastic-metrics-sidecar) is restarting  times / 5 minutes.
4,[pr-cp-reg-10048 - aris-kube-prometheus-stack] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-prometheus-stack has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_k8s_pod_restarted - The pod aris-nginx-ingress/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container jobs-svc in pod jobs-svc.
3,[pr-cp-reg-10021 - kube-public] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10016 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10033 - aris-keda] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10009 - aris-cluster-autoscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_argocd_app_out_of_sync - The ArgoCD app 705ccec2911bb44176bb5a664bb99d8cb0e9ff09e31f6355ef29837c92560a00 is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10048 - aris-keda] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
2,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10002 cluster.
2,[pr-cp-reg-10007 - aris-spot] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10007 cluster.
3,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod portalserver-0.
1,[pr-cp-reg-10045 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10004 - default] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10017 - aris-cluster-autoscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10030 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10031 - default] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10003 - management] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controllermanager-svc in pod metering-svc-685b59dfb-pcknm.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available inodes left.
2,[pr-cp-reg-10009 - aris-spot] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10009 cluster may be low for segment merges.
1,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-sealed-secrets/pr-cp-reg-10020 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10009 - aris-sealed-secrets] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
2,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10020 cluster.
3,[pr-cp-reg-10031 - falcon-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10037 - aris-spot-metrics] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot-metrics namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - kube-system] - KubeletClientCertificateExpiration - Client certificate for Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal expires in .
3,[pr-cp-reg-10039 - aris-keda] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10001 - aris-keda] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10010 - kube-public] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
3,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10016 - kube-system] - KubeQuotaAlmostFull - Namespace kube-system is using  of its cpu quota.
4,[pr-cp-reg-10048 - kube-node-lease] - CPUThrottlingHigh -  throttling of CPU in namespace kube-node-lease for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""lo"" changing its up status often on node-exporter aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus-0"
1,[pr-cp-reg-10014 - aris-spot] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container schema-upgrade-check in pod catalog-svc-bf949fc65-9cblt.
1,[pr-cp-reg-10030 - aris-sealed-secrets] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-0 failed to send  of notifications to slack.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/register-smtp-server-6d78c-stqpc (ces) is restarting  times / 5 minutes.
1,[pr-cp-reg-10049 - pr-customer-env-spark] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10007 - pr-customer-env] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs (cloudsearch) is restarting  times / 5 minutes.
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_critical - Latency to the upstream service aris-nginx-ingress/argocd-server has been critically high (more than 30 seconds) for the last 3 minutes. Service is unresponsive and should be restarted.
1,[pr-cp-reg-10049 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container controller in pod loadbalancer-0.
3,[pr-cp-reg-10024 - aris-sealed-secrets] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10028 - aris-kube-downscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10048 - aris-spot] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10012 - aris-cluster-autoscaler] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container create-es-index-template in pod elasticsearch-default-0.
4,[pr-cp-reg-10005 - kube-system] - KubeQuotaAlmostFull - Namespace kube-system is using  of its cpu quota.
3,[pr-cp-reg-10002 - aris-sealed-secrets] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10020 - kube-node-lease] - CPUThrottlingHigh -  throttling of CPU in namespace kube-node-lease for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10046 - pr-customer-env] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10032 - aris-kube-prometheus-stack] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10011 - aris-cluster-autoscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10022 - aris-kube-downscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10005 - pr-customer-env] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10047 - management] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10005 - kube-node-lease] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
1,[pr-cp-reg-10031 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,"[pr-cp-reg-10042 - aris-nginx-ingress] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10004 - aris-keda] - ARIS_argocd_app_unknown - The ArgoCD app triggerauthentication is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10035 - kube-node-lease] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10041 - aris-kube-prometheus-stack] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin (kanister-sidecar) is restarting  times / 5 minutes.
1,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10033 - aris-nginx-ingress] - ARIS_postgresql_down - The postgres pod of namespace aris-nginx-ingress is down.
1,[pr-cp-reg-10023 - aris-spot] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10002 - kube-system] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
1,[pr-cp-reg-10007 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
1,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10025 - falcon-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10014 - aris-keda] - ARIS_postgresql_down - The postgres pod of namespace aris-keda is down.
1,[pr-cp-reg-10004 - falcon-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-sealed-secrets/pr-cp-reg-10020 cluster is .
3,[pr-cp-reg-10003 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API prometheus/kube-system has been only % available over the last 10m.
1,[pr-cp-reg-10003 - aris-spot] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10003 cluster.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-prometheus-stack is down.
1,[pr-cp-reg-10029 - aris-nginx-ingress] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,"[pr-cp-reg-10002 - aris-spot-metrics] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot-metrics namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10047 - aris-kube-prometheus-stack] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterCrashlooping -  of Alertmanager instances within the aris-kube-prometheus-stack-alertmanager cluster have restarted at least 5 times in the last 10m.
1,[pr-cp-reg-10006 - aris-keda] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container container in pod create-es-index-template-61d85-nz2dz.
3,[pr-cp-reg-10045 - pr-customer-env] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10048 - aris-cluster-autoscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
2,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/solutiondata is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10022 - pr-customer-env-spark] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10001 - aris-nginx-ingress] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
0,"[pr-cp-reg-10032 - aris-kube-downscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controller in pod aggregatedapis-svc.
3,[pr-cp-reg-10017 - pr-customer-env] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env/pr-cp-reg-10017 health status has been YELLOW for at least 20 minutes.
0,"[pr-cp-reg-10050 - pr-customer-env-spark] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10036 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
1,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10017 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
4,[pr-cp-reg-10040 - kube-public] - CPUThrottlingHigh -  throttling of CPU in namespace kube-public for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (processengine) is restarting  times / 5 minutes.
2,[pr-cp-reg-10014 - aris-spot] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10014 cluster.
3,[pr-cp-reg-10036 - aris-keda] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container liveness-probe in pod argocd-redis-ha-haproxy-84b857bc4b-8lfwq.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k (node-exporter) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10003 - aris-sealed-secrets] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-sealed-secrets namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10004 - aris-spot] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot/pr-cp-reg-10004 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10047 - pr-customer-env] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10050 - aris-spot-metrics] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10023 - aris-kube-prometheus-stack] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10020 - kube-node-lease] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10012 - kasten-io] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10013 - pr-customer-env] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface eth2 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-cluster-autoscaler is down.
3,[pr-cp-reg-10036 - aris-sealed-secrets] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10021 - kube-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10014 - aris-nginx-ingress] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10003 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API loadbalance/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
3,"[pr-cp-reg-10002 - aris-spot] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10038 - basic-application-bootstrapping] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10045 - aris-keda] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/serviceenabling-0 (ces) is restarting  times / 5 minutes.
3,[pr-cp-reg-10041 - pr-customer-env] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10013 - kube-node-lease] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod cdf-0.
3,[pr-cp-reg-10043 - aris-nginx-ingress] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10002 - management] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme5n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10003 - aris-keda] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10036 - aris-nginx-ingress] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10048 - pr-customer-env] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10012 - kube-public] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10019 - aris-sealed-secrets] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the node-exporter/alertmanager-operated targets in aris-kube-prometheus-stack namespace are down.
1,[pr-cp-reg-10013 - aris-keda] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10038 - basic-application-bootstrapping] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10004 - aris-keda] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10001 - pr-customer-env] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme4n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10046 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-sealed-secrets/pr-cp-reg-10046 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10016 - aris-sealed-secrets] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env-spark/pr-cp-reg-10046 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10001 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10009 - aris-keda] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-0 has only found  members of the kubelet cluster.
3,[pr-cp-reg-10021 - aris-nginx-ingress] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on eth2 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10036 - aris-kube-prometheus-stack] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10038 - aris-kube-prometheus-stack] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana will run full within 24 hours."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod serviceenabling.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_node_memory_pressure - ip-10-0-138-163.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10038 - pr-customer-env-spark] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10027 - aris-sealed-secrets] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-822lp"
1,[pr-cp-reg-10006 - aris-sealed-secrets] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app prometheusAzureOverrideAudience is out of sync for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10029 - kube-public] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-public namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10009 - aris-kube-downscaler] - ARIS_k8s_pod_restarted - The pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10001 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme5n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10037 - pr-customer-env-spark] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env-spark/exported_job failed to complete.
1,[pr-cp-reg-10035 - pr-customer-env] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10019 - pr-customer-env] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10046 - pr-customer-env] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10049 - aris-sealed-secrets] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10002 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10003 - aris-spot] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod umcadmin.
3,[pr-cp-reg-10036 - aris-kube-downscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10005 - aris-sealed-secrets] - ARIS_k8s_pod_crash_looping - Pod aris-sealed-secrets/sealed-secrets-controller (kube-state-metrics) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/tm-0 (zookeeper) is restarting  times / 5 minutes.
3,[pr-cp-reg-10011 - aris-cluster-autoscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10049 - kube-public] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
3,[pr-cp-reg-10045 - aris-kube-downscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
0,"[pr-cp-reg-10029 - kube-public] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10016 - aris-cluster-autoscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10003 - basic-application-bootstrapping] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10032 - aris-keda] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10032 cluster.
3,[pr-cp-reg-10028 - aris-spot] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10019 - pr-customer-env-spark] - ARIS_argocd_app_progressing - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is progressing for longer than 15 minutes and requires manual intervention.
2,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10033 cluster.
1,[pr-cp-reg-10017 - aris-spot] - ARIS_postgresql_down - The postgres pod of namespace aris-spot is down.
1,[pr-cp-reg-10005 - kasten-io] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_k8s_pod_restarted - The pod pr-customer-env-spark/pr-customer-env-spark-spark-operator-74d54645cb-5gpbl has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/adsadmin (octopus) is restarting  times / 5 minutes.
3,[pr-cp-reg-10019 - aris-spot-metrics] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot-metrics has too many active connections. More than 90% of available connections are used.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eni0039e8a4ad3"" changing its up status often on node-exporter aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/processengine"
3,[pr-cp-reg-10007 - kube-system] - KubeNodeNotReady - ip-10-0-138-163.eu-central-1.compute.internal has been unready for more than 15 minutes.
1,[pr-cp-reg-10023 - falcon-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10001 - kube-node-lease] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10006 - aris-cluster-autoscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-cluster-autoscaler namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10039 - aris-sealed-secrets] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10007 - aris-nginx-ingress] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10011 - aris-sealed-secrets] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10032 - aris-sealed-secrets] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_argocd_app_progressing - The ArgoCD app aris-secret-smtp is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10010 - basic-application-bootstrapping] - CPUThrottlingHigh -  throttling of CPU in namespace basic-application-bootstrapping for container kube-state-metrics in pod basic-application-bootstrapping-2wp59.
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10045 - pr-customer-env] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10039 - pr-customer-env-spark] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10039 - aris-kube-prometheus-stack] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10049 - aris-kube-downscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10004 - aris-keda] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10004 - kube-system] - KubeletClientCertificateRenewalErrors - Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal has failed to renew its client certificate ( errors in the last 5 minutes).
3,[pr-cp-reg-10045 - aris-keda] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10018 - aris-keda] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-driver-registrar in pod argocd-redis-ha-haproxy.
1,[pr-cp-reg-10033 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-nginx-ingress/pr-cp-reg-10033 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_argocd_app_out_of_sync - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is out of sync for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod kanister-job-6kclt.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container frontend-svc in pod metering-svc.
1,[pr-cp-reg-10006 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-6kclt (tenant-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10002 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10003 - aris-keda] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container controller in pod octopus.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-nginx-ingress has too many active connections. More than 90% of available connections are used.
3,"[pr-cp-reg-10043 - aris-kube-prometheus-stack] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana will run full within 24 hours."
1,[pr-cp-reg-10026 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-cluster-autoscaler/pr-cp-reg-10026 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10014 - aris-spot-metrics] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot-metrics has too many active connections. More than 90% of available connections are used.
2,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10004 cluster may be low for segment merges.
3,[pr-cp-reg-10012 - management] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10020 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper-0 (zookeeper) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10001 - aris-cluster-autoscaler] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
1,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eni0039e8a4ad3"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics"
3,[pr-cp-reg-10009 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10010 - pr-customer-env] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container get-zone in pod zookeeper.
1,[pr-cp-reg-10020 - pr-customer-env-spark] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10026 - pr-customer-env] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10050 - falcon-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_argocd_app_unknown - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is in unknown state for longer than 10 minutes and requires manual intervention.
0,"[pr-cp-reg-10040 - aris-spot-metrics] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10039 - aris-cluster-autoscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10041 - aris-keda] - ARIS_postgresql_down - The postgres pod of namespace aris-keda is down.
3,[pr-cp-reg-10016 - aris-keda] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124400-ktxxf (serviceenabling) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10006 - aris-spot-metrics] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot-metrics namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-cluster-autoscaler/exported_job failed to complete.
1,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 (prometheus) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kanister-sidecar in pod kanister-job-6gvp5.
3,[pr-cp-reg-10001 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10035 - kube-public] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10002 - management] - NodeRAIDDegraded - RAID array '/dev/nvme2n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10011 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container abs in pod tm.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedReload - Configuration has failed to load for aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10045 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/backup-logs-28124400-ktxxf"
3,[pr-cp-reg-10022 - aris-sealed-secrets] - ARIS_argocd_app_unknown - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin (postgres) is restarting  times / 5 minutes.
3,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container umcadmin in pod backup-logs-28124280-qwm2q.
3,"[pr-cp-reg-10003 - kube-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kube-system/kube-proxy-5lqbj"
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme7n1"" changing its up status often on node-exporter management/argocd-server"
3,[pr-cp-reg-10020 - aris-spot] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-spot/pr-cp-reg-10020 cluster is .
1,[pr-cp-reg-10025 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10037 - pr-customer-env] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10012 - kube-node-lease] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,"[pr-cp-reg-10005 - kube-node-lease] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-node-lease namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container portalserver in pod serviceenabling-0.
3,[pr-cp-reg-10039 - aris-kube-downscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10014 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10022 - pr-customer-env-spark] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10044 - kube-node-lease] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container config-init in pod efs-csi-node-4cgpg.
3,[pr-cp-reg-10034 - aris-nginx-ingress] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10045 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-cluster-autoscaler/pr-cp-reg-10045 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10018 - aris-spot-metrics] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10014 - kube-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod register-smtp-server-6d78c-stqpc.
3,[pr-cp-reg-10001 - kasten-io] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container octopus in pod octopus-0.
3,[pr-cp-reg-10036 - aris-spot] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10031 - aris-nginx-ingress] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10047 - aris-keda] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-keda has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10033 - default] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10003 - aris-keda] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
0,"[pr-cp-reg-10050 - default] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10039 - pr-customer-env-spark] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10040 - aris-keda] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10040 - aris-keda] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10015 - aris-kube-downscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod octopus-0.
3,[pr-cp-reg-10043 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10044 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env-spark/pr-cp-reg-10044 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10020 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_suspended - CronJob pr-customer-env/backup-logs is suspended.
1,[pr-cp-reg-10018 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_k8s_pod_restarted - The pod aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10026 - aris-nginx-ingress] - ARIS_host_oom_kill_detected - OOM kill detected.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod adsadmin.
3,"[pr-cp-reg-10010 - aris-spot] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-spot/spotinst-kubernetes-cluster-controller-cd9fc697f-hx99n"
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver (admintools) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10006 - falcon-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter falcon-system/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10038 - aris-sealed-secrets] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10006 - management] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eth2"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager"
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin (log-backup) is restarting  times / 5 minutes.
3,[pr-cp-reg-10031 - aris-cluster-autoscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10015 - aris-spot-metrics] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container dashboardbff-svc in pod aggregatedapis-svc-5c7ccfdd78-slnk8.
3,[pr-cp-reg-10023 - aris-sealed-secrets] - ARIS_argocd_app_progressing - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is progressing for longer than 15 minutes and requires manual intervention.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme1n1"" changing its up status often on node-exporter management/argocd-repo-server"
3,[pr-cp-reg-10022 - aris-sealed-secrets] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10013 - kube-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10014 - pr-customer-env] - ARIS_k8s_node_memory_pressure - ip-10-0-138-163.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10010 - aris-nginx-ingress] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10012 - aris-nginx-ingress] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10012 cluster.
1,[pr-cp-reg-10044 - aris-keda] - ARIS_postgresql_down - The postgres pod of namespace aris-keda is down.
1,[pr-cp-reg-10003 - kube-system] - KubeletServerCertificateExpiration - Server certificate for Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal expires in .
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container config-reloader in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10003 - aris-keda] - ARIS_argocd_app_out_of_sync - The ArgoCD app scaledjob is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10002 - kube-system] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10043 - kube-node-lease] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
3,[pr-cp-reg-10006 - management] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10007 - aris-spot] - ConfigReloaderSidecarErrors - Errors encountered while the spot-ocean-metric-exporter-56dfbdbbcb-82vqg config-reloader sidecar attempts to sync config in aris-spot namespace. As a result, configuration for service running in spot-ocean-metric-exporter-56dfbdbbcb-82vqg may be stale and cannot be updated anymore."
3,[pr-cp-reg-10028 - aris-kube-downscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10041 - aris-sealed-secrets] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10004 - kube-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on nvme0 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10027 - aris-nginx-ingress] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
0,"[pr-cp-reg-10023 - aris-nginx-ingress] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10029 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10029 - aris-sealed-secrets] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10025 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_suspended - CronJob pr-customer-env/backup-logs is suspended.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'lo' needs attention and possibly a disk swap.
2,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10050 cluster.
3,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10018 - aris-kube-prometheus-stack] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10036 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,"[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/solutiondata will run full within 24 hours."
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface eth2 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10011 - aris-keda] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container jobs-svc in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10002 - aris-spot-metrics] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container admintools in pod ces-0.
0,"[pr-cp-reg-10025 - pr-customer-env] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10047 - aris-cluster-autoscaler] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controllermanager-svc in pod gateway-cc7bc5b8d-jqbkl.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controllermanager-svc in pod logging-svc-58b457d7d8-pfqmx.
2,[pr-cp-reg-10007 - pr-customer-env] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10007 cluster.
3,[pr-cp-reg-10047 - aris-kube-prometheus-stack] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container adsadmin in pod backup-tenants-28123035-nswx7.
1,[pr-cp-reg-10049 - pr-customer-env-spark] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10039 - aris-nginx-ingress] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10034 - management] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10032 - aris-cluster-autoscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10034 - aris-sealed-secrets] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/cdf"
3,[pr-cp-reg-10014 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (octopus) is restarting  times / 5 minutes.
3,[pr-cp-reg-10012 - pr-customer-env] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10043 - falcon-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10027 - aris-spot-metrics] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10043 - aris-keda] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10017 - aris-kube-downscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10008 - aris-keda] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-keda/exported_job failed to complete.
3,[pr-cp-reg-10021 - pr-customer-env] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10048 - aris-cluster-autoscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod efs-csi-controller.
3,[pr-cp-reg-10014 - pr-customer-env] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana (node-exporter) is restarting  times / 5 minutes.
2,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_k8s_volume_out_of_disk_space - Volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10046 - falcon-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10030 - aris-nginx-ingress] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10004 - falcon-system] - CPUThrottlingHigh -  throttling of CPU in namespace falcon-system for container init-falconstore in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10016 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-prometheus-stack/pr-cp-reg-10016 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10045 - kube-node-lease] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10024 - pr-customer-env] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10029 - aris-kube-prometheus-stack] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10043 - aris-kube-prometheus-stack] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10039 - aris-nginx-ingress] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 (alertmanager) is restarting  times / 5 minutes.
3,[pr-cp-reg-10014 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10011 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10009 - pr-customer-env] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10020 - pr-customer-env] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10025 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_takes_too_long - CronJob pr-customer-env/backup-logs is taking more than 1h to complete.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fznn7 failed to send  of notifications to webhook.
1,[pr-cp-reg-10018 - pr-customer-env-spark] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10002 - kube-node-lease] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
2,[pr-cp-reg-10010 - aris-keda] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10010 cluster.
3,[pr-cp-reg-10023 - aris-kube-prometheus-stack] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env/backup-logs-28124400-ktxxf"
3,[pr-cp-reg-10046 - aris-sealed-secrets] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10006 - kube-system] - KubeNodeNotReady - ip-10-0-138-163.eu-central-1.compute.internal has been unready for more than 15 minutes.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container kube-prometheus-stack in pod aris-kube-prometheus-stack-prometheus-node-exporter-822lp.
1,[pr-cp-reg-10029 - aris-spot] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres-0 (settcpkeepalivetime) is restarting  times / 5 minutes.
1,[pr-cp-reg-10049 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10041 - kasten-io] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod serviceenabling.
1,[pr-cp-reg-10027 - aris-keda] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10042 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10016 - aris-sealed-secrets] - ARIS_sealedsecret_not_unsealed - There are secrets that could not be unsealed in last 30 minutes. Check the sealing key.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container aws-vpc-cni-init in pod kube-proxy-5lqbj.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10004 cluster.
1,[pr-cp-reg-10037 - aris-kube-downscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container cert-manager in pod argocd-repo-server-6676cd4f9c-l95m8.
2,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/data-volume-elasticsearch-default-0 is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10039 - aris-cluster-autoscaler] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10015 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot-metrics/pr-cp-reg-10015 health status has been RED for at least 2 minutes.
3,"[pr-cp-reg-10044 - kube-node-lease] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-node-lease namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10044 - aris-nginx-ingress] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10035 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container crypto-svc in pod aggregatedapis-svc-5c7ccfdd78-slnk8.
1,[pr-cp-reg-10011 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
1,[pr-cp-reg-10025 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kasten-io/aggregatedapis-svc-5c7ccfdd78-slnk8"
1,[pr-cp-reg-10012 - aris-cluster-autoscaler] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10037 - pr-customer-env] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10007 - aris-kube-downscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10033 - aris-cluster-autoscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10027 - falcon-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10033 - pr-customer-env] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - ARIS_host_high_cpu_load - CPU load is > 90%.
0,"[pr-cp-reg-10046 - aris-spot-metrics] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10003 - aris-spot] - ConfigReloaderSidecarErrors - Errors encountered while the spotinst-kubernetes-cluster-controller config-reloader sidecar attempts to sync config in aris-spot namespace. As a result, configuration for service running in spotinst-kubernetes-cluster-controller may be stale and cannot be updated anymore."
3,[pr-cp-reg-10011 - aris-cluster-autoscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
4,[pr-cp-reg-10002 - aris-cluster-autoscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-cluster-autoscaler for container aws-cluster-autoscaler in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
4,[pr-cp-reg-10003 - aris-cluster-autoscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-cluster-autoscaler for container aws-cluster-autoscaler in pod aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf.
1,[pr-cp-reg-10031 - aris-keda] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10021 - aris-kube-downscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10013 - aris-keda] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10034 - aris-cluster-autoscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-cluster-autoscaler has too many active connections. More than 90% of available connections are used.
3,"[pr-cp-reg-10010 - aris-cluster-autoscaler] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
1,[pr-cp-reg-10005 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10040 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot-metrics/pr-cp-reg-10040 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10037 - falcon-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10002 - management] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme6n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controller in pod logging-svc-58b457d7d8-pfqmx.
4,[pr-cp-reg-10023 - aris-spot-metrics] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot-metrics has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine-0 (cdf) is restarting  times / 5 minutes.
3,[pr-cp-reg-10007 - aris-keda] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10044 - aris-kube-downscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10019 - aris-cluster-autoscaler] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10010 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10049 - aris-cluster-autoscaler] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10015 - aris-cluster-autoscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container kube-state-metrics in pod coredns.
1,[pr-cp-reg-10046 - aris-spot] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10016 - aris-sealed-secrets] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10017 - aris-spot] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10044 - pr-customer-env-spark] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,[pr-cp-reg-10006 - management] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme5n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10003 - aris-spot] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-spot/pr-cp-reg-10003 cluster is .
1,[pr-cp-reg-10012 - aris-spot-metrics] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-cluster-autoscaler has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10033 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10016 - aris-spot] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10038 - kube-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container garbagecollector-svc in pod jobs-svc-685557c545-4mnxp.
3,[pr-cp-reg-10029 - aris-cluster-autoscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10015 - aris-keda] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-keda/pr-cp-reg-10015 health status has been YELLOW for at least 20 minutes.
3,"[pr-cp-reg-10003 - aris-spot-metrics] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10011 - kube-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10012 - kube-node-lease] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_argocd_app_progressing - The ArgoCD app aris-secret-elasticsearch is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10006 - pr-customer-env] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_out_of_sync - The ArgoCD app bf4949571e1d348960cfaea8d505c51028858bed802f8e8fe13fc4ea5ab020a1 is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10007 - kube-system] - KubeNodeNotReady - ip-10-0-139-113.eu-central-1.compute.internal has been unready for more than 15 minutes.
3,[pr-cp-reg-10027 - kube-node-lease] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10018 - aris-cluster-autoscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10006 - pr-customer-env-spark] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env-spark for container main in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_argocd_app_out_of_sync - The ArgoCD app 46d6ed101040cf8df2a51291aad961a76c082eba555ed2401548a98d1bb4d54d is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-143-220.eu-central-1.compute.internal in aris-nginx-ingress/pr-cp-reg-10008 cluster is .
3,[pr-cp-reg-10005 - aris-keda] - ARIS_argocd_app_unknown - The ArgoCD app clustertriggerauthentication is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10043 - pr-customer-env-spark] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10009 - aris-spot] - ConfigReloaderSidecarErrors - Errors encountered while the spot-ocean-metric-exporter-56dfbdbbcb-82vqg config-reloader sidecar attempts to sync config in aris-spot namespace. As a result, configuration for service running in spot-ocean-metric-exporter-56dfbdbbcb-82vqg may be stale and cannot be updated anymore."
1,[pr-cp-reg-10006 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,"[pr-cp-reg-10010 - aris-sealed-secrets] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-sealed-secrets/sealed-secrets-controller"
3,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (ces) is restarting  times / 5 minutes.
3,[pr-cp-reg-10032 - pr-customer-env] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container get-zone in pod backup-tenants-28124475-fjqvp.
1,[pr-cp-reg-10006 - aris-spot] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10006 cluster.
3,[pr-cp-reg-10036 - pr-customer-env-spark] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'eni0039e8a4ad3' needs attention and possibly a disk swap.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container jobs-svc in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10019 - aris-keda] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10048 - aris-spot] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10034 - kube-system] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10018 - kube-node-lease] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10030 - aris-nginx-ingress] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10031 - pr-customer-env] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10009 - aris-sealed-secrets] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""lo"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-0"
3,[pr-cp-reg-10019 - pr-customer-env-spark] - ARIS_argocd_app_out_of_sync - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-keda/pr-cp-reg-10050 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10023 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 1h to complete.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/serviceenabling (collaboration) is restarting  times / 5 minutes.
1,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10045 - management] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10014 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10008 - pr-customer-env] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_k8s_pod_crash_looping - Pod aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (aws-cluster-autoscaler) is restarting  times / 5 minutes.
1,[pr-cp-reg-10006 - aris-spot] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,"[pr-cp-reg-10038 - aris-keda] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10001 - basic-application-bootstrapping] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10024 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-downscaler/pr-cp-reg-10024 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10012 - aris-cluster-autoscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10026 - aris-sealed-secrets] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod argocd-application-controller.
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme4n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10042 - aris-keda] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-keda/pr-cp-reg-10042 cluster is .
3,"[pr-cp-reg-10046 - aris-spot] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10002 - kube-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10007 - pr-customer-env] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10008 cluster.
1,[pr-cp-reg-10041 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
1,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10030 - pr-customer-env-spark] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10022 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10002 - aris-keda] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
2,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10007 cluster.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env/backup-logs-28124280 failed to complete.
3,[pr-cp-reg-10031 - management] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10013 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10050 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10043 - aris-nginx-ingress] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-nginx-ingress has too many active connections. More than 90% of available connections are used.
2,[pr-cp-reg-10015 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10015 cluster may be low for segment merges.
1,[pr-cp-reg-10009 - kube-public] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10025 - default] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/simulation-0"
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana (init-config-reloader) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/collaboration"
3,[pr-cp-reg-10049 - kube-node-lease] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10038 - kube-public] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod efs-csi-node-4bvqz.
0,"[pr-cp-reg-10008 - aris-spot-metrics] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container config-init in pod argocd-repo-server-6676cd4f9c-l95m8.
3,[pr-cp-reg-10044 - aris-sealed-secrets] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,"[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,"[pr-cp-reg-10006 - aris-nginx-ingress] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller-cbbc8bd6b-qq22k config-reloader sidecar attempts to sync config in aris-nginx-ingress namespace. As a result, configuration for service running in sealed-secrets-controller-cbbc8bd6b-qq22k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10038 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10004 - pr-customer-env] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10025 - aris-nginx-ingress] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10049 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10018 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10045 - aris-keda] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10007 - aris-keda] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10032 - kube-node-lease] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10016 - aris-kube-downscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_k8s_pod_restarted - The pod aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus-0 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10010 - aris-spot] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container cert-manager in pod aris-management-ingress-delay-ntdn6.
3,[pr-cp-reg-10027 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10014 - kube-node-lease] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10038 - pr-customer-env-spark] - ARIS_k8s_node_disk_pressure - ip-10-0-147-93.eu-central-1.compute.internal has DiskPressure condition.
3,"[pr-cp-reg-10010 - kube-node-lease] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-node-lease namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,"[pr-cp-reg-10008 - aris-cluster-autoscaler] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on eth2 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10007 cluster.
1,[pr-cp-reg-10044 - aris-keda] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10017 - pr-customer-env] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in pr-customer-env/pr-cp-reg-10017 cluster is .
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres-0 (simulation) is restarting  times / 5 minutes.
1,[pr-cp-reg-10004 - pr-customer-env-spark] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env-spark/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (kube-state-metrics) is restarting  times / 5 minutes.
1,[pr-cp-reg-10012 - pr-customer-env] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10019 - aris-kube-prometheus-stack] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10019 - aris-sealed-secrets] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array 'eth0' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10024 - pr-customer-env-spark] - ARIS_argocd_app_unknown - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10042 - aris-kube-prometheus-stack] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10045 - aris-nginx-ingress] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
0,"[pr-cp-reg-10014 - default] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-driver-registrar in pod efs-csi-node-2r2zp.
3,[pr-cp-reg-10024 - aris-nginx-ingress] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10023 - kube-system] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
1,[pr-cp-reg-10037 - management] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_k8s_node_out_of_disk - ip-10-0-143-220.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,"[pr-cp-reg-10012 - aris-sealed-secrets] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-sealed-secrets/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10001 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API cache/kube-system has been only % available over the last 10m.
2,[pr-cp-reg-10022 - aris-keda] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10022 cluster.
3,[pr-cp-reg-10002 - kube-system] - KubeNodeUnreachable - ip-10-0-136-224.eu-central-1.compute.internal is unreachable and some workloads may be rescheduled.
1,[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg (main) is restarting  times / 5 minutes.
3,[pr-cp-reg-10049 - aris-spot] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
2,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10003 cluster may be low for segment merges.
1,[pr-cp-reg-10008 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10011 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processboard-0 (tenant-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124400-ktxxf (log-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10002 - falcon-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10002 - aris-kube-downscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-downscaler has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10028 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-prometheus-stack/pr-cp-reg-10028 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_k8s_node_memory_pressure - ip-10-0-143-220.eu-central-1.compute.internal has MemoryPressure condition.
4,[pr-cp-reg-10045 - aris-spot-metrics] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot-metrics has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10011 - pr-customer-env] - NodeRAIDDegraded - RAID array '/dev/nvme2n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
2,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10013 cluster may be low for segment merges.
3,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-147-93.eu-central-1.compute.internal in pr-customer-env-spark/pr-cp-reg-10040 cluster is .
3,[pr-cp-reg-10025 - management] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10027 - pr-customer-env-spark] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10034 - aris-nginx-ingress] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10027 - aris-keda] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10049 - kube-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10007 - aris-spot-metrics] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot-metrics has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10001 - pr-customer-env] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme3n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10012 - pr-customer-env] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10050 - aris-sealed-secrets] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10025 - pr-customer-env-spark] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10011 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10043 - aris-kube-downscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10013 - aris-spot] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot/pr-cp-reg-10013 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10001 - pr-customer-env-spark] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124160-s7ps9 (abs) is restarting  times / 5 minutes.
3,[pr-cp-reg-10026 - pr-customer-env-spark] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10002 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10032 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-prometheus-stack/pr-cp-reg-10032 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10050 - kube-node-lease] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10033 - aris-cluster-autoscaler] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10025 - pr-customer-env] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10012 - kube-system] - KubeNodeReadinessFlapping - The readiness status of node ip-10-0-139-113.eu-central-1.compute.internal has changed  times in the last 15 minutes.
0,"[pr-cp-reg-10029 - aris-kube-prometheus-stack] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10024 - aris-kube-prometheus-stack] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10050 - pr-customer-env-spark] - ARIS_k8s_node_disk_pressure - ip-10-0-147-93.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10042 - aris-sealed-secrets] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10014 - kube-public] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10050 - aris-nginx-ingress] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10010 - kube-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on eni0039e8a4ad3 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processengine in pod loadbalancer-0.
2,[pr-cp-reg-10005 - pr-customer-env] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10005 cluster.
3,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10017 - kube-node-lease] - CPUThrottlingHigh -  throttling of CPU in namespace kube-node-lease for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10014 - aris-cluster-autoscaler] - ARIS_argocd_app_unknown - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10037 - aris-sealed-secrets] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_argocd_app_progressing - The ArgoCD app e70b0088c1b43b0f14c2263b58659b01c1e590f5abbcacf71ac5eb80a6d11623 is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10014 - kube-system] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10019 - aris-spot] - ARIS_postgresql_down - The postgres pod of namespace aris-spot is down.
4,[pr-cp-reg-10006 - basic-application-bootstrapping] - CPUThrottlingHigh -  throttling of CPU in namespace basic-application-bootstrapping for container basic-application-bootstrapping in pod basic-application-bootstrapping-2wp59.
0,"[pr-cp-reg-10019 - kasten-io] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10002 - pr-customer-env-spark] - ConfigReloaderSidecarErrors - Errors encountered while the pr-customer-env-spark-spark-operat-wh-init-hwzrg config-reloader sidecar attempts to sync config in pr-customer-env-spark namespace. As a result, configuration for service running in pr-customer-env-spark-spark-operat-wh-init-hwzrg may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env/create-es-index-template-61d85 failed to complete.
3,[pr-cp-reg-10027 - pr-customer-env] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10019 - aris-kube-prometheus-stack] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10019 - kube-node-lease] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
1,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10044 - pr-customer-env] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container executor-svc in pod state-svc.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod postgres.
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - ARIS_argocd_app_unknown - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is in unknown state for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container redis in pod cert-manager-webhook.
1,[pr-cp-reg-10028 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
0,"[pr-cp-reg-10017 - kube-public] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10048 - kube-node-lease] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10037 - aris-keda] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,"[pr-cp-reg-10005 - aris-sealed-secrets] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10013 - aris-keda] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10018 - pr-customer-env-spark] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter management/ae-aw-euc1-15995-aws-load-balancer-controller"
3,[pr-cp-reg-10001 - management] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10024 - aris-nginx-ingress] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10006 - management] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme1n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-sealed-secrets/exported_job failed to complete.
1,[pr-cp-reg-10023 - kube-public] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10031 - aris-sealed-secrets] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - kube-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kube-system/coredns-cbbbbb9cb-q2xzz"
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_critical - Latency to the upstream service aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus has been critically high (more than 30 seconds) for the last 3 minutes. Service is unresponsive and should be restarted.
3,[pr-cp-reg-10011 - aris-kube-downscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app prometheusWideSeries is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10026 - aris-spot-metrics] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - management] - NodeRAIDDegraded - RAID array '/dev/nvme4n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10008 - aris-cluster-autoscaler] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-cluster-autoscaler/sealed-secrets-controller-cbbc8bd6b-qq22k"
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container kube-state-metrics in pod ae-aw-euc1-15995-external-dns-56db7877c9-9mtn7.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the aris-kube-prometheus-stack-grafana/aris-kube-prometheus-stack-alertmanager targets in aris-kube-prometheus-stack namespace are down.
3,"[pr-cp-reg-10006 - aris-spot] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-spot/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10009 - pr-customer-env] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme4n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10043 - aris-sealed-secrets] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - AlertmanagerClusterDown -  of Alertmanager instances within the aris-kube-prometheus-stack-alertmanager cluster have been up for less than half of the last 5m.
3,[pr-cp-reg-10021 - aris-keda] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod kanister-job-6gvp5.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/elasticsearch-default-0 (tm) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10001 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the kanister-svc-9559f74-79zgn config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in kanister-svc-9559f74-79zgn may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces-0 (processboard) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10039 - default] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in default namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container ambassador in pod auth-svc.
3,"[pr-cp-reg-10003 - kube-system] - ConfigReloaderSidecarErrors - Errors encountered while the aws-node config-reloader sidecar attempts to sync config in kube-system namespace. As a result, configuration for service running in aws-node may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod cloudsearch-0.
1,[pr-cp-reg-10046 - aris-kube-prometheus-stack] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-prometheus-stack is down.
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container get-zone in pod elasticsearch-default-0.
2,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10002 cluster.
1,[pr-cp-reg-10031 - pr-customer-env-spark] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10037 - aris-nginx-ingress] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,"[pr-cp-reg-10007 - aris-keda] - ConfigReloaderSidecarErrors - Errors encountered while the keda-operator config-reloader sidecar attempts to sync config in aris-keda namespace. As a result, configuration for service running in keda-operator may be stale and cannot be updated anymore."
1,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10012 - pr-customer-env] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme4n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10015 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10046 - basic-application-bootstrapping] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10017 - kube-system] - KubeletPlegDurationHigh - The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of  seconds on node ip-10-0-139-113.eu-central-1.compute.internal.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eni0039e8a4ad3"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-1"
3,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_argocd_app_progressing - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10025 - aris-keda] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - aris-spot] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot for container spotinst-kubernetes-cluster-controller in pod spotinst-kubernetes-cluster-controller.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (zookeeper) is restarting  times / 5 minutes.
3,[pr-cp-reg-10022 - falcon-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10040 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
0,"[pr-cp-reg-10020 - aris-keda] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10022 - pr-customer-env-spark] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10027 - aris-cluster-autoscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10038 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10006 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10020 - pr-customer-env-spark] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10017 - aris-kube-downscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10014 - basic-application-bootstrapping] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container executor-svc in pod controllermanager-svc.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container efs-plugin in pod argocd-redis-ha-haproxy-84b857bc4b-qd5km.
3,[pr-cp-reg-10012 - aris-spot] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot/pr-cp-reg-10012 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10020 - aris-keda] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10033 - aris-spot] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10014 - aris-sealed-secrets] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-sealed-secrets has too many active connections. More than 90% of available connections are used.
4,[pr-cp-reg-10028 - aris-kube-prometheus-stack] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-prometheus-stack/exported_job failed to complete.
1,[pr-cp-reg-10044 - kube-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
0,"[pr-cp-reg-10028 - aris-kube-prometheus-stack] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - ARIS_k8s_node_pid_pressure - ip-10-0-138-163.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10019 - aris-nginx-ingress] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5 (node-exporter) is restarting  times / 5 minutes.
3,[pr-cp-reg-10040 - kube-node-lease] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10016 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10008 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10034 - aris-sealed-secrets] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10042 - kube-public] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10013 - pr-customer-env] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10023 - aris-nginx-ingress] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
0,"[pr-cp-reg-10009 - basic-application-bootstrapping] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10050 - default] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
2,[pr-cp-reg-10005 - pr-customer-env] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10005 cluster may be low for segment merges.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod backup-logs-28124280-qwm2q.
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
0,"[pr-cp-reg-10037 - kube-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10029 - aris-kube-downscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
2,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10004 cluster.
1,[pr-cp-reg-10016 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-operator-78b758c475-qs69v failed to send  of notifications to victorops.
1,[pr-cp-reg-10012 - aris-kube-downscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10023 - aris-sealed-secrets] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-sealed-secrets has too many active connections. More than 90% of available connections are used.
2,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10001 cluster.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env/create-es-index-template-61d85 failed to complete.
4,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-downscaler/exported_job failed to complete.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on tmpfs at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10001 - kube-node-lease] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10003 - default] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10012 - aris-kube-downscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10044 - aris-kube-downscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
2,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10027 cluster.
1,[pr-cp-reg-10050 - falcon-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod abs.
3,[pr-cp-reg-10007 - aris-spot-metrics] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10041 - kube-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_argocd_app_out_of_sync - The ArgoCD app tls-secret is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10004 - pr-customer-env] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme4n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10005 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API prometheus/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124400-ktxxf (create-es-index-template) is restarting  times / 5 minutes.
3,[pr-cp-reg-10004 - pr-customer-env] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10018 - kasten-io] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env-spark is down.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod efs-csi-node-4cgpg.
1,[pr-cp-reg-10043 - aris-sealed-secrets] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
4,[pr-cp-reg-10006 - aris-sealed-secrets] - CPUThrottlingHigh -  throttling of CPU in namespace aris-sealed-secrets for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10005 - pr-customer-env-spark] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10033 - aris-cluster-autoscaler] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
4,[pr-cp-reg-10004 - falcon-system] - CPUThrottlingHigh -  throttling of CPU in namespace falcon-system for container kube-state-metrics in pod falcon-falcon-sensor-2bjwf.
3,[pr-cp-reg-10044 - kube-node-lease] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10017 - aris-spot] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10021 - kube-public] - CPUThrottlingHigh -  throttling of CPU in namespace kube-public for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container schema-upgrade-check in pod jobs-svc.
3,[pr-cp-reg-10026 - aris-keda] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana-sc-dashboard in pod aris-kube-prometheus-stack-operator-78b758c475-qs69v.
3,[pr-cp-reg-10038 - kasten-io] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container garbagecollector-svc in pod dashboardbff-svc.
1,[pr-cp-reg-10001 - pr-customer-env] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10022 - pr-customer-env] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10018 - aris-nginx-ingress] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10007 - aris-cluster-autoscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10012 - kube-node-lease] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10032 - aris-cluster-autoscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10021 - aris-nginx-ingress] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10040 - aris-keda] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-keda/pr-cp-reg-10040 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 failed to send  of notifications to webhook.
3,[pr-cp-reg-10023 - aris-nginx-ingress] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28124475-fjqvp (admintools) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod backup-logs-28124160-s7ps9.
1,[pr-cp-reg-10045 - basic-application-bootstrapping] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10007 - aris-keda] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10044 - aris-keda] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10001 - pr-customer-env-spark] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env-spark/pr-customer-env-spark-spark-operator-74d54645cb-5gpbl (main) is restarting  times / 5 minutes.
3,[pr-cp-reg-10012 - pr-customer-env] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10004 - aris-keda] - ARIS_argocd_app_progressing - The ArgoCD app 79cd6a440adbed21dbd5f26526d771b9c008e326b136813e4a930cd85d28d9bf is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10024 - kube-system] - KubeQuotaFullyUsed - Namespace kube-system is using  of its cpu quota.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the node-exporter/prometheus-operated targets in aris-kube-prometheus-stack namespace are down.
3,[pr-cp-reg-10015 - kube-system] - KubeletServerCertificateRenewalErrors - Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal has failed to renew its server certificate ( errors in the last 5 minutes).
0,"[pr-cp-reg-10036 - kube-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_argocd_app_unknown - The ArgoCD app a6d67856bcc285f0f6ca4349c619dff9fba796cd94aeb13735a2f19b88abfe1e is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10021 - basic-application-bootstrapping] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10028 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
2,[pr-cp-reg-10015 - aris-spot] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10015 cluster may be low for segment merges.
3,[pr-cp-reg-10037 - kube-public] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10003 - management] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_k8s_pod_restarted - The pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env/simulation"
4,[pr-cp-reg-10014 - aris-keda] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-keda has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10021 - aris-nginx-ingress] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_k8s_pod_restarted - The pod pr-customer-env-spark/pr-customer-env-spark-spark-operator-74d54645cb-5gpbl has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
4,[pr-cp-reg-10002 - aris-spot-metrics] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot-metrics for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-resizer in pod ebs-csi-controller-86cb997fdc-bs5bs.
1,[pr-cp-reg-10002 - aris-kube-downscaler] - ARIS_k8s_pod_restarted - The pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10036 - aris-keda] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container admintools in pod cdf-0.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_k8s_pod_restarted - The pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10022 - aris-sealed-secrets] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10014 - aris-sealed-secrets] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10014 cluster.
3,[pr-cp-reg-10021 - kube-system] - KubeQuotaExceeded - Namespace kube-system is using  of its cpu quota.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container ambassador in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10021 - aris-nginx-ingress] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-143-220.eu-central-1.compute.internal in aris-nginx-ingress/pr-cp-reg-10021 cluster is .
3,[pr-cp-reg-10050 - aris-keda] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10046 - aris-sealed-secrets] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10026 - aris-kube-downscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10036 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10038 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10038 cluster may be low for segment merges.
1,[pr-cp-reg-10033 - kube-system] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
1,[pr-cp-reg-10036 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-nginx-ingress/pr-cp-reg-10036 health status has been RED for at least 2 minutes.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme7n1"" changing its up status often on node-exporter management/argocd-redis-ha-server"
3,[pr-cp-reg-10004 - aris-spot] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-spot/pr-cp-reg-10004 cluster is .
3,[pr-cp-reg-10032 - aris-keda] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has only found  members of the sealed-secrets-controller cluster.
3,[pr-cp-reg-10029 - aris-sealed-secrets] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer-0 (abs) is restarting  times / 5 minutes.
1,[pr-cp-reg-10039 - aris-nginx-ingress] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10031 - aris-spot] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container controller in pod alertmanager-aris-kube-prometheus-stack-alertmanager.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processengine in pod simulation-0.
3,[pr-cp-reg-10044 - aris-nginx-ingress] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod argocd-dex-server-6b74fb9695-kbk62.
3,[pr-cp-reg-10010 - pr-customer-env-spark] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10026 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10033 - default] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10042 - aris-sealed-secrets] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10038 - pr-customer-env-spark] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10034 - aris-sealed-secrets] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10039 - kube-system] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/register-smtp-server-6d78c-stqpc (dashboarding) is restarting  times / 5 minutes.
1,[pr-cp-reg-10038 - pr-customer-env] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env/pr-cp-reg-10038 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10012 - aris-spot-metrics] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme1n1"" changing its up status often on node-exporter management/ebs-csi-node-7gdr8"
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10039 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10023 - management] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10049 - aris-cluster-autoscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10002 - falcon-system] - CPUThrottlingHigh -  throttling of CPU in namespace falcon-system for container kube-state-metrics in pod falcon-falcon-sensor-2bjwf.
3,[pr-cp-reg-10040 - aris-sealed-secrets] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - pr-customer-env] - ConfigReloaderSidecarErrors - Errors encountered while the zookeeper-0 config-reloader sidecar attempts to sync config in pr-customer-env namespace. As a result, configuration for service running in zookeeper-0 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10045 - aris-cluster-autoscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10022 - aris-spot] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot/pr-cp-reg-10022 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10029 - aris-keda] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10030 - kube-system] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10013 - aris-keda] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
2,[pr-cp-reg-10017 - pr-customer-env] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10017 cluster may be low for segment merges.
3,[pr-cp-reg-10047 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10042 - kasten-io] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container umcadmin in pod serviceenabling-0.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_out_of_sync - The ArgoCD app clustertriggerauthentication is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10012 - pr-customer-env] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme4n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10018 - kube-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod kanister-job-6gvp5.
1,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10049 - kube-system] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container logging-svc in pod gateway-cc7bc5b8d-jqbkl.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container garbagecollector-svc in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10025 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana-sc-datasources in pod alertmanager-aris-kube-prometheus-stack-alertmanager.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/elasticsearch-default-0 (settcpkeepalivetime) is restarting  times / 5 minutes.
0,"[pr-cp-reg-10035 - aris-cluster-autoscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
0,"[pr-cp-reg-10044 - kasten-io] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10033 - aris-keda] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10015 - aris-kube-downscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod cert-manager-webhook-58765b986c-wxht4.
3,"[pr-cp-reg-10001 - aris-spot] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10012 - pr-customer-env] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10038 - aris-kube-downscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10007 - aris-cluster-autoscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10007 - aris-keda] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10004 - pr-customer-env-spark] - ARIS_k8s_node_out_of_disk - ip-10-0-147-93.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10009 - pr-customer-env] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10003 - aris-nginx-ingress] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4"
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf (loadbalancer) is restarting  times / 5 minutes.
3,[pr-cp-reg-10012 - pr-customer-env] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme3n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10020 - falcon-system] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in falcon-system namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10036 - aris-nginx-ingress] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10049 - pr-customer-env] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10015 - aris-sealed-secrets] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10014 - aris-nginx-ingress] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_k8s_pod_restarted - The pod aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10011 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10042 - aris-sealed-secrets] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10015 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10014 - aris-cluster-autoscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter management/management-external-dns-65bfcbd6d7-zlfjj"
3,[pr-cp-reg-10044 - aris-sealed-secrets] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10032 - aris-sealed-secrets] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10037 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container portalserver in pod portalserver.
3,[pr-cp-reg-10022 - aris-keda] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,"[pr-cp-reg-10013 - falcon-system] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in falcon-system namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10046 - aris-nginx-ingress] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_argocd_app_unknown - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10021 - falcon-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10030 - kasten-io] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/ebs-csi-node"
1,[pr-cp-reg-10047 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
0,"[pr-cp-reg-10036 - kube-node-lease] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10013 - aris-spot] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-143-220.eu-central-1.compute.internal in aris-spot/pr-cp-reg-10013 cluster is .
3,[pr-cp-reg-10014 - pr-customer-env] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on eni0039e8a4ad3 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container umcadmin in pod processboard.
3,[pr-cp-reg-10027 - aris-sealed-secrets] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/octopus-0 (setmaxmapcount) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10001 - pr-customer-env] - ConfigReloaderSidecarErrors - Errors encountered while the octopus-0 config-reloader sidecar attempts to sync config in pr-customer-env namespace. As a result, configuration for service running in octopus-0 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10009 - aris-nginx-ingress] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
2,[pr-cp-reg-10001 - aris-spot] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10001 cluster.
3,[pr-cp-reg-10039 - aris-keda] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_node_disk_pressure - ip-10-0-143-220.eu-central-1.compute.internal has DiskPressure condition.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cdf in pod dashboarding-0.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env/abs-0"
4,[pr-cp-reg-10009 - aris-kube-downscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-downscaler/exported_job failed to complete.
2,[pr-cp-reg-10026 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10026 cluster may be low for segment merges.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10016 - aris-nginx-ingress] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10016 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/tm-0 (umcadmin) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container redis in pod argocd-redis-ha-haproxy-84b857bc4b-gksr6.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10008 - pr-customer-env] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10008 cluster.
3,[pr-cp-reg-10012 - aris-sealed-secrets] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10017 - pr-customer-env-spark] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10002 - kube-system] - KubeletTooManyPods - Kubelet 'ip-10-0-138-163.eu-central-1.compute.internal' is running at  of its Pod capacity.
3,[pr-cp-reg-10040 - aris-sealed-secrets] - ARIS_sealedsecret_not_unsealed - There are secrets that could not be unsealed in last 30 minutes. Check the sealing key.
3,[pr-cp-reg-10044 - pr-customer-env-spark] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10021 - kasten-io] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124400-ktxxf (cdf) is restarting  times / 5 minutes.
2,[pr-cp-reg-10001 - pr-customer-env-spark] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10001 cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kanister-sidecar in pod abs-0.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on eni006a31b57f1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10017 - kube-system] - KubeletTooManyPods - Kubelet 'ip-10-0-139-113.eu-central-1.compute.internal' is running at  of its Pod capacity.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kasten-io/jobs-svc-685557c545-4mnxp"
1,[pr-cp-reg-10032 - pr-customer-env-spark] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10029 - kube-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10021 - aris-keda] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-nginx-ingress/pr-cp-reg-10014 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10016 - aris-cluster-autoscaler] - ARIS_argocd_app_unknown - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is in unknown state for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10030 - aris-spot] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container executor-svc in pod frontend-svc-7f5bd48b8-99m2g.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container umcadmin in pod register-smtp-server-6d78c-stqpc.
3,[pr-cp-reg-10010 - falcon-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container catalog-svc in pod metering-svc-685b59dfb-pcknm.
3,[pr-cp-reg-10015 - aris-nginx-ingress] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10039 - aris-sealed-secrets] - ARIS_postgresql_down - The postgres pod of namespace aris-sealed-secrets is down.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf-0 (tm) is restarting  times / 5 minutes.
4,[pr-cp-reg-10020 - aris-kube-prometheus-stack] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-prometheus-stack has faced > 5 deadlocks in last minute.
3,"[pr-cp-reg-10002 - pr-customer-env-spark] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env-spark/pr-customer-env-spark-spark-operator-74d54645cb-5gpbl"
3,[pr-cp-reg-10028 - aris-kube-downscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_restarted - The pod pr-customer-env/simulation-0 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10012 - kasten-io] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10028 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10033 - kube-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,"[pr-cp-reg-10001 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the logging-svc-58b457d7d8-pfqmx config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in logging-svc-58b457d7d8-pfqmx may be stale and cannot be updated anymore."
1,[pr-cp-reg-10025 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10004 - management] - NodeRAIDDegraded - RAID array '/dev/nvme7n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod postgres.
3,[pr-cp-reg-10012 - aris-sealed-secrets] - ARIS_argocd_app_out_of_sync - The ArgoCD app 46d6ed101040cf8df2a51291aad961a76c082eba555ed2401548a98d1bb4d54d is out of sync for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container frontend-svc in pod logging-svc.
3,[pr-cp-reg-10001 - kube-node-lease] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10015 - kube-node-lease] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (zookeeper) is restarting  times / 5 minutes.
3,[pr-cp-reg-10023 - aris-kube-prometheus-stack] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - aris-kube-downscaler] - ARIS_k8s_pod_crash_looping - Pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10002 - management] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme1n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10041 - pr-customer-env] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env-spark/pr-cp-reg-10046 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-repo-server in pod efs-csi-node.
3,[pr-cp-reg-10029 - aris-keda] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10010 - pr-customer-env-spark] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10010 cluster.
1,[pr-cp-reg-10014 - kube-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
0,"[pr-cp-reg-10020 - aris-nginx-ingress] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10024 - kube-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10017 - kube-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on eni0039e8a4ad3 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces (portalserver) is restarting  times / 5 minutes.
3,[pr-cp-reg-10030 - management] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10014 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-sealed-secrets/pr-cp-reg-10014 health status has been YELLOW for at least 20 minutes.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/argocd-server-74fbd754cb-cjfrb"
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on nvme2 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10045 - pr-customer-env] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/backup-tenants-28123035-nswx7"
3,[pr-cp-reg-10037 - aris-keda] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10005 - management] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme6n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10021 - kube-node-lease] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10038 - aris-sealed-secrets] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10008 - aris-keda] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10038 - aris-keda] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10013 - aris-sealed-secrets] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10037 - aris-keda] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_argocd_app_unknown - The ArgoCD app 46d6ed101040cf8df2a51291aad961a76c082eba555ed2401548a98d1bb4d54d is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10031 - pr-customer-env-spark] - ARIS_k8s_node_out_of_disk - ip-10-0-147-93.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10039 - aris-cluster-autoscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-cluster-autoscaler has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/create-es-index-template-61d85-nz2dz (dashboarding) is restarting  times / 5 minutes.
3,[pr-cp-reg-10043 - kube-system] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-6gvp5 (simulation) is restarting  times / 5 minutes.
1,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10049 - falcon-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10032 - pr-customer-env-spark] - ARIS_k8s_node_memory_pressure - ip-10-0-147-93.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_argocd_app_unknown - The ArgoCD app 0137b8d8487077523c9bc7efe6b90af6636a5722148492370b387532bae836dd is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10035 - kube-system] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10021 - basic-application-bootstrapping] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10019 - pr-customer-env] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env/pr-cp-reg-10019 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10014 - pr-customer-env-spark] - ARIS_argocd_app_unknown - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is in unknown state for longer than 10 minutes and requires manual intervention.
2,[pr-cp-reg-10018 - aris-spot] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10018 cluster.
1,[pr-cp-reg-10048 - aris-keda] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10028 - aris-nginx-ingress] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10016 - kube-system] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
1,[pr-cp-reg-10019 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10027 - aris-kube-prometheus-stack] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10002 cluster.
3,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-nginx-ingress has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10020 - pr-customer-env] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_k8s_node_out_of_disk - ip-10-0-147-93.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10012 - basic-application-bootstrapping] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10036 - aris-sealed-secrets] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10035 - kube-public] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
1,[pr-cp-reg-10049 - aris-keda] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10036 - aris-kube-prometheus-stack] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter management/argocd-redis-ha-haproxy-84b857bc4b-qd5km"
3,[pr-cp-reg-10017 - aris-cluster-autoscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10024 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-prometheus-stack/pr-cp-reg-10024 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env/collaboration-0"
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration-0 (tenant-backup) is restarting  times / 5 minutes.
2,[pr-cp-reg-10024 - aris-kube-prometheus-stack] - ARIS_k8s_volume_out_of_disk_space - Volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10026 - aris-nginx-ingress] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver (cloudsearch) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod umcadmin-0.
3,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10040 - pr-customer-env] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app mining-storage is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10027 - aris-cluster-autoscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10012 - kube-node-lease] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10028 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10002 - aris-keda] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container admin-svc in pod logging-svc-58b457d7d8-pfqmx.
3,[pr-cp-reg-10049 - aris-nginx-ingress] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10037 - aris-sealed-secrets] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on eth2 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding-0 (collaboration) is restarting  times / 5 minutes.
3,[pr-cp-reg-10032 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-nginx-ingress/pr-cp-reg-10032 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer-0 (cloudsearch) is restarting  times / 5 minutes.
1,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/adsadmin-0 (tm) is restarting  times / 5 minutes.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 8h to complete.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kanister-sidecar in pod processengine.
3,[pr-cp-reg-10001 - kube-system] - KubeletPlegDurationHigh - The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of  seconds on node ip-10-0-138-163.eu-central-1.compute.internal.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k has only found  members of the sealed-secrets-controller cluster.
3,[pr-cp-reg-10018 - aris-sealed-secrets] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-sealed-secrets/pr-cp-reg-10018 cluster is .
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_restarted - The pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10035 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-downscaler/pr-cp-reg-10035 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod argocd-repo-server.
1,[pr-cp-reg-10041 - kube-system] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10017 - pr-customer-env] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10011 - aris-spot] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-spot/pr-cp-reg-10011 cluster is .
3,[pr-cp-reg-10036 - aris-nginx-ingress] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10034 - pr-customer-env] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
0,"[pr-cp-reg-10042 - aris-kube-prometheus-stack] - Watchdog - This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the ""DeadMansSnitch"" integration in PagerDuty."
1,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-nginx-ingress/pr-cp-reg-10003 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10004 - aris-cluster-autoscaler] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10049 - aris-kube-downscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10039 - aris-kube-prometheus-stack] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10025 - kasten-io] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,"[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10031 - aris-keda] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-keda/pr-cp-reg-10031 cluster is .
1,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-downscaler is down.
4,[pr-cp-reg-10004 - aris-sealed-secrets] - CPUThrottlingHigh -  throttling of CPU in namespace aris-sealed-secrets for container controller in pod sealed-secrets-controller.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod kanister-job-6gvp5.
3,[pr-cp-reg-10035 - default] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10050 - kube-public] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer (controller) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - aris-cluster-autoscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-cluster-autoscaler for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10046 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app metering-pv-claim is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container kube-state-metrics in pod prometheus-aris-kube-prometheus-stack-prometheus-0.
1,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10032 - aris-keda] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10043 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10042 - aris-cluster-autoscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
0,"[pr-cp-reg-10046 - kube-public] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10006 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod octopus-0.
4,[pr-cp-reg-10004 - aris-nginx-ingress] - CPUThrottlingHigh -  throttling of CPU in namespace aris-nginx-ingress for container kube-state-metrics in pod aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4.
3,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array '0' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,"[pr-cp-reg-10003 - aris-spot] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container log-backup in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container sentinel in pod efs-csi-node-4cgpg.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces-0 (ces) is restarting  times / 5 minutes.
1,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10001 - kube-public] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-public namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10026 - kube-public] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10016 - aris-kube-prometheus-stack] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10029 - aris-nginx-ingress] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on tmpfs at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/elasticsearch-default (umcadmin) is restarting  times / 5 minutes.
3,[pr-cp-reg-10020 - kube-node-lease] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
2,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10013 cluster may be low for segment merges.
3,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10050 - pr-customer-env] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10043 - kube-public] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10045 - aris-kube-prometheus-stack] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10031 - aris-kube-downscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container coredns in pod coredns.
3,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
4,[pr-cp-reg-10032 - aris-kube-downscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-downscaler has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-downscaler is down.
3,[pr-cp-reg-10047 - aris-sealed-secrets] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10008 - aris-spot] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10010 - kube-system] - KubeletClientCertificateRenewalErrors - Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal has failed to renew its client certificate ( errors in the last 5 minutes).
3,[pr-cp-reg-10042 - aris-cluster-autoscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kube-state-metrics in pod postgres.
3,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-138-163.eu-central-1.compute.internal in aris-kube-prometheus-stack/pr-cp-reg-10007 cluster is .
1,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-cluster-autoscaler/pr-cp-reg-10040 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28123035-nswx7 (create-es-index-template) is restarting  times / 5 minutes.
3,[pr-cp-reg-10008 - pr-customer-env] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10023 - pr-customer-env] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10029 - aris-kube-downscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processengine in pod cdf.
3,[pr-cp-reg-10043 - aris-nginx-ingress] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10038 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/adsadmin-0 (log-backup) is restarting  times / 5 minutes.
3,[pr-cp-reg-10042 - pr-customer-env] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10048 - aris-keda] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10030 - kube-public] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/serviceenabling (container) is restarting  times / 5 minutes.
3,[pr-cp-reg-10047 - default] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10038 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10023 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env-spark/pr-cp-reg-10023 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10027 - aris-keda] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5 has only found  members of the aris-kube-prometheus-stack-alertmanager cluster.
3,[pr-cp-reg-10002 - aris-kube-downscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app data-volume-elasticsearch-default-0 is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_k8s_node_pid_pressure - ip-10-0-138-163.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10027 - kube-node-lease] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10046 - pr-customer-env] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10033 - aris-keda] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10002 - kube-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10027 - aris-cluster-autoscaler] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10032 - aris-nginx-ingress] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10006 - falcon-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10003 - kasten-io] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - management] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme6n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10016 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 1h to complete.
3,[pr-cp-reg-10032 - default] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10038 - kasten-io] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-prometheus-stack has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10047 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10033 - kasten-io] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod register-smtp-server-6d78c-stqpc.
1,[pr-cp-reg-10008 - pr-customer-env] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10008 cluster.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager has only found  members of the node-exporter cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod kanister-job-6kclt.
3,[pr-cp-reg-10049 - kasten-io] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10035 - aris-nginx-ingress] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10026 - pr-customer-env-spark] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""0"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fznn7"
2,[pr-cp-reg-10016 - pr-customer-env] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10016 cluster.
1,[pr-cp-reg-10041 - aris-kube-prometheus-stack] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10026 - aris-keda] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10044 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter management/ebs-csi-controller"
3,"[pr-cp-reg-10013 - default] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in default namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-6kclt (cloudsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10032 - pr-customer-env-spark] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container delay in pod ae-aw-euc1-15995-aws-load-balancer-controller-7895559794-c8nd5.
1,[pr-cp-reg-10033 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10001 - aris-spot] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10034 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_k8s_node_disk_pressure - ip-10-0-143-220.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10043 - aris-sealed-secrets] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10027 - aris-nginx-ingress] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10025 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env-spark/pr-cp-reg-10025 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10039 - kube-system] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10019 - aris-sealed-secrets] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10016 - aris-sealed-secrets] - ARIS_postgresql_down - The postgres pod of namespace aris-sealed-secrets is down.
3,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10040 - kube-public] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter management/ebs-csi-node-bspjp"
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10039 - falcon-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration-0 (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10003 - kube-node-lease] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_postgresql_down - The postgres pod of namespace aris-sealed-secrets is down.
3,[pr-cp-reg-10029 - kube-node-lease] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
4,[pr-cp-reg-10001 - pr-customer-env-spark] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env-spark for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,"[pr-cp-reg-10003 - aris-nginx-ingress] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-nginx-ingress namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme1n1"" changing its up status often on node-exporter management/ae-aw-euc1-15995-aws-load-balancer-controller-7895559794-c8nd5"
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme1n1"" changing its up status often on node-exporter management/ae-aw-euc1-15995-external-dns-56db7877c9-9mtn7"
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme7n1"" changing its up status often on node-exporter management/ae-aw-euc1-15995-external-dns-56db7877c9-9mtn7"
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eth0"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-1"
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_argocd_app_progressing - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10018 - aris-sealed-secrets] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10005 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10003 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API 0592d0e00c13ca08599ec30fab4da59b0de6c8ceeeb333d975a04fb1b65e3da3/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod umcadmin-0.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container config-init in pod management-external-dns.
1,[pr-cp-reg-10036 - aris-spot] - ARIS_postgresql_down - The postgres pod of namespace aris-spot is down.
1,[pr-cp-reg-10043 - aris-keda] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10039 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10025 - pr-customer-env-spark] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-147-93.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/simulation-0 (container) is restarting  times / 5 minutes.
0,"[pr-cp-reg-10002 - kube-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10017 - kube-public] - CPUThrottlingHigh -  throttling of CPU in namespace kube-public for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10035 - aris-kube-downscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10024 - pr-customer-env-spark] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10016 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 8h to complete.
3,[pr-cp-reg-10032 - aris-cluster-autoscaler] - ARIS_cluster_autoscaler_unschedulable_pods - The cluster autoscaler is unable to scale up and there are unschedulable pods because of this condition.
1,[pr-cp-reg-10043 - kube-node-lease] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10028 - aris-kube-downscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-provisioner in pod cert-manager-webhook-58765b986c-wxht4.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface 0 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10022 - aris-sealed-secrets] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10030 - pr-customer-env] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10003 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API 0967bcf9cd94edf7fcfc0db86322d8f7af0a81626feafeaae506bfb81271ae15/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod serviceenabling-0.
3,[pr-cp-reg-10026 - pr-customer-env] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (admintools) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processengine in pod octopus.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-822lp has only found  members of the sealed-secrets-controller cluster.
3,[pr-cp-reg-10018 - aris-keda] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10041 - aris-cluster-autoscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-cluster-autoscaler is down.
3,[pr-cp-reg-10026 - aris-nginx-ingress] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,"[pr-cp-reg-10013 - aris-kube-downscaler] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller-cbbc8bd6b-qq22k config-reloader sidecar attempts to sync config in aris-kube-downscaler namespace. As a result, configuration for service running in sealed-secrets-controller-cbbc8bd6b-qq22k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10043 - aris-sealed-secrets] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
0,"[pr-cp-reg-10045 - aris-kube-prometheus-stack] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod kanister-job-6gvp5.
3,"[pr-cp-reg-10004 - aris-sealed-secrets] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller config-reloader sidecar attempts to sync config in aris-sealed-secrets namespace. As a result, configuration for service running in sealed-secrets-controller may be stale and cannot be updated anymore."
1,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_k8s_node_memory_pressure - ip-10-0-147-93.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10022 - kube-system] - KubeQuotaExceeded - Namespace kube-system is using  of its cpu quota.
3,[pr-cp-reg-10015 - aris-kube-downscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env/backup-logs-28124160 failed to complete.
3,[pr-cp-reg-10037 - aris-keda] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10035 - falcon-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
2,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10010 cluster may be low for segment merges.
3,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10050 - aris-keda] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10042 - pr-customer-env] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10010 - aris-spot] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10048 - aris-kube-downscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10040 - pr-customer-env] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_k8s_volume_out_of_disk_space - Volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10037 - kube-public] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
2,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10015 cluster.
1,[pr-cp-reg-10038 - pr-customer-env-spark] - ARIS_k8s_node_pid_pressure - ip-10-0-147-93.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10048 - pr-customer-env-spark] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app data-volume-postgres-0 is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10013 - kube-node-lease] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10033 - aris-keda] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10017 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod simulation-0.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf-0 (elastic-metrics-sidecar) is restarting  times / 5 minutes.
3,[pr-cp-reg-10025 - aris-keda] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10009 - default] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10017 - aris-kube-downscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10027 - pr-customer-env] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter kasten-io/controllermanager-svc"
1,[pr-cp-reg-10005 - management] - NodeRAIDDegraded - RAID array '/dev/nvme6n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10031 - kube-system] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/elasticsearch-default-0 (prometheus-exporter) is restarting  times / 5 minutes.
1,[pr-cp-reg-10017 - aris-sealed-secrets] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10014 - pr-customer-env-spark] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10011 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10003 - kasten-io] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10022 - aris-kube-prometheus-stack] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-138-163.eu-central-1.compute.internal in aris-kube-prometheus-stack/pr-cp-reg-10004 cluster is .
3,[pr-cp-reg-10001 - kube-public] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on lo at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10014 - default] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10047 - pr-customer-env] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
2,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10033 cluster.
1,[pr-cp-reg-10027 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10042 - aris-nginx-ingress] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10004 - aris-keda] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10013 - aris-nginx-ingress] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on tmpfs at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10009 - aris-nginx-ingress] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10017 - aris-keda] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper-0 (kanister-sidecar) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container sentinel in pod cert-manager-webhook-58765b986c-wxht4.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod aris-management-ingress-delay-ntdn6.
2,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10015 cluster.
3,[pr-cp-reg-10022 - aris-kube-prometheus-stack] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10042 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10019 - pr-customer-env-spark] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10030 - falcon-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - kasten-io] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kasten-io/frontend-svc-7f5bd48b8-99m2g"
1,[pr-cp-reg-10014 - aris-keda] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container metering-svc in pod jobs-svc-685557c545-4mnxp.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""shm"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5"
1,[pr-cp-reg-10005 - aris-cluster-autoscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10007 - pr-customer-env] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10041 - aris-cluster-autoscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10039 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10049 - aris-keda] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,"[pr-cp-reg-10024 - basic-application-bootstrapping] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in basic-application-bootstrapping namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres-0 (kanister-sidecar) is restarting  times / 5 minutes.
3,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/tm (adsadmin) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana-sc-dashboard in pod aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5.
3,[pr-cp-reg-10023 - aris-spot] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10016 - pr-customer-env] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env/pr-cp-reg-10016 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10033 - aris-spot] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10024 - kube-node-lease] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10040 - falcon-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10022 - aris-kube-prometheus-stack] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cdf in pod adsadmin.
3,[pr-cp-reg-10033 - kasten-io] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10042 - basic-application-bootstrapping] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs (setmaxmapcount) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10006 - falcon-system] - ConfigReloaderSidecarErrors - Errors encountered while the falcon-falcon-sensor-2bjwf config-reloader sidecar attempts to sync config in falcon-system namespace. As a result, configuration for service running in falcon-falcon-sensor-2bjwf may be stale and cannot be updated anymore."
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10012 - aris-sealed-secrets] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10027 - basic-application-bootstrapping] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10017 - aris-keda] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10033 - falcon-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10018 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 8h to complete.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on nvme0 at <IP_ADDRESS>:<PORT> has only % available space left.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""nvme0"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager"
3,[pr-cp-reg-10027 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-cluster-autoscaler/pr-cp-reg-10027 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10038 - pr-customer-env] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10015 - pr-customer-env] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10047 - aris-keda] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/register-smtp-server-6d78c-stqpc (create-es-index-template) is restarting  times / 5 minutes.
3,[pr-cp-reg-10009 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10023 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container abs in pod dashboarding.
3,[pr-cp-reg-10012 - aris-cluster-autoscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10025 - aris-cluster-autoscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10048 - aris-sealed-secrets] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_sealedsecret_not_unsealed - There are secrets that could not be unsealed in last 30 minutes. Check the sealing key.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod backup-tenants-28125915-9ps25.
1,[pr-cp-reg-10050 - basic-application-bootstrapping] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10034 cluster.
3,[pr-cp-reg-10032 - aris-kube-downscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10022 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10040 - aris-spot] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10016 - aris-keda] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10015 - aris-sealed-secrets] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-136-224.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10047 - aris-kube-downscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10008 - aris-cluster-autoscaler] - ARIS_argocd_app_progressing - The ArgoCD app a6d67856bcc285f0f6ca4349c619dff9fba796cd94aeb13735a2f19b88abfe1e is progressing for longer than 15 minutes and requires manual intervention.
2,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10020 cluster may be low for segment merges.
1,[pr-cp-reg-10025 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10041 - pr-customer-env] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container log-backup in pod postgres.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controllermanager-svc in pod executor-svc.
1,[pr-cp-reg-10043 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10046 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10023 - kube-system] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod backup-logs-28124160-s7ps9.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface nvme2 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10014 - falcon-system] - ConfigReloaderSidecarErrors - Errors encountered while the falcon-falcon-sensor-2bjwf config-reloader sidecar attempts to sync config in falcon-system namespace. As a result, configuration for service running in falcon-falcon-sensor-2bjwf may be stale and cannot be updated anymore."
1,[pr-cp-reg-10045 - aris-keda] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10007 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10004 - aris-keda] - ARIS_host_oom_kill_detected - OOM kill detected.
4,[pr-cp-reg-10006 - basic-application-bootstrapping] - CPUThrottlingHigh -  throttling of CPU in namespace basic-application-bootstrapping for container kube-state-metrics in pod basic-application-bootstrapping-2wp59.
3,[pr-cp-reg-10023 - aris-nginx-ingress] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10026 - kube-public] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10014 - pr-customer-env] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10014 cluster.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container liveness-probe in pod ae-aw-euc1-15995-aws-load-balancer-controller.
3,[pr-cp-reg-10031 - aris-keda] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
0,"[pr-cp-reg-10007 - kasten-io] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_sealedsecret_not_unsealed - There are secrets that could not be unsealed in last 30 minutes. Check the sealing key.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod cdf.
1,[pr-cp-reg-10045 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10006 - aris-keda] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod cert-manager-cainjector-7d8985bbc8-x7www.
3,[pr-cp-reg-10006 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10038 - aris-spot-metrics] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10035 - aris-cluster-autoscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10011 - kube-system] - KubeNodeNotReady - ip-10-0-139-113.eu-central-1.compute.internal has been unready for more than 15 minutes.
3,[pr-cp-reg-10026 - aris-cluster-autoscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10034 - aris-nginx-ingress] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10012 - pr-customer-env] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod cdf-0.
3,[pr-cp-reg-10048 - aris-cluster-autoscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10021 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface tmpfs has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-sidecar in pod kanister-svc.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app aris-kube-prometheus-stack-prometheus is in unknown state for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod portalserver-0.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-nginx-ingress/pr-cp-reg-10008 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10013 - aris-keda] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container jobs-svc in pod state-svc.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124400-ktxxf (portalserver) is restarting  times / 5 minutes.
3,[pr-cp-reg-10042 - kube-public] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10045 - aris-kube-prometheus-stack] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container kube-prometheus-stack in pod aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4.
1,[pr-cp-reg-10003 - aris-cluster-autoscaler] - ARIS_k8s_pod_crash_looping - Pod aris-cluster-autoscaler/sealed-secrets-controller-cbbc8bd6b-qq22k (controller) is restarting  times / 5 minutes.
0,"[pr-cp-reg-10036 - kasten-io] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10002 - aris-sealed-secrets] - ARIS_k8s_pod_crash_looping - Pod aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-sealed-secrets has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10005 - aris-sealed-secrets] - ARIS_k8s_pod_restarted - The pod aris-sealed-secrets/sealed-secrets-controller has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-7lbrs (portalserver) is restarting  times / 5 minutes.
1,[pr-cp-reg-10003 - kasten-io] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - aris-keda] - CPUThrottlingHigh -  throttling of CPU in namespace aris-keda for container controller in pod keda-operator-metrics-apiserver.
3,[pr-cp-reg-10030 - kasten-io] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10049 - kube-system] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface nvme0 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper (setmaxmapcount) is restarting  times / 5 minutes.
1,[pr-cp-reg-10014 - aris-nginx-ingress] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10032 - basic-application-bootstrapping] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10020 - aris-spot-metrics] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10026 - aris-spot] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10010 - pr-customer-env] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container external-dns in pod efs-csi-node-4cgpg.
2,[pr-cp-reg-10043 - aris-kube-prometheus-stack] - ARIS_k8s_volume_out_of_disk_space - Volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10031 - aris-nginx-ingress] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
2,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10036 cluster.
3,[pr-cp-reg-10028 - kube-node-lease] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_argocd_app_unknown - The ArgoCD app tls-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10037 - pr-customer-env-spark] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,"[pr-cp-reg-10001 - aris-spot] - ConfigReloaderSidecarErrors - Errors encountered while the spotinst-kubernetes-cluster-controller-cd9fc697f-hx99n config-reloader sidecar attempts to sync config in aris-spot namespace. As a result, configuration for service running in spotinst-kubernetes-cluster-controller-cd9fc697f-hx99n may be stale and cannot be updated anymore."
4,[pr-cp-reg-10024 - kube-system] - KubeQuotaFullyUsed - Namespace kube-system is using  of its memory quota.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/simulation-0 (tenant-backup) is restarting  times / 5 minutes.
3,[pr-cp-reg-10048 - aris-spot] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10019 - aris-keda] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
0,"[pr-cp-reg-10010 - kube-node-lease] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container dashboardbff-svc in pod metering-svc.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_argocd_app_progressing - The ArgoCD app 04abfe3da01db1f02bdd752a9e3436b7cf688976b23b6f6c51773d813348eea1 is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10034 - kube-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10018 - default] - CPUThrottlingHigh -  throttling of CPU in namespace default for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10044 - aris-sealed-secrets] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10045 - aris-nginx-ingress] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter management/aris-management-ingress-delay-ntdn6"
3,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_k8s_pod_crash_looping - Pod aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k (controller) is restarting  times / 5 minutes.
4,[pr-cp-reg-10012 - aris-kube-downscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-downscaler for container controller in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app commandPalette is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10025 - pr-customer-env-spark] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10033 - falcon-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10020 - aris-kube-downscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,"[pr-cp-reg-10002 - kube-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kube-system/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
1,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_k8s_pod_restarted - The pod aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10026 - pr-customer-env-spark] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on nvme3 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10019 - aris-spot-metrics] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10011 cluster.
1,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10026 - aris-keda] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on eni006a31b57f1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10046 - falcon-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env/backup-logs-8ca1951098b8 failed to complete.
3,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10026 - pr-customer-env] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10024 - aris-keda] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10043 - aris-keda] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10019 - aris-kube-downscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10004 - aris-kube-downscaler] - ARIS_k8s_pod_crash_looping - Pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10005 - kasten-io] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10018 - pr-customer-env-spark] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10032 - kube-node-lease] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10042 - aris-keda] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod cloudsearch.
1,[pr-cp-reg-10009 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10046 - aris-cluster-autoscaler] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-cluster-autoscaler/pr-cp-reg-10046 cluster is .
3,[pr-cp-reg-10015 - aris-keda] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10022 - aris-sealed-secrets] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10001 - management] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_k8s_pod_restarted - The pod aris-nginx-ingress/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,"[pr-cp-reg-10012 - kube-node-lease] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-node-lease namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10026 - aris-kube-downscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10036 - aris-keda] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-keda has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10009 - aris-sealed-secrets] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10031 - pr-customer-env] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10029 - pr-customer-env-spark] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-147-93.eu-central-1.compute.internal in pr-customer-env-spark/pr-cp-reg-10029 cluster is .
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf (cdf) is restarting  times / 5 minutes.
3,[pr-cp-reg-10029 - aris-sealed-secrets] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10009 - pr-customer-env] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10043 - aris-nginx-ingress] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10031 - aris-sealed-secrets] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container kube-state-metrics in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
1,[pr-cp-reg-10009 - kube-node-lease] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme1n1"" changing its up status often on node-exporter management/cert-manager-cainjector-7d8985bbc8-x7www"
3,[pr-cp-reg-10049 - aris-spot] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/register-smtp-server-6d78c-stqpc (cloudsearch) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod cloudsearch-0.
3,[pr-cp-reg-10043 - aris-spot] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10026 - aris-cluster-autoscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k"
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processboard (ces) is restarting  times / 5 minutes.
1,[pr-cp-reg-10014 - aris-cluster-autoscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod ces.
3,[pr-cp-reg-10036 - pr-customer-env] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
4,[pr-cp-reg-10016 - aris-keda] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-keda/exported_job failed to complete.
1,[pr-cp-reg-10013 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
0,"[pr-cp-reg-10022 - management] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
2,[pr-cp-reg-10046 - aris-keda] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10046 cluster.
3,[pr-cp-reg-10003 - kube-system] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-provisioner in pod argocd-dex-server-6b74fb9695-kbk62.
3,[pr-cp-reg-10006 - management] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10044 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10008 - aris-kube-downscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10003 - aris-sealed-secrets] - ARIS_k8s_pod_crash_looping - Pod aris-sealed-secrets/sealed-secrets-controller (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10002 - kube-node-lease] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10013 - aris-kube-downscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10023 - aris-nginx-ingress] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10010 - aris-spot] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-143-220.eu-central-1.compute.internal in aris-spot/pr-cp-reg-10010 cluster is .
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/tm (collaboration) is restarting  times / 5 minutes.
3,[pr-cp-reg-10030 - aris-sealed-secrets] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10018 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-sealed-secrets/pr-cp-reg-10018 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_progressing - The ArgoCD app scaledobject is progressing for longer than 15 minutes and requires manual intervention.
3,"[pr-cp-reg-10006 - aris-nginx-ingress] - ConfigReloaderSidecarErrors - Errors encountered while the aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4 config-reloader sidecar attempts to sync config in aris-nginx-ingress namespace. As a result, configuration for service running in aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10049 - aris-spot-metrics] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10007 - falcon-system] - CPUThrottlingHigh -  throttling of CPU in namespace falcon-system for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10002 - aris-sealed-secrets] - ARIS_argocd_app_progressing - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10032 - aris-keda] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
4,[pr-cp-reg-10029 - pr-customer-env] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10022 - aris-spot] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10022 cluster.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container sentinel in pod cert-manager-795fd7b44f-f5hrn.
0,"[pr-cp-reg-10001 - falcon-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10042 - aris-keda] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10031 - kube-node-lease] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container auth-svc in pod executor-svc-5fbdbbc689-7nnch.
3,[pr-cp-reg-10048 - aris-spot] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10039 - aris-kube-downscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface eth0 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10013 - kube-system] - KubeQuotaExceeded - Namespace kube-system is using  of its memory quota.
1,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-prometheus-stack/pr-cp-reg-10030 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod abs-0.
3,[pr-cp-reg-10028 - aris-cluster-autoscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10043 - pr-customer-env] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10012 - pr-customer-env] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10002 - falcon-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10014 - aris-spot] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot/pr-cp-reg-10014 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10033 - aris-nginx-ingress] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10033 - aris-spot] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10013 - aris-keda] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container container in pod loadbalancer-0.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env/cloudsearch"
3,[pr-cp-reg-10005 - aris-sealed-secrets] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10026 - pr-customer-env-spark] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10004 - kasten-io] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme1n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (cdf) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on nvme1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10007 - aris-spot] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10046 - aris-sealed-secrets] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10005 - aris-sealed-secrets] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10020 - pr-customer-env] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin-0 (tenant-backup) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container abs in pod backup-logs-28124400-ktxxf.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on lo at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10034 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10019 - aris-cluster-autoscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10041 - aris-spot] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot/pr-cp-reg-10041 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10030 - kube-public] - CPUThrottlingHigh -  throttling of CPU in namespace kube-public for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10024 - aris-sealed-secrets] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod argocd-redis-6645d4fb89-kpv95.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter pr-customer-env/zookeeper"
2,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10017 cluster.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10006 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env/backup-logs-28124160 failed to complete.
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10017 - aris-cluster-autoscaler] - ARIS_argocd_app_unknown - The ArgoCD app a6d67856bcc285f0f6ca4349c619dff9fba796cd94aeb13735a2f19b88abfe1e is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10025 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 8h to complete.
1,[pr-cp-reg-10020 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
4,[pr-cp-reg-10020 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_suspended - CronJob pr-customer-env/backup-tenants is suspended.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding (portalserver) is restarting  times / 5 minutes.
1,[pr-cp-reg-10013 - aris-sealed-secrets] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10024 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10043 - aris-spot-metrics] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
0,"[pr-cp-reg-10006 - aris-kube-downscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10026 - aris-sealed-secrets] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10049 - aris-keda] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processengine in pod cloudsearch-0.
1,[pr-cp-reg-10048 - aris-kube-downscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10044 - aris-keda] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10002 - pr-customer-env] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer-0 (processboard) is restarting  times / 5 minutes.
1,[pr-cp-reg-10018 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10007 - aris-spot] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine (serviceenabling) is restarting  times / 5 minutes.
3,[pr-cp-reg-10006 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme6n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container tools in pod aggregatedapis-svc.
3,"[pr-cp-reg-10032 - kube-node-lease] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-node-lease namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container metrics-server in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10013 - kube-system] - KubeletPodStartUpLatencyHigh - Kubelet Pod startup 99th percentile latency is  seconds on node ip-10-0-139-113.eu-central-1.compute.internal.
3,[pr-cp-reg-10002 - aris-spot] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
0,"[pr-cp-reg-10023 - kube-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
2,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10007 cluster.
4,[pr-cp-reg-10002 - aris-nginx-ingress] - CPUThrottlingHigh -  throttling of CPU in namespace aris-nginx-ingress for container controller in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10050 - aris-nginx-ingress] - ARIS_postgresql_down - The postgres pod of namespace aris-nginx-ingress is down.
3,"[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10030 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10041 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot-metrics/pr-cp-reg-10041 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres (elastic-metrics-sidecar) is restarting  times / 5 minutes.
3,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10039 - aris-kube-prometheus-stack] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""tmpfs"" changing its up status often on node-exporter aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus-0"
3,[pr-cp-reg-10024 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10004 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API errors/kube-system has been only % available over the last 10m.
3,[pr-cp-reg-10027 - aris-kube-prometheus-stack] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10012 - aris-keda] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app topnav is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10026 - aris-kube-downscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10027 - aris-sealed-secrets] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10044 - kube-system] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_k8s_pod_crash_looping - Pod aris-nginx-ingress/sealed-secrets-controller-cbbc8bd6b-qq22k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10046 - kube-system] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10013 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-cluster-autoscaler/pr-cp-reg-10013 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface nvme0 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10003 - basic-application-bootstrapping] - CPUThrottlingHigh -  throttling of CPU in namespace basic-application-bootstrapping for container kube-state-metrics in pod basic-application-bootstrapping-2wp59.
1,[pr-cp-reg-10003 - default] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10044 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10050 - aris-kube-prometheus-stack] - ARIS_host_oom_kill_detected - OOM kill detected.
3,"[pr-cp-reg-10012 - aris-cluster-autoscaler] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller-cbbc8bd6b-qq22k config-reloader sidecar attempts to sync config in aris-cluster-autoscaler namespace. As a result, configuration for service running in sealed-secrets-controller-cbbc8bd6b-qq22k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10025 - kube-public] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10004 - aris-sealed-secrets] - CPUThrottlingHigh -  throttling of CPU in namespace aris-sealed-secrets for container kube-state-metrics in pod sealed-secrets-controller.
1,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10009 - kube-system] - KubeletServerCertificateExpiration - Server certificate for Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal expires in .
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container redis in pod argocd-redis-ha-haproxy.
3,[pr-cp-reg-10042 - aris-nginx-ingress] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10037 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-downscaler/pr-cp-reg-10037 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the kube-state-metrics/sealed-secrets-controller targets in aris-kube-prometheus-stack namespace are down.
3,[pr-cp-reg-10010 - aris-keda] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10032 - kube-public] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
3,[pr-cp-reg-10002 - management] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme5n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_node_disk_pressure - ip-10-0-138-163.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10001 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API 0592d0e00c13ca08599ec30fab4da59b0de6c8ceeeb333d975a04fb1b65e3da3/kube-system has been only % available over the last 10m.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_argocd_app_progressing - The ArgoCD app aris-solutionsgallery-secret is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10040 - aris-keda] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10036 - aris-keda] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
2,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10035 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine (zookeeper) is restarting  times / 5 minutes.
1,[pr-cp-reg-10041 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_unknown - The ArgoCD app 23122faab4957433d2ae942409d7bd757433bb06c6ef58aabc0599bb6382b619 is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10029 - aris-spot-metrics] - ARIS_postgresql_down - The postgres pod of namespace aris-spot-metrics is down.
1,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-138-163.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,"[pr-cp-reg-10004 - aris-nginx-ingress] - ConfigReloaderSidecarErrors - Errors encountered while the aris-nginx-ingress-ingress-nginx-controller config-reloader sidecar attempts to sync config in aris-nginx-ingress namespace. As a result, configuration for service running in aris-nginx-ingress-ingress-nginx-controller may be stale and cannot be updated anymore."
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env/backup-tenants-28123035 failed to complete.
1,[pr-cp-reg-10048 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10041 - aris-kube-prometheus-stack] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10007 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10028 - pr-customer-env-spark] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10018 - aris-kube-downscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
2,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10020 cluster.
1,[pr-cp-reg-10050 - pr-customer-env-spark] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10021 - aris-keda] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10037 - kube-public] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10010 - aris-keda] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10003 - aris-kube-downscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10006 - kube-system] - KubeClientErrors - Kubernetes API server client 'coredns/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10031 - kube-node-lease] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
2,[pr-cp-reg-10013 - pr-customer-env] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10013 cluster.
3,"[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10021 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10011 - aris-nginx-ingress] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10045 - pr-customer-env-spark] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10015 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10030 cluster.
1,[pr-cp-reg-10048 - aris-sealed-secrets] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod backup-tenants-28124475-fjqvp.
3,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10004 - kube-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kube-system/kube-proxy-5lqbj"
4,[pr-cp-reg-10025 - aris-cluster-autoscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-cluster-autoscaler has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
0,"[pr-cp-reg-10022 - falcon-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_k8s_node_pid_pressure - ip-10-0-138-163.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10034 cluster.
3,[pr-cp-reg-10042 - aris-kube-downscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10019 - aris-spot] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_argocd_app_out_of_sync - The ArgoCD app 46d6ed101040cf8df2a51291aad961a76c082eba555ed2401548a98d1bb4d54d is out of sync for longer than 10 minutes and requires manual intervention.
2,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10006 cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container ces in pod zookeeper.
3,[pr-cp-reg-10022 - kube-public] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10011 - aris-sealed-secrets] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10037 - aris-nginx-ingress] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10030 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10042 - aris-kube-downscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10006 - aris-spot] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10042 - kube-system] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10048 - aris-kube-downscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env-spark has faced > 5 deadlocks in last minute.
4,[pr-cp-reg-10039 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env/exported_job failed to complete.
3,[pr-cp-reg-10050 - aris-kube-downscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10012 - pr-customer-env] - ARIS_host_oom_kill_detected - OOM kill detected.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container ces in pod cdf.
1,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10015 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10046 - aris-kube-prometheus-stack] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10044 - aris-kube-downscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10030 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10010 - aris-nginx-ingress] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-nginx-ingress has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container external-dns in pod argocd-dex-server-6b74fb9695-kbk62.
1,[pr-cp-reg-10015 - kube-public] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10035 - aris-nginx-ingress] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10013 - aris-sealed-secrets] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller-cbbc8bd6b-qq22k config-reloader sidecar attempts to sync config in aris-sealed-secrets namespace. As a result, configuration for service running in sealed-secrets-controller-cbbc8bd6b-qq22k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10034 - aris-kube-prometheus-stack] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10001 - kube-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10037 - aris-nginx-ingress] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/cdf"
3,[pr-cp-reg-10027 - pr-customer-env] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10011 - aris-keda] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper (elasticsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10048 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-cluster-autoscaler/pr-cp-reg-10021 health status has been RED for at least 2 minutes.
3,"[pr-cp-reg-10019 - aris-keda] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,"[pr-cp-reg-10005 - aris-cluster-autoscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf config-reloader sidecar attempts to sync config in aris-cluster-autoscaler namespace. As a result, configuration for service running in aris-cluster-autoscaler-aws-cluster-autoscaler-7b6ffcbbf5-khvzf may be stale and cannot be updated anymore."
3,[pr-cp-reg-10016 - pr-customer-env-spark] - ARIS_argocd_app_out_of_sync - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is out of sync for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10024 - falcon-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter falcon-system/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10030 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container controller in pod aris-kube-prometheus-stack-operator-78b758c475-qs69v.
3,[pr-cp-reg-10046 - kasten-io] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10012 - pr-customer-env] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""nvme1"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-1"
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eth0"" changing its up status often on node-exporter aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus"
2,[pr-cp-reg-10002 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/data-volume-zookeeper-0 is full (< 5% free space left) and should be increased.
1,[pr-cp-reg-10006 - aris-kube-downscaler] - ARIS_k8s_pod_crash_looping - Pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k (controller) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10040 - kube-public] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-public namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10041 - kasten-io] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding (log-backup) is restarting  times / 5 minutes.
3,[pr-cp-reg-10018 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10042 - falcon-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10028 - aris-spot-metrics] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot-metrics namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10033 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10006 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-sealed-secrets/pr-cp-reg-10006 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10037 - aris-spot] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10022 - aris-kube-downscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10047 - aris-kube-downscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-downscaler is down.
3,[pr-cp-reg-10038 - pr-customer-env-spark] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10044 - pr-customer-env] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env is down.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container state-svc in pod controllermanager-svc-788dc96c8d-rg47t.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces (octopus) is restarting  times / 5 minutes.
1,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10036 cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container umcadmin in pod kanister-job-6gvp5.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container ces in pod tm-0.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-svc in pod metering-svc.
3,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10009 - aris-cluster-autoscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
0,"[pr-cp-reg-10050 - aris-sealed-secrets] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10021 - aris-cluster-autoscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10016 - pr-customer-env] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-sidecar in pod kanister-svc-9559f74-79zgn.
3,[pr-cp-reg-10047 - falcon-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10043 - aris-nginx-ingress] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_pod_restarted - The pod aris-nginx-ingress/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10044 - aris-kube-downscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10045 - kube-public] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container efs-plugin in pod efs-csi-node-4cgpg.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (setmaxmapcount) is restarting  times / 5 minutes.
1,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_k8s_node_pid_pressure - ip-10-0-147-93.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10036 - pr-customer-env-spark] - ARIS_k8s_node_disk_pressure - ip-10-0-147-93.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10009 - kasten-io] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cdf in pod dashboarding.
0,"[pr-cp-reg-10035 - basic-application-bootstrapping] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10001 - aris-spot-metrics] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
4,[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-downscaler/exported_job failed to complete.
1,[pr-cp-reg-10006 - kube-system] - KubeletClientCertificateExpiration - Client certificate for Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal expires in .
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-cluster-autoscaler/pr-cp-reg-10040 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container dashboarding in pod loadbalancer-0.
3,[pr-cp-reg-10014 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
4,[pr-cp-reg-10050 - pr-customer-env-spark] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env-spark/exported_job failed to complete.
1,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10037 - aris-sealed-secrets] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_k8s_pod_restarted - The pod pr-customer-env-spark/pr-customer-env-spark-spark-operator-74d54645cb-5gpbl has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10016 - aris-keda] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10032 - aris-sealed-secrets] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-repo-server in pod argocd-redis-ha-server-2.
1,[pr-cp-reg-10039 - pr-customer-env] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env/backup-logs-8ca1951098b8 failed to complete.
1,[pr-cp-reg-10001 - management] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container get-zone in pod ces.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod processboard.
1,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
3,"[pr-cp-reg-10029 - pr-customer-env-spark] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10011 - kube-public] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container auth-svc in pod aggregatedapis-svc-5c7ccfdd78-slnk8.
3,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10034 - kube-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10009 - aris-sealed-secrets] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5 failed to send  of notifications to opsgenie.
3,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
2,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10041 cluster.
2,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10017 cluster may be low for segment merges.
3,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10011 - kube-system] - KubeletServerCertificateExpiration - Server certificate for Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal expires in .
2,[pr-cp-reg-10040 - aris-keda] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10040 cluster may be low for segment merges.
2,[pr-cp-reg-10047 - pr-customer-env-spark] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10047 cluster.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eni006a31b57f1"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5"
1,[pr-cp-reg-10028 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10050 - kube-public] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container controllermanager-svc in pod jobs-svc.
3,[pr-cp-reg-10019 - aris-keda] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10021 - falcon-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to telegram sent from any instance in the kube-state-metrics cluster is .
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10003 - kasten-io] - NodeRAIDDegraded - RAID array '/dev/nvme1n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,"[pr-cp-reg-10001 - aris-nginx-ingress] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-nginx-ingress namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10025 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elastic-metrics-sidecar in pod dashboarding.
3,[pr-cp-reg-10049 - aris-cluster-autoscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
2,[pr-cp-reg-10009 - aris-sealed-secrets] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10009 cluster.
3,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_argocd_app_unknown - The ArgoCD app aris-secret-aris is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10006 - aris-keda] - ARIS_postgresql_down - The postgres pod of namespace aris-keda is down.
3,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container executor-svc in pod executor-svc-5fbdbbc689-7nnch.
1,[pr-cp-reg-10033 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10041 - pr-customer-env] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10003 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,"[pr-cp-reg-10001 - management] - ConfigReloaderSidecarErrors - Errors encountered while the ebs-csi-node config-reloader sidecar attempts to sync config in management namespace. As a result, configuration for service running in ebs-csi-node may be stale and cannot be updated anymore."
3,[pr-cp-reg-10042 - aris-kube-downscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10013 - kube-system] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
1,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_k8s_node_pid_pressure - ip-10-0-147-93.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10042 - aris-keda] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager has only found  members of the aris-kube-prometheus-stack-grafana cluster.
3,"[pr-cp-reg-10024 - aris-spot-metrics] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot-metrics namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container setmaxmapcount in pod tm-0.
1,[pr-cp-reg-10002 - aris-spot] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10047 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod abs-0.
3,[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10005 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'eni0039e8a4ad3' needs attention and possibly a disk swap.
1,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_k8s_node_disk_pressure - ip-10-0-143-220.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10014 - aris-keda] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-keda/pr-cp-reg-10014 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env-spark has too many active connections. More than 90% of available connections are used.
2,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/logs is full (< 5% free space left) and should be increased.
1,[pr-cp-reg-10007 - aris-keda] - ARIS_k8s_pod_restarted - The pod aris-keda/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
0,"[pr-cp-reg-10006 - kasten-io] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10029 - aris-keda] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus-0 (grafana) is restarting  times / 5 minutes.
1,[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_node_memory_pressure - ip-10-0-138-163.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (kube-prometheus-stack) is restarting  times / 5 minutes.
3,[pr-cp-reg-10015 - aris-kube-downscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10049 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10048 - kube-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10048 - basic-application-bootstrapping] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_argocd_app_unknown - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is in unknown state for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app database_metrics is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_progressing - The ArgoCD app triggerauthentication is progressing for longer than 15 minutes and requires manual intervention.
2,[pr-cp-reg-10030 - aris-keda] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10030 cluster.
3,[pr-cp-reg-10028 - pr-customer-env-spark] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10047 - kube-public] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10049 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10044 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kanister-sidecar in pod processengine-0.
1,[pr-cp-reg-10022 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10031 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10037 - default] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10024 - aris-kube-downscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter management/efs-csi-node-4bvqz"
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod elasticsearch-default-0.
3,[pr-cp-reg-10028 - aris-nginx-ingress] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10044 - pr-customer-env] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10002 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,"[pr-cp-reg-10001 - pr-customer-env] - ConfigReloaderSidecarErrors - Errors encountered while the simulation-0 config-reloader sidecar attempts to sync config in pr-customer-env namespace. As a result, configuration for service running in simulation-0 may be stale and cannot be updated anymore."
4,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env-spark/exported_job failed to complete.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container config-init in pod management-external-dns-65bfcbd6d7-zlfjj.
3,[pr-cp-reg-10030 - aris-keda] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10004 - aris-keda] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
4,[pr-cp-reg-10035 - aris-kube-prometheus-stack] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-prometheus-stack/exported_job failed to complete.
4,[pr-cp-reg-10021 - kube-system] - KubeQuotaAlmostFull - Namespace kube-system is using  of its memory quota.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10043 - kube-public] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10038 - aris-kube-prometheus-stack] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,"[pr-cp-reg-10022 - pr-customer-env] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10048 - pr-customer-env-spark] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10007 - aris-spot] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10007 cluster.
3,[pr-cp-reg-10032 - pr-customer-env-spark] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container cert-manager in pod argocd-redis-ha-server-0.
3,[pr-cp-reg-10031 - aris-sealed-secrets] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10047 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10005 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10022 - aris-spot-metrics] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10030 - aris-keda] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10031 - aris-spot-metrics] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10008 - kube-public] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
1,[pr-cp-reg-10008 - aris-keda] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10008 cluster.
3,[pr-cp-reg-10014 - aris-keda] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10015 - pr-customer-env] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10043 - kube-system] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus (init-config-reloader) is restarting  times / 5 minutes.
3,[pr-cp-reg-10039 - pr-customer-env] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10009 - kube-public] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10025 - aris-kube-downscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,"[pr-cp-reg-10022 - pr-customer-env-spark] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container tools in pod executor-svc.
3,[pr-cp-reg-10005 - kube-public] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod processboard-0.
1,[pr-cp-reg-10016 - aris-keda] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-keda/pr-cp-reg-10016 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10003 - management] - NodeRAIDDegraded - RAID array '/dev/nvme6n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10005 - kube-system] - KubeNodeUnreachable - ip-10-0-136-224.eu-central-1.compute.internal is unreachable and some workloads may be rescheduled.
3,[pr-cp-reg-10046 - kube-node-lease] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10015 - aris-nginx-ingress] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container admintools in pod backup-tenants-28125915-9ps25.
3,[pr-cp-reg-10007 - aris-cluster-autoscaler] - ARIS_argocd_app_progressing - The ArgoCD app a6d67856bcc285f0f6ca4349c619dff9fba796cd94aeb13735a2f19b88abfe1e is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10032 - aris-sealed-secrets] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10023 - aris-keda] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app a6d67856bcc285f0f6ca4349c619dff9fba796cd94aeb13735a2f19b88abfe1e is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10037 - aris-kube-prometheus-stack] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
4,[pr-cp-reg-10042 - aris-spot-metrics] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot-metrics for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10050 - aris-spot] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10009 - pr-customer-env-spark] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10032 - aris-keda] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod processboard.
3,[pr-cp-reg-10016 - kube-system] - KubeletClientCertificateRenewalErrors - Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal has failed to renew its client certificate ( errors in the last 5 minutes).
3,[pr-cp-reg-10001 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme6n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10047 - aris-kube-prometheus-stack] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10037 - aris-keda] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10029 - aris-keda] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10035 - kube-node-lease] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager has only found  members of the kube-state-metrics cluster.
3,[pr-cp-reg-10012 - aris-keda] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10024 - basic-application-bootstrapping] - ConfigReloaderSidecarErrors - Errors encountered while the basic-application-bootstrapping-2wp59 config-reloader sidecar attempts to sync config in basic-application-bootstrapping namespace. As a result, configuration for service running in basic-application-bootstrapping-2wp59 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10045 - aris-nginx-ingress] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10009 - aris-spot-metrics] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10026 - pr-customer-env] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container executor-svc in pod dashboardbff-svc.
3,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_host_oom_kill_detected - OOM kill detected.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod portalserver-0.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter management/argocd-server-74fbd754cb-n2sgr"
1,[pr-cp-reg-10013 - aris-nginx-ingress] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10001 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10012 - pr-customer-env] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10006 - falcon-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10028 - management] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env-spark/pr-customer-env-spark-spark-operator-74d54645cb-5gpbl (main) is restarting  times / 5 minutes.
3,[pr-cp-reg-10048 - kube-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10043 - aris-cluster-autoscaler] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10005 - management] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10004 - falcon-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10004 - kube-system] - ConfigReloaderSidecarErrors - Errors encountered while the aws-node-4fvrp config-reloader sidecar attempts to sync config in kube-system namespace. As a result, configuration for service running in aws-node-4fvrp may be stale and cannot be updated anymore."
4,[pr-cp-reg-10033 - aris-spot-metrics] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot-metrics for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10045 - pr-customer-env] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env is down.
3,[pr-cp-reg-10049 - aris-nginx-ingress] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
4,[pr-cp-reg-10021 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_suspended - CronJob pr-customer-env/backup-tenants is suspended.
1,[pr-cp-reg-10029 - aris-keda] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10013 - pr-customer-env] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10045 - aris-kube-downscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app dashboardComments is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10023 - pr-customer-env] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env is down.
3,[pr-cp-reg-10035 - aris-cluster-autoscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10043 - pr-customer-env-spark] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-server in pod argocd-redis-ha-haproxy-84b857bc4b-gksr6.
1,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10027 cluster.
3,[pr-cp-reg-10010 - aris-nginx-ingress] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,"[pr-cp-reg-10031 - aris-kube-prometheus-stack] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana will run full within 24 hours."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container portalserver in pod simulation-0.
3,"[pr-cp-reg-10031 - aris-cluster-autoscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10040 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10040 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28125915-9ps25 (loadbalancer) is restarting  times / 5 minutes.
3,[pr-cp-reg-10016 - pr-customer-env] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10013 - aris-sealed-secrets] - ARIS_k8s_pod_restarted - The pod aris-sealed-secrets/sealed-secrets-controller has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10001 - management] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-secret-smtp is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding-0 (setmaxmapcount) is restarting  times / 5 minutes.
1,[pr-cp-reg-10012 - kasten-io] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10040 - aris-keda] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf-0 (zookeeper) is restarting  times / 5 minutes.
4,[pr-cp-reg-10004 - default] - CPUThrottlingHigh -  throttling of CPU in namespace default for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10042 - aris-spot-metrics] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
2,[pr-cp-reg-10047 - aris-keda] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10047 cluster may be low for segment merges.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod octopus-0.
2,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10003 cluster.
3,[pr-cp-reg-10023 - kasten-io] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container liveness-probe in pod ebs-csi-node-2fbnc.
1,[pr-cp-reg-10023 - kube-system] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container bloblifecyclemanager-svc in pod gateway.
0,"[pr-cp-reg-10030 - kube-node-lease] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10017 - aris-sealed-secrets] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
2,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/data-volume-postgres-0 is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10041 - aris-spot] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10030 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on nvme0 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10029 - pr-customer-env-spark] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana has only found  members of the node-exporter cluster.
3,[pr-cp-reg-10035 - kube-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10024 - aris-nginx-ingress] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10020 - pr-customer-env] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10006 - aris-spot] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-spot/spotinst-kubernetes-cluster-controller"
1,[pr-cp-reg-10035 - default] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container dex in pod argocd-redis-ha-server-2.
3,[pr-cp-reg-10050 - aris-kube-downscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10046 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10003 - aris-kube-downscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elastic-metrics-sidecar in pod tm.
3,[pr-cp-reg-10042 - aris-spot-metrics] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,"[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10045 - aris-cluster-autoscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container abs in pod tm-0.
3,[pr-cp-reg-10024 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10001 - aris-cluster-autoscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10021 - aris-keda] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_cluster_autoscaler_unschedulable_pods - The cluster autoscaler is unable to scale up and there are unschedulable pods because of this condition.
1,[pr-cp-reg-10039 - kube-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10039 - aris-sealed-secrets] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10005 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10032 - kube-system] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,[pr-cp-reg-10041 - aris-kube-downscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10007 - aris-cluster-autoscaler] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10026 - aris-kube-prometheus-stack] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10014 - aris-spot] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10008 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10013 - kube-public] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10039 - pr-customer-env-spark] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - AlertmanagerClusterCrashlooping -  of Alertmanager instances within the aris-kube-prometheus-stack-alertmanager cluster have restarted at least 5 times in the last 10m.
3,[pr-cp-reg-10011 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod argocd-redis-ha-haproxy-84b857bc4b-8lfwq.
3,[pr-cp-reg-10017 - aris-keda] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-keda has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10030 - pr-customer-env] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10022 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot-metrics/pr-cp-reg-10022 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_argocd_app_progressing - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on eni006a31b57f1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10017 - aris-spot] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine-0 (container) is restarting  times / 5 minutes.
3,[pr-cp-reg-10002 - kube-system] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10006 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
2,[pr-cp-reg-10046 - aris-cluster-autoscaler] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10046 cluster.
3,"[pr-cp-reg-10017 - pr-customer-env-spark] - ConfigReloaderSidecarErrors - Errors encountered while the pr-customer-env-spark-spark-operat-wh-init-hwzrg config-reloader sidecar attempts to sync config in pr-customer-env-spark namespace. As a result, configuration for service running in pr-customer-env-spark-spark-operat-wh-init-hwzrg may be stale and cannot be updated anymore."
3,[pr-cp-reg-10039 - kasten-io] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10018 - kube-node-lease] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container frontend-svc in pod crypto-svc.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10002 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver-0 (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10002 - aris-kube-downscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine (kube-state-metrics) is restarting  times / 5 minutes.
1,[pr-cp-reg-10033 - aris-spot-metrics] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_argocd_app_progressing - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10027 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot-metrics/pr-cp-reg-10027 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10011 - aris-cluster-autoscaler] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10048 - aris-kube-downscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10020 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-prometheus-stack/pr-cp-reg-10020 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10005 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerClusterCrashlooping -  of Alertmanager instances within the kube-state-metrics cluster have restarted at least 5 times in the last 10m.
1,[pr-cp-reg-10005 - management] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10027 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env-spark/pr-cp-reg-10027 health status has been RED for at least 2 minutes.
2,[pr-cp-reg-10013 - pr-customer-env] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10013 cluster may be low for segment merges.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter management/argocd-redis-ha-server-2"
3,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10013 - aris-cluster-autoscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container events-svc in pod aggregatedapis-svc-5c7ccfdd78-slnk8.
1,[pr-cp-reg-10016 - aris-nginx-ingress] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10016 cluster.
1,[pr-cp-reg-10001 - aris-cluster-autoscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10030 - aris-sealed-secrets] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10009 - aris-nginx-ingress] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-nginx-ingress/exported_job failed to complete.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app live-service-web-worker is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_postgresql_down - The postgres pod of namespace aris-sealed-secrets is down.
3,[pr-cp-reg-10024 - aris-keda] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10001 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot-metrics/pr-cp-reg-10001 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10003 - aris-kube-downscaler] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller-cbbc8bd6b-qq22k config-reloader sidecar attempts to sync config in aris-kube-downscaler namespace. As a result, configuration for service running in sealed-secrets-controller-cbbc8bd6b-qq22k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10029 - aris-keda] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10019 - pr-customer-env] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-sealed-secrets has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10016 - kube-system] - KubeNodeUnreachable - ip-10-0-136-224.eu-central-1.compute.internal is unreachable and some workloads may be rescheduled.
1,[pr-cp-reg-10004 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
2,[pr-cp-reg-10045 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10045 cluster.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10031 - aris-cluster-autoscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10005 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot-metrics/pr-cp-reg-10005 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10038 - aris-spot-metrics] - ARIS_postgresql_down - The postgres pod of namespace aris-spot-metrics is down.
1,[pr-cp-reg-10015 - falcon-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10001 - kube-system] - KubeletPodStartUpLatencyHigh - Kubelet Pod startup 99th percentile latency is  seconds on node ip-10-0-138-163.eu-central-1.compute.internal.
4,[pr-cp-reg-10001 - aris-sealed-secrets] - CPUThrottlingHigh -  throttling of CPU in namespace aris-sealed-secrets for container kube-state-metrics in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10002 - kasten-io] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10020 - kube-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eth2"" changing its up status often on node-exporter aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10035 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10019 - pr-customer-env-spark] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10032 - pr-customer-env] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper-0 (octopus) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container dex in pod argocd-repo-server.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer-0 (adsadmin) is restarting  times / 5 minutes.
1,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_k8s_pod_crash_looping - Pod aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-hwdv6 (kube-state-metrics) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/tmdata will run full within 24 hours."
0,"[pr-cp-reg-10047 - kube-public] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env/backup-logs-8ca1951098b8 failed to complete.
3,[pr-cp-reg-10016 - aris-kube-downscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10008 - kasten-io] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10044 - aris-keda] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding-0 (zookeeper) is restarting  times / 5 minutes.
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-sealed-secrets/pr-cp-reg-10033 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10046 - aris-cluster-autoscaler] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10022 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10028 - kube-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-nginx-ingress/exported_job failed to complete.
1,[pr-cp-reg-10003 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on lo at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10038 - aris-sealed-secrets] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10048 - aris-cluster-autoscaler] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter management/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
0,"[pr-cp-reg-10010 - aris-kube-prometheus-stack] - Watchdog - This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the ""DeadMansSnitch"" integration in PagerDuty."
1,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10038 - falcon-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-nginx-ingress/pr-cp-reg-10005 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_k8s_pod_restarted - The pod pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10032 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-sealed-secrets/pr-cp-reg-10032 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10014 - falcon-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod argocd-redis-ha-haproxy-84b857bc4b-gksr6.
3,[pr-cp-reg-10042 - pr-customer-env-spark] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10029 - aris-cluster-autoscaler] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10043 - aris-kube-prometheus-stack] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10019 - pr-customer-env-spark] - ARIS_argocd_app_unknown - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is in unknown state for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10009 - basic-application-bootstrapping] - CPUThrottlingHigh -  throttling of CPU in namespace basic-application-bootstrapping for container kube-state-metrics in pod basic-application-bootstrapping-2wp59.
3,"[pr-cp-reg-10004 - aris-sealed-secrets] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-sealed-secrets namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10033 - aris-cluster-autoscaler] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10005 - pr-customer-env-spark] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env-spark for container spark-operator in pod pr-customer-env-spark-spark-operat-wh-init-hwzrg.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container dex in pod argocd-dex-server-6b74fb9695-kbk62.
3,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_argocd_app_progressing - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10031 - kube-public] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_argocd_app_unknown - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is in unknown state for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container init-config-reloader in pod aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4.
1,[pr-cp-reg-10024 - pr-customer-env-spark] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10007 - aris-keda] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-keda has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10024 - aris-kube-downscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10007 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
0,"[pr-cp-reg-10017 - kube-node-lease] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10002 - aris-keda] - ConfigReloaderSidecarErrors - Errors encountered while the keda-operator-metrics-apiserver-7c746565cb-t6krb config-reloader sidecar attempts to sync config in aris-keda namespace. As a result, configuration for service running in keda-operator-metrics-apiserver-7c746565cb-t6krb may be stale and cannot be updated anymore."
3,[pr-cp-reg-10005 - aris-sealed-secrets] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10022 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 1h to complete.
2,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10046 cluster.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-provisioner in pod aris-management-ingress-delay-ntdn6.
2,[pr-cp-reg-10034 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10034 cluster may be low for segment merges.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container metering-svc in pod gateway.
1,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10010 - aris-cluster-autoscaler] - ARIS_argocd_app_unknown - The ArgoCD app a6d67856bcc285f0f6ca4349c619dff9fba796cd94aeb13735a2f19b88abfe1e is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10010 - kube-system] - KubeletClientCertificateRenewalErrors - Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal has failed to renew its client certificate ( errors in the last 5 minutes).
3,[pr-cp-reg-10041 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-cluster-autoscaler/pr-cp-reg-10041 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10026 - aris-sealed-secrets] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10019 - aris-cluster-autoscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,"[pr-cp-reg-10016 - falcon-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter falcon-system/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10017 - aris-kube-downscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10012 - aris-cluster-autoscaler] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-cluster-autoscaler/pr-cp-reg-10012 cluster is .
3,[pr-cp-reg-10001 - pr-customer-env-spark] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10023 - aris-spot] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10049 - aris-cluster-autoscaler] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-cluster-autoscaler/pr-cp-reg-10049 cluster is .
4,[pr-cp-reg-10002 - aris-keda] - CPUThrottlingHigh -  throttling of CPU in namespace aris-keda for container keda-operator-metrics-apiserver in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10003 - kube-system] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10008 - kube-node-lease] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
2,[pr-cp-reg-10019 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10019 cluster may be low for segment merges.
3,[pr-cp-reg-10005 - aris-keda] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10025 - aris-keda] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterCrashlooping -  of Alertmanager instances within the aris-kube-prometheus-stack-operator cluster have restarted at least 5 times in the last 10m.
1,[pr-cp-reg-10015 - kube-system] - KubeletServerCertificateExpiration - Server certificate for Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal expires in .
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container container in pod zookeeper-0.
1,[pr-cp-reg-10021 - aris-spot] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10023 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10004 - aris-cluster-autoscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-cluster-autoscaler for container aws-cluster-autoscaler in pod aris-cluster-autoscaler-aws-cluster-autoscaler.
3,[pr-cp-reg-10050 - aris-sealed-secrets] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10013 - aris-kube-downscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-downscaler has too many active connections. More than 90% of available connections are used.
4,[pr-cp-reg-10031 - aris-spot-metrics] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot-metrics for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces-0 (umcadmin) is restarting  times / 5 minutes.
3,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container liveness-probe in pod ebs-csi-node-bspjp.
4,[pr-cp-reg-10002 - aris-sealed-secrets] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-sealed-secrets/exported_job failed to complete.
3,[pr-cp-reg-10013 - aris-spot] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10048 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,[pr-cp-reg-10003 - pr-customer-env] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (log-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10050 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10048 - aris-cluster-autoscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10020 - aris-spot-metrics] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10034 cluster may be low for segment merges.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/serviceenabling (elasticsearch) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod dashboarding.
1,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,"[pr-cp-reg-10019 - kube-node-lease] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-node-lease namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,"[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10020 - aris-keda] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10017 - pr-customer-env] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env/pr-cp-reg-10017 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10049 - kube-public] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10049 - pr-customer-env] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10001 - falcon-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10005 - management] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_argocd_app_out_of_sync - The ArgoCD app 96ff84e25e745ef28b97b02b2ccf87ca2cb688b9ac0a8c41bb217d4cb18ccddb is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10022 - aris-kube-downscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10049 - aris-nginx-ingress] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container kube-state-metrics in pod aris-kube-prometheus-stack-prometheus-node-exporter.
3,[pr-cp-reg-10035 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding (loadbalancer) is restarting  times / 5 minutes.
1,[pr-cp-reg-10013 - aris-nginx-ingress] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10042 - pr-customer-env] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10034 - aris-kube-downscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-downscaler is down.
2,[pr-cp-reg-10010 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10010 cluster may be low for segment merges.
2,[pr-cp-reg-10016 - pr-customer-env] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10016 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin-0 (collaboration) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod cloudsearch-0.
3,[pr-cp-reg-10018 - default] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10045 - aris-nginx-ingress] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
2,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10043 cluster.
3,[pr-cp-reg-10026 - aris-spot] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerConfigInconsistent - Alertmanager instances within the kubelet cluster have different configurations.
3,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer (settcpkeepalivetime) is restarting  times / 5 minutes.
1,[pr-cp-reg-10033 - pr-customer-env] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod dashboarding.
3,[pr-cp-reg-10041 - aris-kube-downscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10008 - management] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10037 - aris-keda] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10005 - kube-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10032 - kube-system] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10039 - kube-node-lease] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10025 - aris-nginx-ingress] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10047 - aris-nginx-ingress] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10007 cluster.
2,[pr-cp-reg-10035 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10035 cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container container in pod backup-logs-28124400-ktxxf.
1,[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10006 - falcon-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10039 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10049 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container adsadmin in pod dashboarding.
3,[pr-cp-reg-10047 - aris-kube-downscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10028 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10049 - aris-nginx-ingress] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10008 - aris-keda] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-prometheus-stack has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10050 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod tm-0.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/octopus"
4,[pr-cp-reg-10042 - aris-nginx-ingress] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-nginx-ingress has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10005 - aris-spot] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10013 - aris-kube-downscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container external-dns in pod efs-csi-node.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container collaboration in pod loadbalancer-0.
3,[pr-cp-reg-10025 - kube-node-lease] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container octopus in pod processboard-0.
3,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
4,[pr-cp-reg-10001 - aris-spot] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot for container controller in pod spot-ocean-metric-exporter-56dfbdbbcb-82vqg.
3,[pr-cp-reg-10026 - aris-kube-downscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,"[pr-cp-reg-10009 - pr-customer-env-spark] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env-spark/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10012 - aris-spot-metrics] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""nvme0"" changing its up status often on node-exporter aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-1"
3,[pr-cp-reg-10031 - aris-kube-downscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10020 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10004 - aris-keda] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10031 - falcon-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_argocd_app_progressing - The ArgoCD app aris-nginx-ingress-ingress-nginx-leader is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-6gvp5 (log-backup) is restarting  times / 5 minutes.
3,[pr-cp-reg-10037 - aris-spot] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10020 - kube-node-lease] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
1,[pr-cp-reg-10015 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,"[pr-cp-reg-10001 - management] - ConfigReloaderSidecarErrors - Errors encountered while the argocd-dex-server-6b74fb9695-kbk62 config-reloader sidecar attempts to sync config in management namespace. As a result, configuration for service running in argocd-dex-server-6b74fb9695-kbk62 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10010 - aris-nginx-ingress] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10039 - kasten-io] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10012 - kube-system] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
1,[pr-cp-reg-10031 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10044 - pr-customer-env-spark] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10012 - aris-nginx-ingress] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10012 cluster.
3,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - aris-spot] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10016 - aris-kube-prometheus-stack] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
0,"[pr-cp-reg-10031 - aris-sealed-secrets] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10002 - aris-sealed-secrets] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10018 - pr-customer-env-spark] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env-spark is down.
1,[pr-cp-reg-10016 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10032 - aris-keda] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container portalserver in pod portalserver-0.
3,[pr-cp-reg-10012 - basic-application-bootstrapping] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app showDashboardValidationWarnings is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10037 - pr-customer-env] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10044 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-nginx-ingress/pr-cp-reg-10044 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10028 - falcon-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10017 - pr-customer-env] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10043 - pr-customer-env] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10013 - management] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10005 - kube-node-lease] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10026 - kube-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10003 - pr-customer-env] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10046 - pr-customer-env] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-cluster-autoscaler/exported_job failed to complete.
1,[pr-cp-reg-10040 - kube-public] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the sealed-secrets-controller/aris-kube-prometheus-stack-kube-state-metrics targets in aris-kube-prometheus-stack namespace are down.
3,[pr-cp-reg-10037 - pr-customer-env] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10030 - aris-sealed-secrets] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface tmpfs has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10028 - aris-cluster-autoscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_argocd_app_progressing - The ArgoCD app aris-nginx-ingress-ingress-nginx-leader is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10038 - pr-customer-env-spark] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10040 - falcon-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10034 - aris-kube-prometheus-stack] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10016 - aris-kube-prometheus-stack] - ARIS_k8s_node_pid_pressure - ip-10-0-138-163.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10007 - aris-kube-downscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-downscaler for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10007 - kube-system] - KubeNodeUnreachable - ip-10-0-139-113.eu-central-1.compute.internal is unreachable and some workloads may be rescheduled.
3,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
0,"[pr-cp-reg-10013 - falcon-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10027 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10010 - pr-customer-env] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,"[pr-cp-reg-10013 - aris-kube-downscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-kube-downscaler namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10008 - kube-system] - KubeQuotaExceeded - Namespace kube-system is using  of its cpu quota.
3,[pr-cp-reg-10002 - aris-kube-downscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10013 - aris-cluster-autoscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
0,"[pr-cp-reg-10012 - basic-application-bootstrapping] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10028 - aris-kube-downscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-downscaler is down.
3,"[pr-cp-reg-10021 - falcon-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter falcon-system/falcon-falcon-sensor-2bjwf"
3,[pr-cp-reg-10012 - pr-customer-env] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  transmit errors in the last two minutes.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container kube-state-metrics in pod aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod argocd-redis-ha-haproxy-84b857bc4b-gksr6.
3,[pr-cp-reg-10016 - aris-keda] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10048 - aris-cluster-autoscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
4,[pr-cp-reg-10005 - aris-nginx-ingress] - CPUThrottlingHigh -  throttling of CPU in namespace aris-nginx-ingress for container kube-state-metrics in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10040 - kube-public] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10044 - aris-spot] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10010 - aris-nginx-ingress] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10007 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10013 - falcon-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10033 - kube-public] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
0,"[pr-cp-reg-10010 - aris-cluster-autoscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10032 - pr-customer-env-spark] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10012 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env-spark is down.
1,[pr-cp-reg-10048 - kube-node-lease] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10015 - aris-kube-downscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10044 - management] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10005 - aris-keda] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10045 - aris-kube-prometheus-stack] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10045 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10038 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10044 - aris-keda] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10044 cluster.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10019 - aris-nginx-ingress] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available space left.
3,"[pr-cp-reg-10001 - pr-customer-env] - ConfigReloaderSidecarErrors - Errors encountered while the serviceenabling-0 config-reloader sidecar attempts to sync config in pr-customer-env namespace. As a result, configuration for service running in serviceenabling-0 may be stale and cannot be updated anymore."
4,[pr-cp-reg-10034 - aris-kube-downscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-downscaler has faced > 5 deadlocks in last minute.
4,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-nginx-ingress has faced > 5 deadlocks in last minute.
3,"[pr-cp-reg-10039 - aris-spot] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10005 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API forward/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
4,[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env-spark/exported_job failed to complete.
3,[pr-cp-reg-10018 - kube-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_argocd_app_unknown - The ArgoCD app e70b0088c1b43b0f14c2263b58659b01c1e590f5abbcacf71ac5eb80a6d11623 is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10022 - kube-system] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10005 - aris-cluster-autoscaler] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10031 - aris-kube-prometheus-stack] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10033 - kasten-io] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10002 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-sealed-secrets/pr-cp-reg-10002 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod serviceenabling.
4,[pr-cp-reg-10013 - pr-customer-env] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10005 - pr-customer-env] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,"[pr-cp-reg-10001 - pr-customer-env] - ConfigReloaderSidecarErrors - Errors encountered while the simulation config-reloader sidecar attempts to sync config in pr-customer-env namespace. As a result, configuration for service running in simulation may be stale and cannot be updated anymore."
3,[pr-cp-reg-10013 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10011 - aris-sealed-secrets] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10041 - aris-kube-downscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
4,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-prometheus-stack has faced > 5 deadlocks in last minute.
0,"[pr-cp-reg-10049 - aris-spot-metrics] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10013 - kube-system] - KubeletServerCertificateExpiration - Server certificate for Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal expires in .
1,[pr-cp-reg-10001 - pr-customer-env-spark] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10035 - aris-keda] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-keda/pr-cp-reg-10035 health status has been YELLOW for at least 20 minutes.
0,"[pr-cp-reg-10007 - basic-application-bootstrapping] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container config-init in pod efs-csi-node-2r2zp.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container events-svc in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
1,[pr-cp-reg-10010 - pr-customer-env] - ARIS_k8s_node_pid_pressure - ip-10-0-138-163.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10006 - aris-kube-prometheus-stack] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,"[pr-cp-reg-10003 - kube-system] - ConfigReloaderSidecarErrors - Errors encountered while the metrics-server-5756d96f6-tm5h8 config-reloader sidecar attempts to sync config in kube-system namespace. As a result, configuration for service running in metrics-server-5756d96f6-tm5h8 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10030 - aris-sealed-secrets] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10017 - kube-node-lease] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10009 - aris-spot-metrics] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot-metrics for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin (processboard) is restarting  times / 5 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/adsadmin-0 (abs) is restarting  times / 5 minutes.
1,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg (main) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container copyutil in pod argocd-dex-server-6b74fb9695-kbk62.
2,[pr-cp-reg-10022 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10022 cluster may be low for segment merges.
3,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-kube-prometheus-stack/pr-cp-reg-10007 cluster is .
2,[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_volume_out_of_disk_space - Volume pr-customer-env/data-volume-cloudsearch-0 is full (< 5% free space left) and should be increased.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container executor-svc in pod metering-svc.
1,[pr-cp-reg-10040 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10033 - aris-kube-prometheus-stack] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10044 - kube-public] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10039 - pr-customer-env-spark] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
2,[pr-cp-reg-10014 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10014 cluster.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10030 - pr-customer-env-spark] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10033 - falcon-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-cluster-autoscaler/exported_job failed to complete.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine-0 (processboard) is restarting  times / 5 minutes.
3,[pr-cp-reg-10008 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 8h to complete.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface nvme0 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10011 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10047 - aris-kube-prometheus-stack] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,"[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/enhancements will run full within 24 hours."
1,[pr-cp-reg-10031 - aris-kube-prometheus-stack] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-cluster-autoscaler/pr-cp-reg-10005 cluster is .
1,[pr-cp-reg-10028 - pr-customer-env-spark] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10011 - basic-application-bootstrapping] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10004 - aris-nginx-ingress] - CPUThrottlingHigh -  throttling of CPU in namespace aris-nginx-ingress for container controller in pod aris-nginx-ingress-ingress-nginx-controller.
3,"[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana will run full within 24 hours."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container sentinel in pod argocd-repo-server.
1,[pr-cp-reg-10039 - aris-cluster-autoscaler] - ARIS_postgresql_down - The postgres pod of namespace aris-cluster-autoscaler is down.
3,[pr-cp-reg-10003 - kube-public] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10008 - kube-system] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10034 - aris-nginx-ingress] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/kanister-job-7lbrs (processboard) is restarting  times / 5 minutes.
3,[pr-cp-reg-10034 - aris-spot-metrics] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - ARIS_argocd_app_progressing - The ArgoCD app 46d6ed101040cf8df2a51291aad961a76c082eba555ed2401548a98d1bb4d54d is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10033 - kube-public] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10017 - aris-sealed-secrets] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on 0 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10024 - aris-nginx-ingress] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10050 - aris-kube-prometheus-stack] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10031 - aris-cluster-autoscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod zookeeper.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app autoMigrateGraphPanels is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10042 - pr-customer-env-spark] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10023 - aris-kube-downscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface tmpfs has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter pr-customer-env/tm"
3,[pr-cp-reg-10041 - aris-kube-downscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10012 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10024 - aris-spot] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10019 - pr-customer-env] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10035 - aris-keda] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10038 - aris-kube-prometheus-stack] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10007 - aris-nginx-ingress] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4"
3,[pr-cp-reg-10018 - falcon-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10006 - aris-cluster-autoscaler] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10004 - kube-system] - ConfigReloaderSidecarErrors - Errors encountered while the coredns-cbbbbb9cb-q2xzz config-reloader sidecar attempts to sync config in kube-system namespace. As a result, configuration for service running in coredns-cbbbbb9cb-q2xzz may be stale and cannot be updated anymore."
3,[pr-cp-reg-10041 - aris-kube-prometheus-stack] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,"[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/data-volume-postgres-0 will run full within 24 hours."
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to opsgenie sent from any instance in the node-exporter cluster is .
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tm in pod loadbalancer-0.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on lo at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10011 - aris-kube-downscaler] - ARIS_k8s_pod_crash_looping - Pod aris-kube-downscaler/sealed-secrets-controller-cbbc8bd6b-qq22k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_unknown - The ArgoCD app aris-image-pull-secret is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10012 - pr-customer-env] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10048 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10002 - kube-system] - KubeNodeNotReady - ip-10-0-136-224.eu-central-1.compute.internal has been unready for more than 15 minutes.
3,[pr-cp-reg-10028 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10031 - falcon-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10025 - pr-customer-env] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10003 - kube-system] - KubeNodeReadinessFlapping - The readiness status of node ip-10-0-139-113.eu-central-1.compute.internal has changed  times in the last 15 minutes.
3,[pr-cp-reg-10034 - aris-kube-downscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-downscaler has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10039 - aris-cluster-autoscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10017 - aris-kube-downscaler] - ARIS_k8s_pod_restarted - The pod aris-kube-downscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10011 - kube-system] - KubeletServerCertificateRenewalErrors - Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal has failed to renew its server certificate ( errors in the last 5 minutes).
1,[pr-cp-reg-10030 - aris-keda] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on nvme0 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (create-es-index-template) is restarting  times / 5 minutes.
3,[pr-cp-reg-10007 - pr-customer-env] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in pr-customer-env/pr-cp-reg-10007 cluster is .
1,[pr-cp-reg-10019 - aris-sealed-secrets] - ARIS_postgresql_down - The postgres pod of namespace aris-sealed-secrets is down.
3,"[pr-cp-reg-10040 - aris-spot-metrics] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10049 - aris-keda] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10006 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-operator-78b758c475-qs69v has only found  members of the node-exporter cluster.
1,[pr-cp-reg-10027 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
2,[pr-cp-reg-10017 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10017 cluster may be low for segment merges.
4,[pr-cp-reg-10035 - aris-spot] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10021 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10043 - aris-keda] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container copyutil in pod ae-aw-euc1-15995-aws-load-balancer-controller.
3,[pr-cp-reg-10030 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10026 - pr-customer-env] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10017 - aris-kube-prometheus-stack] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container abs in pod kanister-job-7lbrs.
1,[pr-cp-reg-10018 - kube-system] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10006 - pr-customer-env] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container crypto-svc in pod executor-svc.
1,[pr-cp-reg-10016 - aris-spot-metrics] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10014 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10027 - falcon-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container tools in pod crypto-svc-67b9b7c65d-gg6z9.
1,[pr-cp-reg-10012 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10011 - pr-customer-env] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10011 cluster may be low for segment merges.
3,[pr-cp-reg-10006 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme5n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10006 - kube-node-lease] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
3,[pr-cp-reg-10017 - aris-nginx-ingress] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10014 - aris-kube-downscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10010 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_restarted - The pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10032 - kube-public] - CPUThrottlingHigh -  throttling of CPU in namespace kube-public for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-nginx-ingress/pr-cp-reg-10001 cluster is .
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/management-external-dns"
3,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-prometheus-stack has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10039 - kube-system] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10004 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot-metrics/pr-cp-reg-10004 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-prometheus-stack has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10037 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10046 - aris-nginx-ingress] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10012 - aris-kube-prometheus-stack] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10009 - kube-system] - KubeletTooManyPods - Kubelet 'ip-10-0-139-113.eu-central-1.compute.internal' is running at  of its Pod capacity.
4,[pr-cp-reg-10037 - kube-node-lease] - CPUThrottlingHigh -  throttling of CPU in namespace kube-node-lease for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10023 - pr-customer-env] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10037 - pr-customer-env-spark] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container executor-svc in pod frontend-svc.
3,[pr-cp-reg-10003 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10022 - aris-cluster-autoscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10048 - kasten-io] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10008 - kube-public] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/create-es-index-template-61d85-nz2dz (loadbalancer) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-application-controller in pod efs-csi-node-4bvqz.
3,[pr-cp-reg-10012 - aris-nginx-ingress] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10004 - pr-customer-env] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme4n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod collaboration.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""nvme0"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics"
4,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-sealed-secrets/exported_job failed to complete.
1,[pr-cp-reg-10003 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_oom_kill_detected - OOM kill detected.
4,[pr-cp-reg-10004 - aris-nginx-ingress] - CPUThrottlingHigh -  throttling of CPU in namespace aris-nginx-ingress for container kube-state-metrics in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
0,"[pr-cp-reg-10006 - aris-nginx-ingress] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10005 - aris-kube-prometheus-stack] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-147-93.eu-central-1.compute.internal in pr-customer-env-spark/pr-cp-reg-10043 cluster is .
3,[pr-cp-reg-10045 - aris-kube-downscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10002 - aris-cluster-autoscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-cluster-autoscaler for container aws-cluster-autoscaler in pod aris-cluster-autoscaler-aws-cluster-autoscaler.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine (kanister-sidecar) is restarting  times / 5 minutes.
1,[pr-cp-reg-10015 - pr-customer-env] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10010 - aris-nginx-ingress] - ARIS_k8s_pod_restarted - The pod aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10011 - falcon-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
4,[pr-cp-reg-10050 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env/exported_job failed to complete.
3,"[pr-cp-reg-10002 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the auth-svc-97bd685df-phk9x config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in auth-svc-97bd685df-phk9x may be stale and cannot be updated anymore."
3,[pr-cp-reg-10026 - kube-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10009 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/dashboarding-0 (serviceenabling) is restarting  times / 5 minutes.
3,[pr-cp-reg-10028 - pr-customer-env-spark] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10038 - pr-customer-env] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10045 - aris-spot-metrics] - CPUThrottlingHigh -  throttling of CPU in namespace aris-spot-metrics for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10011 - kube-system] - KubeQuotaExceeded - Namespace kube-system is using  of its memory quota.
3,[pr-cp-reg-10028 - aris-cluster-autoscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10005 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10017 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,"[pr-cp-reg-10005 - aris-keda] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-keda/sealed-secrets-controller-cbbc8bd6b-qq22k"
1,[pr-cp-reg-10034 - aris-keda] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container auth-svc in pod gateway-cc7bc5b8d-jqbkl.
3,[pr-cp-reg-10009 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10034 - pr-customer-env] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
2,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10043 cluster may be low for segment merges.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod serviceenabling.
3,[pr-cp-reg-10046 - aris-keda] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10039 - aris-keda] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
0,"[pr-cp-reg-10039 - pr-customer-env-spark] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10012 - aris-cluster-autoscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10007 - aris-keda] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container events-svc in pod auth-svc-97bd685df-phk9x.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tm in pod kanister-job-6kclt.
3,"[pr-cp-reg-10007 - aris-cluster-autoscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-cluster-autoscaler-aws-cluster-autoscaler config-reloader sidecar attempts to sync config in aris-cluster-autoscaler namespace. As a result, configuration for service running in aris-cluster-autoscaler-aws-cluster-autoscaler may be stale and cannot be updated anymore."
1,[pr-cp-reg-10013 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10034 - kube-node-lease] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_k8s_pod_crash_looping - Pod aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-hwdv6 (kube-state-metrics) is restarting  times / 5 minutes.
2,[pr-cp-reg-10026 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10026 cluster may be low for segment merges.
1,[pr-cp-reg-10048 - aris-nginx-ingress] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10007 - pr-customer-env-spark] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10004 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10014 - falcon-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10029 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot-metrics/pr-cp-reg-10029 health status has been YELLOW for at least 20 minutes.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""0"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4"
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app aris-grafana-admin-creds is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container octopus in pod tm-0.
3,[pr-cp-reg-10019 - aris-nginx-ingress] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
0,"[pr-cp-reg-10026 - aris-spot] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10014 - aris-keda] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10050 - aris-kube-prometheus-stack] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana will run full within 24 hours."
1,[pr-cp-reg-10021 - falcon-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,"[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10003 - pr-customer-env] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod elasticsearch-default-0.
4,[pr-cp-reg-10043 - kube-public] - CPUThrottlingHigh -  throttling of CPU in namespace kube-public for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/tm (portalserver) is restarting  times / 5 minutes.
3,[pr-cp-reg-10017 - kube-system] - KubeletClientCertificateRenewalErrors - Kubelet on node ip-10-0-138-163.eu-central-1.compute.internal has failed to renew its client certificate ( errors in the last 5 minutes).
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_restarted - The pod pr-customer-env/elasticsearch-default-0 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10008 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10032 - aris-cluster-autoscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-cluster-autoscaler has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10030 - aris-sealed-secrets] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,"[pr-cp-reg-10002 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the aggregatedapis-svc config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in aggregatedapis-svc may be stale and cannot be updated anymore."
2,[pr-cp-reg-10022 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10022 cluster.
3,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
1,[pr-cp-reg-10031 - kube-public] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fv6q4 failed to send  of notifications to pushover.
3,[pr-cp-reg-10047 - aris-sealed-secrets] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_high - Latency to the upstream service aris-kube-prometheus-stack/argocd-server has been high (more than 10 seconds) for the last 3 minutes. Service might become unresponsive shortly.
3,"[pr-cp-reg-10010 - pr-customer-env-spark] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg"
3,[pr-cp-reg-10035 - aris-nginx-ingress] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10009 - pr-customer-env] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10048 - aris-sealed-secrets] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10039 - aris-keda] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-keda/pr-cp-reg-10039 cluster is .
3,[pr-cp-reg-10014 - pr-customer-env-spark] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10015 - kube-public] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,[pr-cp-reg-10048 - aris-kube-downscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10023 - kube-node-lease] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10039 - aris-kube-downscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
2,[pr-cp-reg-10017 - aris-spot] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10017 cluster.
1,[pr-cp-reg-10042 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10001 - kube-public] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container ambassador in pod logging-svc.
3,[pr-cp-reg-10047 - pr-customer-env-spark] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10016 - aris-sealed-secrets] - ARIS_argocd_app_progressing - The ArgoCD app 46d6ed101040cf8df2a51291aad961a76c082eba555ed2401548a98d1bb4d54d is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10011 - pr-customer-env] - ARIS_postgresql_too_many_connections - The postgres instance of namespace pr-customer-env has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10029 - aris-cluster-autoscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
2,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10020 cluster.
3,[pr-cp-reg-10001 - management] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10005 - management] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10004 - pr-customer-env-spark] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10040 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10008 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10028 - aris-sealed-secrets] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10001 - aris-spot] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10001 cluster.
2,[pr-cp-reg-10011 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10011 cluster.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array 'nvme0' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container cert-manager in pod ebs-csi-controller.
3,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10032 - aris-nginx-ingress] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10008 - kube-public] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10046 - aris-spot-metrics] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot-metrics has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10042 - aris-kube-downscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10013 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10037 - aris-keda] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10039 - aris-nginx-ingress] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter management/argocd-redis-ha-server-2"
3,[pr-cp-reg-10006 - pr-customer-env] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10043 - management] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_takes_too_long - CronJob pr-customer-env/backup-tenants is taking more than 1h to complete.
3,[pr-cp-reg-10029 - aris-cluster-autoscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10039 - pr-customer-env-spark] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10001 - aris-sealed-secrets] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10023 - pr-customer-env] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10003 - aris-cluster-autoscaler] - CPUThrottlingHigh -  throttling of CPU in namespace aris-cluster-autoscaler for container controller in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
4,[pr-cp-reg-10003 - aris-spot-metrics] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot-metrics has faced > 5 deadlocks in last minute.
2,[pr-cp-reg-10009 - aris-cluster-autoscaler] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10009 cluster.
1,[pr-cp-reg-10005 - pr-customer-env] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver-0 (serviceenabling) is restarting  times / 5 minutes.
3,[pr-cp-reg-10015 - pr-customer-env] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in pr-customer-env/pr-cp-reg-10015 cluster is .
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter management/ebs-csi-controller-86cb997fdc-bs5bs"
1,[pr-cp-reg-10046 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10046 cluster.
3,[pr-cp-reg-10014 - kube-public] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
4,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-sealed-secrets has faced > 5 deadlocks in last minute.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container copyutil in pod ebs-csi-controller-86cb997fdc-t2z6p.
3,[pr-cp-reg-10025 - aris-nginx-ingress] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10012 - aris-kube-downscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10045 - aris-kube-downscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10016 - aris-kube-prometheus-stack] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-application-controller in pod cert-manager-cainjector-7d8985bbc8-x7www.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container efs-plugin in pod efs-csi-controller-94d8968c6-6ts4k.
3,"[pr-cp-reg-10003 - kube-system] - ConfigReloaderSidecarErrors - Errors encountered while the metrics-server config-reloader sidecar attempts to sync config in kube-system namespace. As a result, configuration for service running in metrics-server may be stale and cannot be updated anymore."
3,[pr-cp-reg-10042 - pr-customer-env-spark] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10015 - pr-customer-env-spark] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-server in pod management-ingress-delay-nbwqf.
1,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-138-163.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10006 - falcon-system] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10003 - kube-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kube-system/coredns"
3,[pr-cp-reg-10012 - pr-customer-env] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10044 - pr-customer-env-spark] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10042 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10033 - aris-cluster-autoscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedReload - Configuration has failed to load for aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5.
3,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-cluster-autoscaler has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10011 - aris-sealed-secrets] - ARIS_k8s_pod_restarted - The pod aris-sealed-secrets/sealed-secrets-controller has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10002 - aris-kube-downscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10043 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10016 - aris-sealed-secrets] - ARIS_k8s_node_out_of_disk - ip-10-0-136-224.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10031 - kube-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface nvme3 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver (loadbalancer) is restarting  times / 5 minutes.
3,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10007 - kasten-io] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme2n1' needs attention and possibly a disk swap.
3,"[pr-cp-reg-10002 - pr-customer-env-spark] - ConfigReloaderSidecarErrors - Errors encountered while the pr-customer-env-spark-spark-operator-74d54645cb-5gpbl config-reloader sidecar attempts to sync config in pr-customer-env-spark namespace. As a result, configuration for service running in pr-customer-env-spark-spark-operator-74d54645cb-5gpbl may be stale and cannot be updated anymore."
3,[pr-cp-reg-10048 - aris-nginx-ingress] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container coredns in pod kube-proxy-5lqbj.
3,"[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-operator-78b758c475-qs69v config-reloader sidecar attempts to sync config in aris-kube-prometheus-stack namespace. As a result, configuration for service running in aris-kube-prometheus-stack-operator-78b758c475-qs69v may be stale and cannot be updated anymore."
0,"[pr-cp-reg-10003 - aris-kube-prometheus-stack] - Watchdog - This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the ""DeadMansSnitch"" integration in PagerDuty."
3,[pr-cp-reg-10021 - aris-sealed-secrets] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10050 - aris-keda] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10041 cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container get-zone in pod cdf-0.
3,[pr-cp-reg-10037 - aris-keda] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10020 - aris-sealed-secrets] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-cluster-autoscaler has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10025 - kube-public] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10007 - aris-spot] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10036 - aris-kube-downscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10022 - aris-kube-prometheus-stack] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-prometheus-stack/exported_job failed to complete.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container haproxy in pod ae-aw-euc1-15995-aws-load-balancer-controller.
1,[pr-cp-reg-10042 - aris-cluster-autoscaler] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10010 - aris-keda] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-sealed-secrets/pr-cp-reg-10020 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10026 - aris-keda] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10016 - aris-keda] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10028 - aris-keda] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerConfigInconsistent - Alertmanager instances within the node-exporter cluster have different configurations.
1,[pr-cp-reg-10013 - pr-customer-env] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10008 - kube-public] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces-0 (container) is restarting  times / 5 minutes.
3,[pr-cp-reg-10026 - pr-customer-env] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10041 - aris-sealed-secrets] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_k8s_node_out_of_disk - ip-10-0-147-93.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container aggregatedapis-svc in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
1,[pr-cp-reg-10004 - pr-customer-env] - NodeRAIDDegraded - RAID array '/dev/nvme4n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10023 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-prometheus-stack/pr-cp-reg-10023 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10030 - aris-keda] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-keda has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10035 - aris-keda] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod cdf-0.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces (processengine) is restarting  times / 5 minutes.
1,[pr-cp-reg-10007 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10001 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-1 (node-exporter) is restarting  times / 5 minutes.
3,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-nginx-ingress/pr-cp-reg-10025 cluster is .
3,[pr-cp-reg-10011 - aris-sealed-secrets] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10011 - aris-keda] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod kanister-job-6kclt.
3,[pr-cp-reg-10039 - falcon-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/register-smtp-server-6d78c-stqpc (controller) is restarting  times / 5 minutes.
1,[pr-cp-reg-10030 - aris-keda] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10045 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28125915-9ps25 (get-zone) is restarting  times / 5 minutes.
3,[pr-cp-reg-10013 - aris-sealed-secrets] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10004 - aris-keda] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on eni0039e8a4ad3 at <IP_ADDRESS>:<PORT> has only % available inodes left.
0,"[pr-cp-reg-10020 - management] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10003 - kube-system] - KubeQuotaExceeded - Namespace kube-system is using  of its cpu quota.
3,[pr-cp-reg-10015 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
2,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10001 cluster.
3,[pr-cp-reg-10012 - pr-customer-env] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10019 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10029 - aris-spot-metrics] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-operator-78b758c475-qs69v"
3,"[pr-cp-reg-10003 - aris-nginx-ingress] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-nginx-ingress/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container jobs-svc in pod gateway-cc7bc5b8d-jqbkl.
1,[pr-cp-reg-10045 - aris-cluster-autoscaler] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
2,[pr-cp-reg-10015 - pr-customer-env] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-136-224.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10015 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/sealed-secrets-controller-cbbc8bd6b-qq22k (adsadmin) is restarting  times / 5 minutes.
1,[pr-cp-reg-10044 - aris-nginx-ingress] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10016 - aris-keda] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10041 - aris-nginx-ingress] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-nginx-ingress/exported_job failed to complete.
3,[pr-cp-reg-10033 - aris-kube-prometheus-stack] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_filesystem_out_of_files - Filesystem on nvme1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10039 - aris-keda] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10032 - kasten-io] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10018 - aris-sealed-secrets] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod argocd-application-controller-0.
3,[pr-cp-reg-10028 - aris-cluster-autoscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env/backup-logs-28124400 failed to complete.
3,[pr-cp-reg-10040 - kasten-io] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-driver-registrar in pod argocd-redis-6645d4fb89-kpv95.
1,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-sealed-secrets/pr-cp-reg-10007 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin (prometheus-exporter) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container container in pod elasticsearch-default-0.
3,[pr-cp-reg-10022 - aris-kube-downscaler] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container config-init in pod argocd-redis-ha-haproxy-84b857bc4b-gksr6.
3,"[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/adsdata will run full within 24 hours."
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
4,[pr-cp-reg-10015 - kube-system] - KubeletTooManyPods - Kubelet 'ip-10-0-136-224.eu-central-1.compute.internal' is running at  of its Pod capacity.
3,[pr-cp-reg-10017 - kube-system] - KubeletPlegDurationHigh - The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of  seconds on node ip-10-0-136-224.eu-central-1.compute.internal.
3,[pr-cp-reg-10035 - aris-keda] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10016 - aris-cluster-autoscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10032 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10002 - aris-sealed-secrets] - CPUThrottlingHigh -  throttling of CPU in namespace aris-sealed-secrets for container kube-state-metrics in pod sealed-secrets-controller.
3,[pr-cp-reg-10011 - aris-keda] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10017 - aris-spot] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10017 cluster.
3,[pr-cp-reg-10029 - aris-nginx-ingress] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10018 - aris-keda] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
2,[pr-cp-reg-10034 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10034 cluster.
3,[pr-cp-reg-10034 - falcon-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10011 - kube-node-lease] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
3,[pr-cp-reg-10018 - aris-cluster-autoscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10010 - pr-customer-env] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,[pr-cp-reg-10001 - aris-keda] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10004 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-downscaler/pr-cp-reg-10004 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10027 - aris-sealed-secrets] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10042 - basic-application-bootstrapping] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - ARIS_host_filesystem_almost_out_of_files - Filesystem on lo at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10012 - aris-keda] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10006 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env/backup-tenants-28123035 failed to complete.
3,[pr-cp-reg-10023 - aris-kube-downscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
2,[pr-cp-reg-10017 - pr-customer-env] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10017 cluster.
1,[pr-cp-reg-10041 - aris-spot-metrics] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10041 - aris-nginx-ingress] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10020 - pr-customer-env-spark] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10007 - aris-keda] - ARIS_k8s_pod_restarted - The pod aris-keda/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10024 - pr-customer-env] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10038 - pr-customer-env-spark] - ARIS_k8s_node_out_of_disk - ip-10-0-147-93.eu-central-1.compute.internal has OutOfDisk condition.
0,"[pr-cp-reg-10011 - aris-nginx-ingress] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_k8s_node_out_of_disk - ip-10-0-143-220.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10032 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10050 - aris-keda] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10030 - aris-sealed-secrets] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10018 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10046 - aris-nginx-ingress] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10035 - aris-spot] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10003 - management] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
4,[pr-cp-reg-10039 - aris-kube-prometheus-stack] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-prometheus-stack/exported_job failed to complete.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on nvme2 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10016 - aris-nginx-ingress] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
3,"[pr-cp-reg-10008 - aris-keda] - ConfigReloaderSidecarErrors - Errors encountered while the keda-operator-5867cd94d9-vhxdf config-reloader sidecar attempts to sync config in aris-keda namespace. As a result, configuration for service running in keda-operator-5867cd94d9-vhxdf may be stale and cannot be updated anymore."
3,[pr-cp-reg-10006 - aris-keda] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
2,[pr-cp-reg-10022 - aris-spot] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10022 cluster.
3,[pr-cp-reg-10013 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10045 - pr-customer-env-spark] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k (prometheus) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-resizer in pod argocd-redis-ha-server-2.
1,[pr-cp-reg-10020 - aris-nginx-ingress] - ARIS_k8s_node_memory_pressure - ip-10-0-143-220.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10025 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10003 - aris-nginx-ingress] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10032 - aris-keda] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10030 - aris-spot-metrics] - ARIS_postgresql_down - The postgres pod of namespace aris-spot-metrics is down.
1,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env/backup-logs-28124160 failed to complete.
3,[pr-cp-reg-10008 - kube-system] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tm in pod simulation-0.
3,[pr-cp-reg-10011 - aris-kube-downscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10022 - aris-keda] - ARIS_postgresql_down - The postgres pod of namespace aris-keda is down.
1,[pr-cp-reg-10017 - aris-nginx-ingress] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-143-220.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10009 - aris-kube-prometheus-stack] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10005 - aris-kube-downscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10006 - kube-system] - KubeClientErrors - Kubernetes API server client 'kubelet/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,"[pr-cp-reg-10009 - falcon-system] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in falcon-system namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration (abs) is restarting  times / 5 minutes.
1,[pr-cp-reg-10007 - kube-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10044 - aris-spot] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
0,"[pr-cp-reg-10013 - default] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10044 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10009 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-downscaler/pr-cp-reg-10009 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10006 - falcon-system] - CPUThrottlingHigh -  throttling of CPU in namespace falcon-system for container falcon-node-sensor in pod falcon-falcon-sensor-2bjwf.
3,[pr-cp-reg-10020 - pr-customer-env-spark] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10019 - aris-nginx-ingress] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10015 - aris-keda] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
1,[pr-cp-reg-10043 - aris-sealed-secrets] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
0,"[pr-cp-reg-10009 - default] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124160-s7ps9 (umcadmin) is restarting  times / 5 minutes.
1,[pr-cp-reg-10002 - kasten-io] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app data-volume-elasticsearch-default-1 is out of sync for longer than 10 minutes and requires manual intervention.
2,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10015 cluster.
3,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10003 - kube-system] - KubeAggregatedAPIErrors - Kubernetes aggregated API cache/kube-system has reported errors. It has appeared unavailable  times averaged over the past 10m.
1,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10005 cluster.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-kube-state-metrics (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10043 - kasten-io] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana-sc-datasources in pod alertmanager-aris-kube-prometheus-stack-alertmanager-1.
3,[pr-cp-reg-10021 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10026 - kube-node-lease] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10049 - pr-customer-env-spark] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container dashboardbff-svc in pod logging-svc.
3,[pr-cp-reg-10039 - pr-customer-env-spark] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env-spark/pr-cp-reg-10034 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_k8s_pod_restarted - The pod aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/adsadmin (ces) is restarting  times / 5 minutes.
4,[pr-cp-reg-10045 - pr-customer-env-spark] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env-spark/exported_job failed to complete.
0,"[pr-cp-reg-10043 - aris-kube-prometheus-stack] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10006 - aris-spot] - ConfigReloaderSidecarErrors - Errors encountered while the spot-ocean-metric-exporter-56dfbdbbcb-82vqg config-reloader sidecar attempts to sync config in aris-spot namespace. As a result, configuration for service running in spot-ocean-metric-exporter-56dfbdbbcb-82vqg may be stale and cannot be updated anymore."
3,[pr-cp-reg-10027 - pr-customer-env] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10042 - kube-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10020 - pr-customer-env-spark] - ARIS_k8s_node_out_of_disk - ip-10-0-147-93.eu-central-1.compute.internal has OutOfDisk condition.
3,[pr-cp-reg-10032 - aris-cluster-autoscaler] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cloudsearch (portalserver) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod adsadmin.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cdf (create-es-index-template) is restarting  times / 5 minutes.
3,[pr-cp-reg-10046 - aris-kube-downscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerFailedReload - Configuration has failed to load for aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-1.
3,[pr-cp-reg-10024 - aris-sealed-secrets] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container admintools in pod cloudsearch-0.
3,[pr-cp-reg-10011 - kube-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10024 - aris-spot] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10007 - aris-keda] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-keda/exported_job failed to complete.
3,[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10027 - pr-customer-env] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10023 - kube-public] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10031 - aris-kube-downscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10041 - pr-customer-env] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env is down.
3,[pr-cp-reg-10011 - pr-customer-env] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-operator-78b758c475-qs69v failed to send  of notifications to webhook.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_failed - Job pr-customer-env/create-es-index-template-61d85 failed to complete.
1,[pr-cp-reg-10049 - kube-public] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10045 - aris-keda] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer-0 (ces) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod umcadmin-0.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""eni006a31b57f1"" changing its up status often on node-exporter aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana"
1,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-cluster-autoscaler/pr-cp-reg-10047 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10043 - management] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10033 - aris-keda] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10045 - aris-kube-prometheus-stack] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10029 - kube-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10015 - pr-customer-env] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-138-163.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,"[pr-cp-reg-10005 - pr-customer-env] - ARIS_k8s_volume_full_in_24hours - Based on the last 6 hours on usage, it is expected that volume pr-customer-env/enhancements will run full within 24 hours."
2,[pr-cp-reg-10016 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10016 cluster.
3,[pr-cp-reg-10016 - aris-kube-downscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-nginx-ingress-ingress-nginx-leader is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10003 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeRAIDDegraded - RAID array 'shm' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container adsadmin in pod create-es-index-template-61d85-nz2dz.
3,[pr-cp-reg-10005 - pr-customer-env] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10041 - kube-public] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerConfigInconsistent - Alertmanager instances within the aris-kube-prometheus-stack-grafana cluster have different configurations.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container config-reloader in pod aris-kube-prometheus-stack-prometheus-node-exporter-822lp.
1,[pr-cp-reg-10028 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10001 - aris-spot-metrics] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10017 - aris-keda] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10006 - pr-customer-env] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10024 - pr-customer-env-spark] - ARIS_argocd_app_out_of_sync - The ArgoCD app ac719dfadff867de0c53369ea1b179225783dc233b1a1a432cf4105cd0958977 is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10029 - aris-nginx-ingress] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/serviceenabling (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10012 - aris-keda] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10012 - aris-sealed-secrets] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10001 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot-metrics/pr-cp-reg-10001 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres (container) is restarting  times / 5 minutes.
1,[pr-cp-reg-10007 - aris-keda] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10007 cluster.
1,[pr-cp-reg-10037 - kube-public] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10029 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-prometheus-stack/pr-cp-reg-10029 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10021 - aris-sealed-secrets] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-sealed-secrets/pr-cp-reg-10021 health status has been YELLOW for at least 20 minutes.
3,[pr-cp-reg-10016 - aris-cluster-autoscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
3,[pr-cp-reg-10003 - management] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme7n1 has encountered  receive errors in the last two minutes.
3,"[pr-cp-reg-10014 - kube-node-lease] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-node-lease namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container adsadmin in pod loadbalancer-0.
3,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_argocd_app_out_of_sync - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is out of sync for longer than 10 minutes and requires manual intervention.
2,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10047 cluster.
4,[pr-cp-reg-10011 - kube-system] - KubeQuotaAlmostFull - Namespace kube-system is using  of its cpu quota.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-operator-78b758c475-qs69v has only found  members of the kube-state-metrics cluster.
3,[pr-cp-reg-10023 - aris-nginx-ingress] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10010 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
0,"[pr-cp-reg-10028 - aris-keda] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
2,[pr-cp-reg-10002 - aris-spot] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10002 cluster.
1,[pr-cp-reg-10006 - kube-public] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-143-220.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_k8s_pod_restarted - The pod aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10029 - pr-customer-env-spark] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10025 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10026 - aris-nginx-ingress] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (octopus) is restarting  times / 5 minutes.
3,[pr-cp-reg-10005 - aris-keda] - ARIS_argocd_app_out_of_sync - The ArgoCD app 23122faab4957433d2ae942409d7bd757433bb06c6ef58aabc0599bb6382b619 is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10039 - aris-kube-downscaler] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10046 - aris-keda] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10018 - aris-kube-downscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10017 - aris-keda] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container efs-plugin in pod argocd-server-74fbd754cb-n2sgr.
3,[pr-cp-reg-10015 - aris-cluster-autoscaler] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10045 - aris-nginx-ingress] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tm in pod collaboration.
3,[pr-cp-reg-10020 - aris-cluster-autoscaler] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10027 - kube-public] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10012 - pr-customer-env-spark] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,"[pr-cp-reg-10008 - aris-spot] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-spot/spotinst-kubernetes-cluster-controller"
3,[pr-cp-reg-10027 - pr-customer-env-spark] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10001 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10050 - aris-keda] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10016 - pr-customer-env-spark] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init failed to complete.
3,[pr-cp-reg-10009 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10048 - aris-spot] - ARIS_postgresql_down - The postgres pod of namespace aris-spot is down.
3,[pr-cp-reg-10013 - kube-system] - KubeletPlegDurationHigh - The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of  seconds on node ip-10-0-136-224.eu-central-1.compute.internal.
3,[pr-cp-reg-10004 - aris-keda] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
0,"[pr-cp-reg-10009 - aris-kube-prometheus-stack] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10016 - pr-customer-env-spark] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10041 - pr-customer-env] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
1,[pr-cp-reg-10036 - kube-public] - KubeAPIErrorBudgetBurn - The API server is burning too much error budget.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod tm.
3,[pr-cp-reg-10006 - falcon-system] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme4n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10025 - pr-customer-env-spark] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10012 - aris-keda] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10028 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env/exported_job failed to complete.
3,[pr-cp-reg-10029 - basic-application-bootstrapping] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface nvme3 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10045 - kube-node-lease] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
1,[pr-cp-reg-10039 - aris-cluster-autoscaler] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10009 - pr-customer-env] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10008 - pr-customer-env] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10003 - pr-customer-env-spark] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10008 - aris-keda] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
1,[pr-cp-reg-10011 - kube-public] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod ces.
4,[pr-cp-reg-10017 - kube-system] - KubeQuotaFullyUsed - Namespace kube-system is using  of its cpu quota.
1,[pr-cp-reg-10046 - aris-keda] - ARIS_k8s_node_out_of_disk - ip-10-0-139-113.eu-central-1.compute.internal has OutOfDisk condition.
1,[pr-cp-reg-10004 - aris-keda] - ARIS_k8s_pod_restarted - The pod aris-keda/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter failed to send  of notifications to webhook.
1,[pr-cp-reg-10007 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
2,[pr-cp-reg-10008 - pr-customer-env] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10008 cluster may be low for segment merges.
3,"[pr-cp-reg-10002 - pr-customer-env-spark] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in pr-customer-env-spark namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager failed to send  of notifications to telegram.
1,[pr-cp-reg-10003 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme6n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10039 - falcon-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10007 - falcon-system] - CPUThrottlingHigh -  throttling of CPU in namespace falcon-system for container falcon-node-sensor in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration (octopus) is restarting  times / 5 minutes.
3,[pr-cp-reg-10009 - aris-spot] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10005 - kasten-io] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on shm at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10023 - default] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10023 - aris-keda] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container prometheus-exporter in pod cdf.
1,[pr-cp-reg-10007 - aris-nginx-ingress] - ARIS_k8s_pod_restarted - The pod aris-nginx-ingress/aris-nginx-ingress-ingress-nginx-controller-5f46b79bc7-7j2x4 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tm in pod create-es-index-template-61d85-nz2dz.
1,[pr-cp-reg-10031 - kube-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-nginx-ingress-ingress-nginx-leader is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10031 - basic-application-bootstrapping] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container ambassador in pod aggregatedapis-svc-5c7ccfdd78-slnk8.
1,[pr-cp-reg-10041 - aris-kube-prometheus-stack] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10014 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10043 - pr-customer-env-spark] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
2,[pr-cp-reg-10037 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10037 cluster may be low for segment merges.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container auth-svc in pod controllermanager-svc-788dc96c8d-rg47t.
3,[pr-cp-reg-10007 - kube-system] - KubeletClientCertificateRenewalErrors - Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal has failed to renew its client certificate ( errors in the last 5 minutes).
1,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container settcpkeepalivetime in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
3,[pr-cp-reg-10039 - aris-kube-prometheus-stack] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10005 - management] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme1n1' needs attention and possibly a disk swap.
4,[pr-cp-reg-10002 - kube-system] - KubeQuotaAlmostFull - Namespace kube-system is using  of its memory quota.
3,[pr-cp-reg-10011 - aris-spot-metrics] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10023 - aris-keda] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10027 - aris-keda] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10036 - aris-nginx-ingress] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10022 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10025 - basic-application-bootstrapping] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
2,[pr-cp-reg-10017 - aris-cluster-autoscaler] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10017 cluster may be low for segment merges.
1,[pr-cp-reg-10039 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10001 - kasten-io] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme2n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10001 - kube-system] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10049 - aris-cluster-autoscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10041 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10027 - aris-nginx-ingress] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-nginx-ingress has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10016 - pr-customer-env] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10026 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container setmaxmapcount in pod sealed-secrets-controller-cbbc8bd6b-qq22k.
1,[pr-cp-reg-10044 - aris-spot] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-spot/pr-cp-reg-10044 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10009 - aris-keda] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10002 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API kubernetes/kube-system has been only % available over the last 10m.
3,[pr-cp-reg-10004 - pr-customer-env-spark] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array 'nvme3' needs attention and possibly a disk swap.
1,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_k8s_pod_restarted - The pod aris-cluster-autoscaler/sealed-secrets-controller-cbbc8bd6b-qq22k has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
4,[pr-cp-reg-10026 - aris-kube-prometheus-stack] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-prometheus-stack/exported_job failed to complete.
3,[pr-cp-reg-10040 - aris-spot] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10008 - aris-spot-metrics] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10017 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10024 - aris-kube-prometheus-stack] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
0,"[pr-cp-reg-10043 - aris-keda] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10002 - kasten-io] - ConfigReloaderSidecarErrors - Errors encountered while the aggregatedapis-svc-5c7ccfdd78-slnk8 config-reloader sidecar attempts to sync config in kasten-io namespace. As a result, configuration for service running in aggregatedapis-svc-5c7ccfdd78-slnk8 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
4,[pr-cp-reg-10012 - aris-nginx-ingress] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-nginx-ingress has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10035 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10017 - kasten-io] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
0,"[pr-cp-reg-10003 - aris-cluster-autoscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10042 - aris-keda] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-keda/pr-cp-reg-10042 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10042 - falcon-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10047 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10009 - aris-keda] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10004 - pr-customer-env] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10050 - falcon-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10047 - aris-cluster-autoscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-cluster-autoscaler has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10011 - aris-keda] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10017 - aris-nginx-ingress] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10026 - aris-keda] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-keda has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10016 - aris-cluster-autoscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10031 - aris-sealed-secrets] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/create-es-index-template-61d85-nz2dz (serviceenabling) is restarting  times / 5 minutes.
1,[pr-cp-reg-10015 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,"[pr-cp-reg-10001 - kube-system] - ConfigReloaderSidecarErrors - Errors encountered while the kube-proxy-5lqbj config-reloader sidecar attempts to sync config in kube-system namespace. As a result, configuration for service running in kube-proxy-5lqbj may be stale and cannot be updated anymore."
2,[pr-cp-reg-10019 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-143-220.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10019 cluster.
3,"[pr-cp-reg-10004 - aris-kube-downscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-kube-downscaler namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
0,"[pr-cp-reg-10001 - aris-nginx-ingress] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10013 - kasten-io] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available space left.
2,[pr-cp-reg-10031 - pr-customer-env-spark] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10031 cluster may be low for segment merges.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/octopus (elasticsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10015 - aris-kube-prometheus-stack] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-138-163.eu-central-1.compute.internal in aris-kube-prometheus-stack/pr-cp-reg-10015 cluster is .
3,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10047 - aris-sealed-secrets] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10019 - kube-node-lease] - CPUThrottlingHigh -  throttling of CPU in namespace kube-node-lease for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10020 - kube-public] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
1,[pr-cp-reg-10034 - aris-nginx-ingress] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
4,[pr-cp-reg-10002 - aris-keda] - CPUThrottlingHigh -  throttling of CPU in namespace aris-keda for container kube-state-metrics in pod keda-operator-metrics-apiserver.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeFilesystemSpaceFillingUp - Filesystem on eth0 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10005 - kasten-io] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme2n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_k8s_pod_restarted - The pod aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-0 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
2,[pr-cp-reg-10005 - aris-keda] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10005 cluster may be low for segment merges.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app canvasPanelNesting is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10045 - default] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10011 - kube-system] - KubeletPlegDurationHigh - The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of  seconds on node ip-10-0-138-163.eu-central-1.compute.internal.
3,[pr-cp-reg-10046 - pr-customer-env] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""nvme2"" changing its up status often on node-exporter aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus"
1,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - ARIS_postgresql_down - The postgres pod of namespace aris-kube-prometheus-stack is down.
1,[pr-cp-reg-10001 - default] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/adsadmin-0 (processboard) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10016 - pr-customer-env] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10033 - pr-customer-env] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env/pr-cp-reg-10033 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10008 - aris-keda] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_restarted - The pod pr-customer-env/processboard-0 has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10016 - kasten-io] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10014 - aris-nginx-ingress] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10003 - aris-spot] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
1,[pr-cp-reg-10049 - aris-keda] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - aris-cluster-autoscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10043 - pr-customer-env] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/argocd-server-74fbd754cb-n2sgr"
3,[pr-cp-reg-10025 - aris-spot-metrics] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,"[pr-cp-reg-10011 - aris-cluster-autoscaler] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-cluster-autoscaler namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10004 - pr-customer-env-spark] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
0,"[pr-cp-reg-10010 - falcon-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10009 - aris-keda] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-keda has faced > 5 deadlocks in last minute.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container cert-manager in pod cert-manager-webhook.
3,[pr-cp-reg-10016 - aris-keda] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod efs-csi-controller-94d8968c6-6ts4k.
3,[pr-cp-reg-10004 - pr-customer-env] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container kanister-sidecar in pod logging-svc-58b457d7d8-pfqmx.
3,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10010 - falcon-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10010 - pr-customer-env-spark] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10036 - aris-cluster-autoscaler] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10019 - kube-node-lease] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10001 - management] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container node-driver-registrar in pod cert-manager-webhook-58765b986c-wxht4.
1,[pr-cp-reg-10014 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10014 cluster.
3,[pr-cp-reg-10019 - aris-sealed-secrets] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10043 - pr-customer-env] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
2,[pr-cp-reg-10035 - aris-keda] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10035 cluster may be low for segment merges.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus-0 failed to send  of notifications to wechat.
3,[pr-cp-reg-10045 - pr-customer-env] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10001 - aris-keda] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10021 - aris-cluster-autoscaler] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10026 - aris-kube-prometheus-stack] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10044 - pr-customer-env-spark] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10048 - aris-sealed-secrets] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme5n1"" changing its up status often on node-exporter management/management-ingress-delay-nbwqf"
1,[pr-cp-reg-10049 - management] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container kanister-sidecar in pod zookeeper-0.
2,[pr-cp-reg-10027 - aris-kube-prometheus-stack] - ARIS_k8s_volume_out_of_disk_space - Volume aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana is full (< 5% free space left) and should be increased.
3,[pr-cp-reg-10024 - aris-keda] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10045 - aris-nginx-ingress] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10017 - aris-cluster-autoscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10004 - aris-sealed-secrets] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10049 - management] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
0,"[pr-cp-reg-10008 - kube-node-lease] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod argocd-redis-ha-haproxy-84b857bc4b-8lfwq.
4,[pr-cp-reg-10044 - aris-spot] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot has faced > 5 deadlocks in last minute.
3,"[pr-cp-reg-10044 - aris-sealed-secrets] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_argocd_app_unknown - The ArgoCD app aris-secret-smtp is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10038 - pr-customer-env] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env/pr-cp-reg-10038 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container ambassador in pod state-svc-745f7ffd6d-jfrkr.
3,[pr-cp-reg-10043 - aris-kube-downscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/serviceenabling-0 (get-zone) is restarting  times / 5 minutes.
0,"[pr-cp-reg-10050 - kube-node-lease] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container container in pod processboard-0.
1,[pr-cp-reg-10047 - falcon-system] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124160-s7ps9 (kube-state-metrics) is restarting  times / 5 minutes.
2,[pr-cp-reg-10010 - aris-keda] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10010 cluster.
1,[pr-cp-reg-10025 - aris-spot-metrics] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10028 - pr-customer-env] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10009 - aris-cluster-autoscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10026 - kube-node-lease] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
1,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10035 - pr-customer-env-spark] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10028 - aris-sealed-secrets] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10033 - aris-kube-prometheus-stack] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-prometheus-stack has too many active connections. More than 90% of available connections are used.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processboard-0 (loadbalancer) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container schema-upgrade-check in pod kanister-svc-9559f74-79zgn.
3,[pr-cp-reg-10032 - kube-public] - KubeClientErrors - Kubernetes API server client 'kube-state-metrics/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10029 - pr-customer-env-spark] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
2,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10002 cluster may be low for segment merges.
3,[pr-cp-reg-10015 - kube-system] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container aws-load-balancer-controller in pod argocd-redis-ha-server-2.
4,[pr-cp-reg-10041 - aris-kube-prometheus-stack] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-prometheus-stack has faced > 5 deadlocks in last minute.
3,"[pr-cp-reg-10014 - aris-spot-metrics] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10027 - kube-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container kube-state-metrics in pod aris-management-ingress-delay-ntdn6.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124280-qwm2q (portalserver) is restarting  times / 5 minutes.
1,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10006 cluster.
3,[pr-cp-reg-10019 - pr-customer-env] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10043 - aris-kube-prometheus-stack] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  receive errors in the last two minutes.
1,[pr-cp-reg-10005 - aris-sealed-secrets] - ARIS_k8s_node_memory_pressure - ip-10-0-136-224.eu-central-1.compute.internal has MemoryPressure condition.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme7n1"" changing its up status often on node-exporter management/argocd-redis-6645d4fb89-kpv95"
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager-0 has only found  members of the kube-state-metrics cluster.
3,[pr-cp-reg-10012 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter pr-customer-env/tm-0"
4,[pr-cp-reg-10002 - aris-keda] - CPUThrottlingHigh -  throttling of CPU in namespace aris-keda for container kube-state-metrics in pod keda-operator.
3,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10028 - aris-keda] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10045 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10025 - kube-node-lease] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10002 - aris-cluster-autoscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10008 - aris-keda] - ARIS_k8s_pod_restarted - The pod aris-keda/keda-operator-metrics-apiserver-7c746565cb-t6krb has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10037 - kube-public] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10019 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10003 - aris-kube-downscaler] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10046 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10034 - aris-sealed-secrets] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10037 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10046 - aris-kube-downscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10011 - aris-nginx-ingress] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10026 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
1,[pr-cp-reg-10001 - kasten-io] - NodeRAIDDegraded - RAID array '/dev/nvme1n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
2,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10007 cluster.
3,[pr-cp-reg-10012 - kube-node-lease] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/prometheus-aris-kube-prometheus-stack-prometheus (grafana-sc-dashboard) is restarting  times / 5 minutes.
3,[pr-cp-reg-10038 - aris-keda] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10013 - aris-sealed-secrets] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10015 - aris-cluster-autoscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10037 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,"[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10046 - aris-nginx-ingress] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10007 - pr-customer-env] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env/pr-cp-reg-10007 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container dashboarding in pod backup-logs-28124400-ktxxf.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter management/efs-csi-controller"
3,[pr-cp-reg-10010 - aris-cluster-autoscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10017 - kube-system] - KubeNodeNotReady - ip-10-0-136-224.eu-central-1.compute.internal has been unready for more than 15 minutes.
3,[pr-cp-reg-10031 - kube-public] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10030 - aris-sealed-secrets] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-sealed-secrets has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver (adsadmin) is restarting  times / 5 minutes.
3,[pr-cp-reg-10021 - aris-keda] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10015 - kasten-io] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10023 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10007 - aris-kube-downscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10027 - aris-spot-metrics] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-spot-metrics/pr-cp-reg-10027 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container dex in pod efs-csi-node.
1,[pr-cp-reg-10017 - kasten-io] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme1n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10033 - aris-kube-prometheus-stack] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-prometheus-stack/pr-cp-reg-10033 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10024 - aris-kube-downscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10046 - aris-nginx-ingress] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_argocd_app_unknown - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10042 - pr-customer-env-spark] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-147-93.eu-central-1.compute.internal node in pr-customer-env-spark/pr-cp-reg-10042 cluster.
3,[pr-cp-reg-10008 - pr-customer-env] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in pr-customer-env/pr-cp-reg-10008 cluster is .
1,[pr-cp-reg-10006 - aris-sealed-secrets] - ARIS_k8s_pod_crash_looping - Pod aris-sealed-secrets/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (kube-state-metrics) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - aris-keda] - CPUThrottlingHigh -  throttling of CPU in namespace aris-keda for container kube-state-metrics in pod keda-operator-metrics-apiserver-7c746565cb-t6krb.
2,[pr-cp-reg-10013 - aris-cluster-autoscaler] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-cluster-autoscaler/pr-cp-reg-10013 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/zookeeper (controller) is restarting  times / 5 minutes.
3,[pr-cp-reg-10038 - pr-customer-env-spark] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10003 - aris-keda] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10025 - aris-keda] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
0,"[pr-cp-reg-10050 - pr-customer-env] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10013 - aris-keda] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10048 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-downscaler/pr-cp-reg-10048 health status has been RED for at least 2 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container ces in pod cdf-0.
1,[pr-cp-reg-10004 - pr-customer-env] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10021 - aris-keda] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app lokiDataframeApi is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10031 - aris-kube-downscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
4,[pr-cp-reg-10041 - aris-kube-downscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-downscaler has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10002 - pr-customer-env-spark] - ARIS_k8s_pod_restarted - The pod pr-customer-env-spark/pr-customer-env-spark-spark-operat-wh-init-hwzrg has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10032 - kube-public] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10014 - pr-customer-env-spark] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter pr-customer-env-spark/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
3,[pr-cp-reg-10037 - aris-cluster-autoscaler] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10046 - pr-customer-env-spark] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-147-93.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10018 - aris-kube-prometheus-stack] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
3,"[pr-cp-reg-10013 - aris-sealed-secrets] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller config-reloader sidecar attempts to sync config in aris-sealed-secrets namespace. As a result, configuration for service running in sealed-secrets-controller may be stale and cannot be updated anymore."
3,"[pr-cp-reg-10015 - default] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in default namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10003 - aris-keda] - ARIS_k8s_pod_restarted - The pod aris-keda/keda-operator has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on shm at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
1,[pr-cp-reg-10006 - aris-sealed-secrets] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10023 - kube-system] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10011 - default] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
4,[pr-cp-reg-10005 - falcon-system] - CPUThrottlingHigh -  throttling of CPU in namespace falcon-system for container kube-state-metrics in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10040 - pr-customer-env] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10023 - aris-kube-downscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10024 - management] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod register-smtp-server-6d78c-stqpc.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/collaboration (controller) is restarting  times / 5 minutes.
1,[pr-cp-reg-10018 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
1,[pr-cp-reg-10020 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10012 - pr-customer-env-spark] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10025 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10032 - kube-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
0,"[pr-cp-reg-10050 - aris-kube-downscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10030 - basic-application-bootstrapping] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10003 - management] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme7n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10006 - aris-cluster-autoscaler] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/processengine-0 (cloudsearch) is restarting  times / 5 minutes.
3,[pr-cp-reg-10034 - pr-customer-env-spark] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
3,[pr-cp-reg-10015 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10020 - aris-spot] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,[pr-cp-reg-10049 - aris-sealed-secrets] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (adsadmin) is restarting  times / 5 minutes.
4,[pr-cp-reg-10016 - pr-customer-env-spark] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env-spark/exported_job failed to complete.
1,[pr-cp-reg-10029 - pr-customer-env-spark] - ARIS_k8s_node_pid_pressure - ip-10-0-147-93.eu-central-1.compute.internal has PIDPressure condition.
0,"[pr-cp-reg-10049 - aris-cluster-autoscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10040 - pr-customer-env-spark] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10010 - kube-node-lease] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
1,[pr-cp-reg-10036 - falcon-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
0,"[pr-cp-reg-10011 - falcon-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme2n1"" changing its up status often on node-exporter management/argocd-dex-server-6b74fb9695-kbk62"
3,[pr-cp-reg-10005 - management] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme5n1 has encountered  receive errors in the last two minutes.
2,[pr-cp-reg-10003 - aris-keda] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-keda/pr-cp-reg-10003 cluster.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
2,[pr-cp-reg-10009 - pr-customer-env] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10009 cluster.
2,[pr-cp-reg-10010 - pr-customer-env] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10010 cluster.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cloudsearch (elasticsearch) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container metering-svc in pod kanister-svc-9559f74-79zgn.
1,[pr-cp-reg-10035 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_argocd_app_unknown - The ArgoCD app 96ff84e25e745ef28b97b02b2ccf87ca2cb688b9ac0a8c41bb217d4cb18ccddb is in unknown state for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10032 - aris-kube-downscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10005 - kasten-io] - NodeNetworkTransmitErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme1n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10021 - aris-keda] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container abs in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
4,[pr-cp-reg-10048 - pr-customer-env-spark] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env-spark has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10011 - aris-cluster-autoscaler] - ARIS_k8s_node_pid_pressure - ip-10-0-139-113.eu-central-1.compute.internal has PIDPressure condition.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to sns sent from any instance in the kube-state-metrics cluster is .
3,[pr-cp-reg-10001 - aris-spot-metrics] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-spot-metrics has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10048 - pr-customer-env] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10001 - aris-keda] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container umcadmin in pod elasticsearch-default.
1,[pr-cp-reg-10036 - kube-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10007 - aris-sealed-secrets] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10013 - aris-kube-downscaler] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10006 - pr-customer-env] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10047 - aris-keda] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10031 - pr-customer-env-spark] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10005 - aris-sealed-secrets] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-136-224.eu-central-1.compute.internal in aris-sealed-secrets/pr-cp-reg-10005 cluster is .
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container liveness-probe in pod efs-csi-node-2r2zp.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elasticsearch in pod cloudsearch.
1,[pr-cp-reg-10041 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod processengine.
3,[pr-cp-reg-10045 - aris-spot] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10018 - pr-customer-env] - ARIS_host_oom_kill_detected - OOM kill detected.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerClusterFailedToSendAlerts - The minimum notification failure rate to webhook sent from any instance in the kubelet cluster is .
3,[pr-cp-reg-10017 - kube-system] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10029 - pr-customer-env] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10003 - aris-nginx-ingress] - ARIS_argocd_app_progressing - The ArgoCD app 96ff84e25e745ef28b97b02b2ccf87ca2cb688b9ac0a8c41bb217d4cb18ccddb is progressing for longer than 15 minutes and requires manual intervention.
3,"[pr-cp-reg-10007 - falcon-system] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in falcon-system namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10009 - aris-keda] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod ae-aw-euc1-15995-external-dns.
3,[pr-cp-reg-10043 - pr-customer-env] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10043 - kube-node-lease] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10009 - aris-cluster-autoscaler] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10035 - pr-customer-env] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10026 - aris-sealed-secrets] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10050 - aris-cluster-autoscaler] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10022 - pr-customer-env-spark] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10001 - aris-kube-downscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10015 - pr-customer-env-spark] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10037 - pr-customer-env-spark] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerClusterDown -  of Alertmanager instances within the sealed-secrets-controller cluster have been up for less than half of the last 5m.
3,"[pr-cp-reg-10023 - basic-application-bootstrapping] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in basic-application-bootstrapping namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_k8s_pod_crash_looping - Pod aris-nginx-ingress/sealed-secrets-controller-cbbc8bd6b-qq22k (kube-state-metrics) is restarting  times / 5 minutes.
3,[pr-cp-reg-10017 - aris-sealed-secrets] - ARIS_argocd_app_unknown - The ArgoCD app 46d6ed101040cf8df2a51291aad961a76c082eba555ed2401548a98d1bb4d54d is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10049 - aris-kube-prometheus-stack] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
1,[pr-cp-reg-10024 - kube-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10038 - aris-kube-prometheus-stack] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10033 - aris-kube-downscaler] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10040 - falcon-system] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-grafana-cf647cb96-2hnq5 failed to send  of notifications to email.
3,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-139-113.eu-central-1.compute.internal in aris-kube-prometheus-stack/pr-cp-reg-10010 cluster is .
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container loadbalancer in pod serviceenabling.
3,[pr-cp-reg-10020 - kube-public] - KubeMemoryOvercommit - Cluster has overcommitted memory resource requests for Pods by  bytes and cannot tolerate node failure.
3,"[pr-cp-reg-10021 - kube-public] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-public namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10007 - kasten-io] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,"[pr-cp-reg-10033 - kube-node-lease] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in kube-node-lease namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10035 - aris-kube-prometheus-stack] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10036 - aris-cluster-autoscaler] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28124475-fjqvp (octopus) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container create-es-index-template in pod adsadmin.
1,[pr-cp-reg-10006 - aris-sealed-secrets] - ARIS_k8s_pod_crash_looping - Pod aris-sealed-secrets/sealed-secrets-controller-cbbc8bd6b-qq22k (kube-state-metrics) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod backup-tenants-28125915-9ps25.
1,[pr-cp-reg-10038 - pr-customer-env] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-822lp failed to send  of notifications to slack.
1,[pr-cp-reg-10027 - kube-node-lease] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10014 - pr-customer-env] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
2,[pr-cp-reg-10024 - aris-nginx-ingress] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10024 cluster.
3,[pr-cp-reg-10032 - aris-cluster-autoscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
3,"[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10014 - aris-cluster-autoscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-cluster-autoscaler/pr-cp-reg-10014 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10045 - aris-sealed-secrets] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container csi-attacher in pod ae-aw-euc1-15995-aws-load-balancer-controller-7895559794-c8nd5.
3,[pr-cp-reg-10050 - pr-customer-env-spark] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10049 - aris-nginx-ingress] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10035 - pr-customer-env-spark] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10002 - aris-keda] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter aris-keda/keda-operator-metrics-apiserver"
3,[pr-cp-reg-10038 - aris-sealed-secrets] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,"[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10008 - aris-kube-downscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
2,[pr-cp-reg-10046 - aris-sealed-secrets] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-sealed-secrets/pr-cp-reg-10046 cluster.
1,[pr-cp-reg-10003 - pr-customer-env] - NodeRAIDDegraded - RAID array '/dev/nvme3n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container ebs-plugin in pod ebs-csi-node-7gdr8.
3,[pr-cp-reg-10014 - aris-keda] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10007 - aris-keda] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
3,[pr-cp-reg-10025 - aris-kube-prometheus-stack] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10031 - aris-sealed-secrets] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10035 - falcon-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10002 - aris-kube-prometheus-stack] - NodeFilesystemFilesFillingUp - Filesystem on nvme2 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_host_high_cpu_load - CPU load is > 90%.
3,[pr-cp-reg-10035 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10003 - pr-customer-env] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10006 - kasten-io] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-147-93.eu-central-1.compute.internal in pr-customer-env-spark/pr-cp-reg-10041 cluster is .
4,[pr-cp-reg-10050 - aris-spot] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_out_of_sync - The ArgoCD app internationalization is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10003 - pr-customer-env-spark] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10041 - aris-spot-metrics] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container metering-svc in pod state-svc.
3,[pr-cp-reg-10029 - aris-keda] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10012 - aris-cluster-autoscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
1,[pr-cp-reg-10050 - aris-nginx-ingress] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10004 - aris-spot-metrics] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot-metrics has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10004 - aris-nginx-ingress] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10033 - aris-kube-prometheus-stack] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
0,"[pr-cp-reg-10020 - aris-cluster-autoscaler] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10039 - kube-system] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,[pr-cp-reg-10033 - kube-public] - KubeMemoryQuotaOvercommit - Cluster has overcommitted memory resource requests for Namespaces.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processboard in pod abs.
3,"[pr-cp-reg-10001 - aris-kube-prometheus-stack] - NodeNetworkInterfaceFlapping - Network interface ""tmpfs"" changing its up status often on node-exporter aris-kube-prometheus-stack/sealed-secrets-controller-cbbc8bd6b-qq22k"
3,[pr-cp-reg-10004 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
3,[pr-cp-reg-10005 - pr-customer-env-spark] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10004 - kube-node-lease] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
1,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_error_watermark_reached - error Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10011 cluster.
4,[pr-cp-reg-10046 - aris-kube-prometheus-stack] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-kube-prometheus-stack/exported_job failed to complete.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28125915-9ps25 (serviceenabling) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10042 - aris-cluster-autoscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10021 - pr-customer-env] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10046 - pr-customer-env] - ARIS_k8s_pod_not_ready - The application pod has not been ready for more than 15 minutes and should be restarted.
3,[pr-cp-reg-10007 - kube-system] - KubeletPodStartUpLatencyHigh - Kubelet Pod startup 99th percentile latency is  seconds on node ip-10-0-139-113.eu-central-1.compute.internal.
2,[pr-cp-reg-10006 - aris-spot] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-143-220.eu-central-1.compute.internal node in aris-spot/pr-cp-reg-10006 cluster.
3,[pr-cp-reg-10004 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme3n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10008 - aris-cluster-autoscaler] - ARIS_host_clock_not_synchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
4,[pr-cp-reg-10039 - aris-keda] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-keda has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_argocd_app_progressing - The ArgoCD app e6f9fffe27de98d428ec80430511d50bd4b650772b6a75af78694e5757d876be is progressing for longer than 15 minutes and requires manual intervention.
1,[pr-cp-reg-10032 - aris-cluster-autoscaler] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10018 - default] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in default namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
1,[pr-cp-reg-10037 - pr-customer-env] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10047 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-downscaler/pr-cp-reg-10047 health status has been RED for at least 2 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_restarted - The pod pr-customer-env/umcadmin has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10016 - aris-cluster-autoscaler] - ARIS_host_out_of_disk_space - Disk is almost full (< 10% left).
3,[pr-cp-reg-10002 - default] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10004 - kube-system] - KubeletServerCertificateExpiration - Server certificate for Kubelet on node ip-10-0-139-113.eu-central-1.compute.internal expires in .
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/octopus-0 (abs) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container copyutil in pod argocd-application-controller.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/ces (log-backup) is restarting  times / 5 minutes.
2,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-139-113.eu-central-1.compute.internal node in aris-nginx-ingress/pr-cp-reg-10025 cluster.
4,[pr-cp-reg-10038 - aris-kube-downscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-downscaler has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10008 - aris-spot] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
0,"[pr-cp-reg-10023 - falcon-system] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
3,[pr-cp-reg-10014 - aris-sealed-secrets] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10004 - aris-cluster-autoscaler] - ARIS_k8s_pod_crash_looping - Pod aris-cluster-autoscaler/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k (controller) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container redis in pod efs-csi-node-4cgpg.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container upgrade-init in pod catalog-svc.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter management/argocd-redis-ha-server-1"
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_high - Latency to the upstream service aris-nginx-ingress/aris-kube-prometheus-stack-prometheus has been high (more than 10 seconds) for the last 3 minutes. Service might become unresponsive shortly.
3,[pr-cp-reg-10011 - pr-customer-env] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10035 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager failed to send  of notifications to pushover.
1,[pr-cp-reg-10015 - pr-customer-env] - ARIS_k8s_node_disk_pressure - ip-10-0-138-163.eu-central-1.compute.internal has DiskPressure condition.
1,[pr-cp-reg-10006 - pr-customer-env] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10027 - pr-customer-env] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env/pr-cp-reg-10027 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10015 - aris-nginx-ingress] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10002 - pr-customer-env] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
2,[pr-cp-reg-10011 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10011 cluster.
1,[pr-cp-reg-10043 - pr-customer-env] - ARIS_elasticsearch_cluster_status_is_RED - Cluster pr-customer-env/pr-cp-reg-10043 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10015 - kube-system] - KubeCPUQuotaOvercommit - Cluster has overcommitted CPU resource requests for Namespaces.
3,"[pr-cp-reg-10048 - pr-customer-env-spark] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10013 - aris-kube-downscaler] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10027 - aris-sealed-secrets] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28123035-nswx7 (collaboration) is restarting  times / 5 minutes.
1,[pr-cp-reg-10043 - aris-cluster-autoscaler] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10012 - aris-nginx-ingress] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10005 - aris-nginx-ingress] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app traceToMetrics is in unknown state for longer than 10 minutes and requires manual intervention.
1,[pr-cp-reg-10023 - pr-customer-env] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10033 - kube-public] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10007 - pr-customer-env] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
1,[pr-cp-reg-10039 - kube-node-lease] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10038 - kube-public] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager has only found  members of the sealed-secrets-controller cluster.
3,[pr-cp-reg-10040 - aris-cluster-autoscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cdf in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
3,[pr-cp-reg-10006 - kube-system] - KubeletServerCertificateRenewalErrors - Kubelet on node ip-10-0-136-224.eu-central-1.compute.internal has failed to renew its server certificate ( errors in the last 5 minutes).
3,[pr-cp-reg-10021 - aris-kube-downscaler] - ARIS_host_out_of_inodes - Disk is almost running out of available inodes (< 10% left).
1,[pr-cp-reg-10003 - aris-sealed-secrets] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - kube-system] - CPUThrottlingHigh -  throttling of CPU in namespace kube-system for container metrics-server in pod metrics-server-5756d96f6-tm5h8.
3,[pr-cp-reg-10001 - aris-keda] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10048 - falcon-system] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10028 - aris-sealed-secrets] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
3,[pr-cp-reg-10044 - aris-nginx-ingress] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10006 - aris-nginx-ingress] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,[pr-cp-reg-10017 - aris-keda] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerMembersInconsistent - Alertmanager aris-kube-prometheus-stack/alertmanager-aris-kube-prometheus-stack-alertmanager has only found  members of the aris-kube-prometheus-stack-prometheus cluster.
3,[pr-cp-reg-10005 - aris-cluster-autoscaler] - ARIS_argocd_app_progressing - The ArgoCD app 4c13d5010bbf3a464660a47a6f86df9a4d4d35cb91b64b79fdf47b0ad1105bac is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10010 - aris-kube-downscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10037 - aris-sealed-secrets] - ARIS_k8s_cronjob_arbitrary_failed - Job aris-sealed-secrets/exported_job failed to complete.
3,[pr-cp-reg-10032 - kube-system] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
3,[pr-cp-reg-10004 - aris-keda] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10023 - aris-sealed-secrets] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_unknown - The ArgoCD app dashboardPreviews is in unknown state for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10041 - pr-customer-env-spark] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env-spark/exported_job failed to complete.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme6n1"" changing its up status often on node-exporter management/argocd-redis-ha-server-2"
3,"[pr-cp-reg-10017 - basic-application-bootstrapping] - ConfigReloaderSidecarErrors - Errors encountered while the basic-application-bootstrapping-2wp59 config-reloader sidecar attempts to sync config in basic-application-bootstrapping namespace. As a result, configuration for service running in basic-application-bootstrapping-2wp59 may be stale and cannot be updated anymore."
3,[pr-cp-reg-10046 - aris-sealed-secrets] - ARIS_host_cpu_steal_noisy_neighbor - CPU steal is > 10%. A noisy neighbor is killing VM performance.
1,[pr-cp-reg-10008 - falcon-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
4,[pr-cp-reg-10036 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_failed - Job pr-customer-env/exported_job failed to complete.
3,[pr-cp-reg-10001 - kube-public] - KubeCPUOvercommit - Cluster has overcommitted CPU resource requests for Pods by  CPU shares and cannot tolerate node failure.
3,"[pr-cp-reg-10013 - basic-application-bootstrapping] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in basic-application-bootstrapping namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10001 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme4n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter (node-exporter) is restarting  times / 5 minutes.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
3,[pr-cp-reg-10006 - aris-keda] - ARIS_argocd_app_out_of_sync - The ArgoCD app scaledjob is out of sync for longer than 10 minutes and requires manual intervention.
4,[pr-cp-reg-10002 - pr-customer-env-spark] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env-spark for container spark-operator in pod pr-customer-env-spark-spark-operat-wh-init-hwzrg.
3,[pr-cp-reg-10015 - aris-nginx-ingress] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124280-qwm2q (abs) is restarting  times / 5 minutes.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfSpace - Filesystem on nvme2 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_restarted - The pod pr-customer-env/octopus has restarted. All other pods in the same namespace should be restarted to resolve resilience issues.
3,[pr-cp-reg-10010 - management] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10049 - aris-sealed-secrets] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod kanister-job-7lbrs.
3,[pr-cp-reg-10014 - aris-kube-downscaler] - ARIS_host_high_number_conntrack_entries_used -  of conntrack entries are used.
3,[pr-cp-reg-10001 - kube-system] - KubeAggregatedAPIDown - Kubernetes aggregated API 0967bcf9cd94edf7fcfc0db86322d8f7af0a81626feafeaae506bfb81271ae15/kube-system has been only % available over the last 10m.
1,[pr-cp-reg-10003 - pr-customer-env] - ARIS_k8s_cronjob_backup_tenants_failed - Job pr-customer-env/backup-tenants-28125915 failed to complete.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container alertmanager in pod alertmanager-aris-kube-prometheus-stack-alertmanager.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter pr-customer-env/cdf-0"
3,"[pr-cp-reg-10019 - aris-nginx-ingress] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
4,[pr-cp-reg-10023 - pr-customer-env] - ARIS_k8s_cronjob_arbitrary_suspended - CronJob pr-customer-env/backup-tenants is suspended.
3,[pr-cp-reg-10025 - aris-sealed-secrets] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container schema-upgrade-check in pod executor-svc.
3,[pr-cp-reg-10033 - aris-keda] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10021 - pr-customer-env-spark] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container cert-manager in pod cert-manager-795fd7b44f-f5hrn.
3,[pr-cp-reg-10017 - aris-keda] - ARIS_host_high_cpu_load - CPU load is > 90%.
1,[pr-cp-reg-10045 - pr-customer-env] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-logs-28124400-ktxxf (tm) is restarting  times / 5 minutes.
3,[pr-cp-reg-10031 - aris-kube-downscaler] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10043 - aris-spot] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
1,[pr-cp-reg-10039 - management] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10006 - pr-customer-env] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme2n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up fast.
3,[pr-cp-reg-10008 - pr-customer-env-spark] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster pr-customer-env-spark/pr-cp-reg-10008 health status has been YELLOW for at least 20 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container simulation in pod portalserver.
1,[pr-cp-reg-10046 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,"[pr-cp-reg-10015 - aris-spot-metrics] - ConfigReloaderSidecarErrors - Errors encountered while the aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k config-reloader sidecar attempts to sync config in aris-spot-metrics namespace. As a result, configuration for service running in aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10015 - pr-customer-env-spark] - ARIS_host_out_of_memory - Node memory is filling up (< 10% left).
3,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10032 - kube-system] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/postgres-0 (octopus) is restarting  times / 5 minutes.
3,"[pr-cp-reg-10001 - aris-keda] - ConfigReloaderSidecarErrors - Errors encountered while the sealed-secrets-controller-cbbc8bd6b-qq22k config-reloader sidecar attempts to sync config in aris-keda namespace. As a result, configuration for service running in sealed-secrets-controller-cbbc8bd6b-qq22k may be stale and cannot be updated anymore."
3,[pr-cp-reg-10020 - aris-kube-prometheus-stack] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/backup-tenants-28123035-nswx7 (kube-state-metrics) is restarting  times / 5 minutes.
1,[pr-cp-reg-10015 - kasten-io] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container serviceenabling in pod kanister-job-6gvp5.
3,[pr-cp-reg-10047 - aris-sealed-secrets] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10033 - pr-customer-env-spark] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-147-93.eu-central-1.compute.internal in pr-customer-env-spark/pr-cp-reg-10033 cluster is .
3,[pr-cp-reg-10041 - aris-kube-prometheus-stack] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - AlertmanagerFailedToSendAlerts - Alertmanager aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter failed to send  of notifications to victorops.
3,[pr-cp-reg-10028 - aris-kube-downscaler] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
1,[pr-cp-reg-10013 - pr-customer-env-spark] - ARIS_postgresql_down - The postgres pod of namespace pr-customer-env-spark is down.
3,[pr-cp-reg-10001 - aris-kube-downscaler] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/elasticsearch-default-0 (abs) is restarting  times / 5 minutes.
1,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_k8s_node_disk_pressure - ip-10-0-139-113.eu-central-1.compute.internal has DiskPressure condition.
3,[pr-cp-reg-10016 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-kube-downscaler/pr-cp-reg-10016 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10031 - pr-customer-env-spark] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
3,"[pr-cp-reg-10001 - management] - ConfigReloaderSidecarErrors - Errors encountered while the ebs-csi-controller-86cb997fdc-bs5bs config-reloader sidecar attempts to sync config in management namespace. As a result, configuration for service running in ebs-csi-controller-86cb997fdc-bs5bs may be stale and cannot be updated anymore."
3,[pr-cp-reg-10005 - kube-system] - KubeClientErrors - Kubernetes API server client 'coredns/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
3,[pr-cp-reg-10023 - aris-nginx-ingress] - ARIS_elasticsearch_process_cpu_error - Elasticsearch process CPU usage on the node ip-10-0-143-220.eu-central-1.compute.internal in aris-nginx-ingress/pr-cp-reg-10023 cluster is .
3,[pr-cp-reg-10017 - aris-keda] - ARIS_nginx_ssl_certificate_will_expire - The SSL certificate will expire in less than 14 days and must be replaced.
1,[pr-cp-reg-10016 - aris-keda] - NodeFilesystemAlmostOutOfSpace - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left.
1,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_host_file_descriptor_limit_reached - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10048 - aris-kube-prometheus-stack] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10008 - aris-spot-metrics] - ARIS_postgresql_down - The postgres pod of namespace aris-spot-metrics is down.
1,[pr-cp-reg-10024 - kube-public] - KubeProxyDown - KubeProxy has disappeared from Prometheus target discovery.
3,[pr-cp-reg-10036 - default] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,"[pr-cp-reg-10001 - pr-customer-env] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme4n1"" changing its up status often on node-exporter pr-customer-env/aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k"
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container argocd-server in pod argocd-dex-server-6b74fb9695-kbk62.
3,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - ARIS_host_high_cpu_load - CPU load is > 90%.
4,[pr-cp-reg-10005 - aris-nginx-ingress] - CPUThrottlingHigh -  throttling of CPU in namespace aris-nginx-ingress for container controller in pod aris-nginx-ingress-ingress-nginx-controller.
4,[pr-cp-reg-10001 - kube-system] - KubeQuotaFullyUsed - Namespace kube-system is using  of its cpu quota.
4,[pr-cp-reg-10039 - aris-kube-downscaler] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-kube-downscaler has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - TargetDown - % of the kubelet/alertmanager-operated targets in aris-kube-prometheus-stack namespace are down.
3,[pr-cp-reg-10031 - aris-sealed-secrets] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter management/ebs-csi-node-7gdr8"
3,[pr-cp-reg-10003 - aris-keda] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
2,[pr-cp-reg-10004 - pr-customer-env] - ARIS_elasticsearch_disk_low_watermark_reached - Low Watermark Reached at ip-10-0-138-163.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10004 cluster.
3,[pr-cp-reg-10035 - aris-nginx-ingress] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10008 - aris-spot-metrics] - ARIS_postgresql_deadlocks - The postgres instance of namespace aris-spot-metrics has faced > 5 deadlocks in last minute.
3,[pr-cp-reg-10038 - aris-cluster-autoscaler] - ARIS_host_memory_under_memory_pressure - The node is under heavy memory pressure. High rate of major page faults.
1,[pr-cp-reg-10011 - aris-keda] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container zookeeper in pod serviceenabling-0.
3,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_argocd_app_progressing - The ArgoCD app autoMigrateGraphPanels is progressing for longer than 15 minutes and requires manual intervention.
3,[pr-cp-reg-10036 - aris-nginx-ingress] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-nginx-ingress has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10026 - aris-nginx-ingress] - ARIS_elasticsearch_cluster_status_Is_YELLOW - Cluster aris-nginx-ingress/pr-cp-reg-10026 health status has been YELLOW for at least 20 minutes.
1,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - AlertmanagerConfigInconsistent - Alertmanager instances within the aris-kube-prometheus-stack-operator cluster have different configurations.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/elasticsearch-default (cdf) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elastic-metrics-sidecar in pod elasticsearch-default.
3,[pr-cp-reg-10035 - kube-system] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10001 - falcon-system] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up fast.
3,[pr-cp-reg-10030 - aris-kube-downscaler] - ARIS_host_oom_kill_detected - OOM kill detected.
3,[pr-cp-reg-10023 - aris-cluster-autoscaler] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10023 - pr-customer-env-spark] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10034 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10042 - kube-system] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container external-dns in pod ebs-csi-node-7gdr8.
1,[pr-cp-reg-10010 - kube-node-lease] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10005 - pr-customer-env] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10028 - aris-spot] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10020 - pr-customer-env] - ARIS_k8s_cronjob_backup_logs_takes_too_long - CronJob pr-customer-env/backup-logs is taking more than 1h to complete.
3,[pr-cp-reg-10043 - pr-customer-env] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10037 - aris-nginx-ingress] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
1,[pr-cp-reg-10011 - pr-customer-env-spark] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container external-dns in pod argocd-redis-ha-server.
1,[pr-cp-reg-10023 - kube-public] - KubeletDown - Kubelet has disappeared from Prometheus target discovery.
1,[pr-cp-reg-10004 - aris-kube-prometheus-stack] - NodeFilesystemAlmostOutOfFiles - Filesystem on shm at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10010 - aris-sealed-secrets] - ARIS_k8s_node_pid_pressure - ip-10-0-136-224.eu-central-1.compute.internal has PIDPressure condition.
3,[pr-cp-reg-10011 - pr-customer-env-spark] - ARIS_host_filesystem_almost_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10019 - aris-kube-downscaler] - ARIS_argocd_app_progressing - The ArgoCD app aris-image-pull-secret is progressing for longer than 15 minutes and requires manual intervention.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container sentinel in pod argocd-repo-server-6676cd4f9c-l95m8.
4,[pr-cp-reg-10001 - management] - CPUThrottlingHigh -  throttling of CPU in namespace management for container efs-plugin in pod aris-kube-prometheus-stack-kube-state-metrics-785d575975-s2j2k.
1,[pr-cp-reg-10033 - kube-system] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10037 - aris-spot-metrics] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10026 - aris-nginx-ingress] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container processengine in pod loadbalancer.
1,[pr-cp-reg-10013 - aris-kube-prometheus-stack] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container cloudsearch in pod cloudsearch.
3,[pr-cp-reg-10008 - kube-public] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10023 - kube-public] - KubeAPITerminatedRequests - The kubernetes apiserver has terminated  of its incoming requests.
3,[pr-cp-reg-10017 - pr-customer-env-spark] - ARIS_host_disk_will_fill_in_24_hours - Filesystem is predicted to run out of space within the next 24 hours at current write rate.
3,[pr-cp-reg-10002 - management] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme7n1' needs attention and possibly a disk swap.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/tm-0 (dashboarding) is restarting  times / 5 minutes.
3,[pr-cp-reg-10010 - kasten-io] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme2n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10049 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
1,[pr-cp-reg-10023 - aris-sealed-secrets] - ARIS_k8s_node_disk_pressure - ip-10-0-136-224.eu-central-1.compute.internal has DiskPressure condition.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme3n1"" changing its up status often on node-exporter management/argocd-redis-ha-server-0"
3,[pr-cp-reg-10029 - aris-cluster-autoscaler] - ARIS_cluster_autoscaler_unschedulable_pods - The cluster autoscaler is unable to scale up and there are unschedulable pods because of this condition.
4,[pr-cp-reg-10021 - pr-customer-env] - ARIS_postgresql_deadlocks - The postgres instance of namespace pr-customer-env has faced > 5 deadlocks in last minute.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/cloudsearch-0 (container) is restarting  times / 5 minutes.
1,[pr-cp-reg-10019 - aris-spot] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10037 - kube-system] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10024 - aris-keda] - NodeFilesystemSpaceFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available space left and is filling up.
3,[pr-cp-reg-10033 - aris-sealed-secrets] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-sealed-secrets has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10017 - aris-kube-downscaler] - ARIS_postgresql_too_many_connections - The postgres instance of namespace aris-kube-downscaler has too many active connections. More than 90% of available connections are used.
3,[pr-cp-reg-10050 - aris-cluster-autoscaler] - NodeHighNumberConntrackEntriesUsed -  of conntrack entries are used.
3,[pr-cp-reg-10019 - aris-sealed-secrets] - ARIS_argocd_app_out_of_sync - The ArgoCD app 67339622db464b6c57a685a17f7473ab5ceba74c82eb6352fa3b482a524cc185 is out of sync for longer than 10 minutes and requires manual intervention.
3,"[pr-cp-reg-10001 - management] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme7n1"" changing its up status often on node-exporter management/efs-csi-node-4cgpg"
1,[pr-cp-reg-10024 - aris-cluster-autoscaler] - ARIS_k8s_node_memory_pressure - ip-10-0-139-113.eu-central-1.compute.internal has MemoryPressure condition.
3,[pr-cp-reg-10004 - aris-sealed-secrets] - NodeFilesystemFilesFillingUp - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left and is filling up.
3,[pr-cp-reg-10050 - aris-cluster-autoscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10010 - aris-kube-prometheus-stack] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container elastic-metrics-sidecar in pod backup-tenants-28124475-fjqvp.
1,[pr-cp-reg-10040 - kube-node-lease] - KubeAPIDown - KubeAPI has disappeared from Prometheus target discovery.
3,"[pr-cp-reg-10044 - aris-cluster-autoscaler] - ARIS_spot_ocean_unusual_scaling - The cluster has grown by more than 2 nodes per zone in the last hour. This might be legit, but should be confirmed manually."
3,[pr-cp-reg-10044 - aris-nginx-ingress] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
4,[pr-cp-reg-10001 - kasten-io] - CPUThrottlingHigh -  throttling of CPU in namespace kasten-io for container dashboardbff-svc in pod jobs-svc-685557c545-4mnxp.
3,[pr-cp-reg-10048 - kube-system] - KubeVersionMismatch - There are  different semantic versions of Kubernetes components running.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container admintools in pod simulation-0.
1,[pr-cp-reg-10004 - aris-kube-downscaler] - ARIS_elasticsearch_cluster_status_is_RED - Cluster aris-kube-downscaler/pr-cp-reg-10004 health status has been RED for at least 2 minutes.
3,[pr-cp-reg-10030 - aris-keda] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/abs-0 (setmaxmapcount) is restarting  times / 5 minutes.
3,[pr-cp-reg-10020 - aris-sealed-secrets] - ARIS_prometheus_all_targets_missing - A Prometheus job does not have living target anymore.
1,[pr-cp-reg-10019 - aris-keda] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10009 - kube-public] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10050 - aris-sealed-secrets] - NodeFilesystemAlmostOutOfFiles - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
1,[pr-cp-reg-10029 - aris-sealed-secrets] - ARIS_k8s_pod_stuck_as_zombie - The node exporter provides duplicate metrics for the same pod and container. Most probably a pod has been force deleted and is now stuck on the worker node.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container postgres in pod register-smtp-server-6d78c-stqpc.
3,[pr-cp-reg-10001 - kube-system] - KubeClientErrors - Kubernetes API server client 'kubelet/<IP_ADDRESS>:<PORT>' is experiencing  errors.'
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/loadbalancer-0 (log-backup) is restarting  times / 5 minutes.
1,[pr-cp-reg-10029 - aris-kube-downscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
3,[pr-cp-reg-10026 - aris-spot-metrics] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10012 - aris-cluster-autoscaler] - ARIS_spot_ocean_unavailable - The cluster could not reach the Spot Ocean SaaS for at least 10 minutes. The service might be unavailable.
1,[pr-cp-reg-10038 - aris-cluster-autoscaler] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10003 - aris-sealed-secrets] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
3,[pr-cp-reg-10030 - aris-kube-prometheus-stack] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
4,[pr-cp-reg-10001 - pr-customer-env] - CPUThrottlingHigh -  throttling of CPU in namespace pr-customer-env for container tenant-backup in pod backup-logs-28124160-s7ps9.
3,[pr-cp-reg-10026 - basic-application-bootstrapping] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10003 - aris-kube-prometheus-stack] - ARIS_host_network_receive_errors - <IP_ADDRESS>:<PORT> interface nvme1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10012 - aris-spot] - NodeNetworkReceiveErrs - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  receive errors in the last two minutes.
3,[pr-cp-reg-10030 - aris-cluster-autoscaler] - ARIS_host_network_transmit_errors - <IP_ADDRESS>:<PORT> interface /dev/nvme0n1 has encountered  transmit errors in the last two minutes.
1,[pr-cp-reg-10019 - aris-spot] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/portalserver-0 (create-es-index-template) is restarting  times / 5 minutes.
2,[pr-cp-reg-10003 - pr-customer-env] - ARIS_elasticsearch_bulk_requests_rejection_error - error Bulk Rejection Ratio at ip-10-0-139-113.eu-central-1.compute.internal node in pr-customer-env/pr-cp-reg-10003 cluster.
3,[pr-cp-reg-10002 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_high - Latency to the upstream service pr-customer-env/aris-kube-prometheus-stack-prometheus has been high (more than 10 seconds) for the last 3 minutes. Service might become unresponsive shortly.
1,[pr-cp-reg-10001 - aris-nginx-ingress] - ARIS_nginx_upstream_latency_critical - Latency to the upstream service pr-customer-env/aris-kube-prometheus-stack-prometheus has been critically high (more than 30 seconds) for the last 3 minutes. Service is unresponsive and should be restarted.
3,[pr-cp-reg-10020 - aris-kube-downscaler] - ARIS_prometheus_target_missing_after_warmup_time - A Prometheus job does not have any living targets after the warumup time of 10 minutes.
3,[pr-cp-reg-10050 - aris-keda] - ARIS_host_file_descriptor_limit_approaching - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,[pr-cp-reg-10043 - aris-cluster-autoscaler] - NodeTextFileCollectorScrapeError - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10016 - aris-cluster-autoscaler] - ARIS_host_text_file_collector_scrape_error - Node Exporter text file collector failed to scrape.
3,[pr-cp-reg-10006 - pr-customer-env-spark] - ARIS_k8s_statefulset_replicas_mismatch - A StatefulSet does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10008 - aris-kube-prometheus-stack] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
2,[pr-cp-reg-10007 - aris-kube-prometheus-stack] - ARIS_elasticsearch_disk_low_for_segment_merges - Free disk at ip-10-0-136-224.eu-central-1.compute.internal node in aris-kube-prometheus-stack/pr-cp-reg-10007 cluster may be low for segment merges.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin-0 (loadbalancer) is restarting  times / 5 minutes.
3,[pr-cp-reg-10036 - default] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
1,[pr-cp-reg-10014 - pr-customer-env-spark] - ARIS_host_filesystem_out_of_files - Filesystem on /dev/nvme0n1 at <IP_ADDRESS>:<PORT> has only % available inodes left.
3,[pr-cp-reg-10042 - aris-kube-downscaler] - ARIS_argocd_app_out_of_sync - The ArgoCD app aris-image-pull-secret is out of sync for longer than 10 minutes and requires manual intervention.
3,[pr-cp-reg-10047 - aris-kube-downscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.
1,[pr-cp-reg-10033 - kube-public] - KubeClientCertificateExpiration - A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
3,[pr-cp-reg-10047 - aris-keda] - ARIS_prometheus_configuration_reload_failure - Prometheus configuration could not be reloaded.
1,[pr-cp-reg-10047 - kube-system] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
3,"[pr-cp-reg-10002 - kube-system] - NodeNetworkInterfaceFlapping - Network interface ""/dev/nvme0n1"" changing its up status often on node-exporter kube-system/coredns"
1,[pr-cp-reg-10025 - aris-nginx-ingress] - ARIS_k8s_cluster_out_of_capacity - ip-10-0-139-113.eu-central-1.compute.internal is out of capacity. Consider adding additional worker nodes.
3,[pr-cp-reg-10048 - aris-sealed-secrets] - NodeClockNotSynchronising - Clock on <IP_ADDRESS>:<PORT> is not synchronising. Ensure NTP is configured on this host.
1,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - ARIS_k8s_pod_crash_looping - Pod aris-kube-prometheus-stack/aris-kube-prometheus-stack-prometheus-node-exporter-fznn7 (grafana-sc-dashboard) is restarting  times / 5 minutes.
1,[pr-cp-reg-10013 - pr-customer-env-spark] - NodeRAIDDegraded - RAID array '/dev/nvme0n1' on <IP_ADDRESS>:<PORT> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
3,[pr-cp-reg-10031 - aris-kube-prometheus-stack] - ARIS_k8s_deployment_replicas_mismatch - A deployment does not match the expected number of replicas for more than 10 minutes.
3,[pr-cp-reg-10010 - falcon-system] - NodeRAIDDiskFailure - At least one device in RAID array on <IP_ADDRESS>:<PORT> failed. Array '/dev/nvme0n1' needs attention and possibly a disk swap.
3,[pr-cp-reg-10020 - kube-public] - NodeClockSkewDetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
3,[pr-cp-reg-10008 - aris-sealed-secrets] - ARIS_host_clock_skew_eetected - Clock on <IP_ADDRESS>:<PORT> is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
0,"[pr-cp-reg-10005 - aris-sealed-secrets] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
0,"[pr-cp-reg-10020 - kube-public] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
0,"[pr-cp-reg-10022 - pr-customer-env-spark] - InfoInhibitor - This is an alert that is used to inhibit info alerts. By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts. This alert fires whenever there's a severity=""info"" alert, and stops firing when another alert with a severity of 'warning' or 'critical' starts firing on the same namespace. This alert should be routed to a null receiver and configured to inhibit alerts with severity=""info""."
1,[pr-cp-reg-10016 - kasten-io] - NodeFileDescriptorLimit - File descriptors limit at <IP_ADDRESS>:<PORT> is currently at %.
1,[pr-cp-reg-10001 - pr-customer-env] - ARIS_k8s_pod_crash_looping - Pod pr-customer-env/umcadmin-0 (elasticsearch) is restarting  times / 5 minutes.
4,[pr-cp-reg-10001 - aris-kube-prometheus-stack] - CPUThrottlingHigh -  throttling of CPU in namespace aris-kube-prometheus-stack for container grafana-sc-dashboard in pod aris-kube-prometheus-stack-grafana.
3,[pr-cp-reg-10049 - aris-cluster-autoscaler] - ARIS_host_inodes_will_fill_in24_hours - Filesystem is predicted to run out of inodes within the next 24 hours at current write rate.